{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import json\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "import tensorflow_io as tfio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_it_all(seed=7):\n",
    "    \"\"\" Attempt to be Reproducible \"\"\"\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "seed_it_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    left_ROWS_per_frame = 21\n",
    "    sequence_length = 20\n",
    "    batch_size = 32\n",
    "\n",
    "labels  = json.load(open('sign_to_prediction_index_map.json','r'))\n",
    "complete_df = pd.read_csv('train.csv')\n",
    "extended_df = pd.read_csv('extended_train.csv')\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, test_df = train_test_split(extended_df, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_loader(with_labels=True):\n",
    "    def load_video(video_path):\n",
    "        #print('herer')\n",
    "        video_df = tfio.IODataset.from_parquet(video_path)\n",
    "        #video_df = pd.read_parquet(video_path, engine='pyarrow')\n",
    "        #video_df.fillna(0,inplace=True)\n",
    "        left_df = video_df[video_df.type=='left_hand']\n",
    "        left_values = left_df[['x','y','z']].values\n",
    "        left_values = left_values.reshape(-1,CFG.left_ROWS_per_frame,3)\n",
    "        left_hand_array =  tf.image.resize(left_values, (CFG.sequence_length, CFG.left_ROWS_per_frame))\n",
    "        right_df = video_df[video_df.type=='right_hand']\n",
    "        right_values = right_df[['x','y','z']].values\n",
    "        right_values = right_values.reshape(-1,CFG.left_ROWS_per_frame,3)\n",
    "        right_hand_array =  tf.image.resize(right_values, (CFG.sequence_length, CFG.left_ROWS_per_frame))\n",
    "        return [left_hand_array, right_hand_array]\n",
    "    \n",
    "    def load_video_with_labels(path, label):\n",
    "        return load_video(path), labels[label]\n",
    "    \n",
    "    return load_video_with_labels if with_labels else load_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomData(tf.keras.utils.Sequence):\n",
    "    def __init__(self,df,num_frames=20,batch_size=8,shuffle=True,\\\n",
    "                 labels_path='sign_to_prediction_index_map.json'):\n",
    "        self.df = df\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.num_frames = num_frames\n",
    "        self.labels  = json.load(open('sign_to_prediction_index_map.json','r'))\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.df))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        batches = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        left_hand_input = np.zeros(shape=(self.batch_size,self.num_frames,CFG.left_ROWS_per_frame,2))\n",
    "        right_hand_input = np.zeros(shape=(self.batch_size,self.num_frames,CFG.left_ROWS_per_frame,2))\n",
    "        labels = []\n",
    "        for i,row_val in enumerate(batches):\n",
    "            row = self.df.iloc[row_val]\n",
    "            left_hand,right_hand = self.load_video(row['path'])\n",
    "            left_hand_input[i,:] = left_hand\n",
    "            right_hand_input[i,:] = right_hand\n",
    "            labels.append(self.labels[row['sign']])\n",
    "        return [left_hand_input,right_hand_input],np.asarray(labels)\n",
    "            \n",
    "    def load_video(self,video_path):\n",
    "        video_df = pd.read_parquet(video_path, engine='pyarrow')\n",
    "        video_df.dropna(inplace=True)\n",
    "        left_df = video_df[video_df.type=='left_hand']\n",
    "        left_values = left_df[['x','y']].values\n",
    "        left_values = left_values.reshape(-1,CFG.left_ROWS_per_frame,2)\n",
    "        if len(left_values)!=0:\n",
    "            left_values[:,:,0] = (left_values[:,:,0]- np.min(left_values[:,:,0]))/(left_values[:,:,0].max()- left_values[:,:,0].min())\n",
    "            left_values[:,:,1] = (left_values[:,:,1]- np.min(left_values[:,:,1]))/(left_values[:,:,1].max()- left_values[:,:,1].min())\n",
    "            left_hand_array =  tf.image.resize(left_values, (CFG.sequence_length, CFG.left_ROWS_per_frame))\n",
    "        else:\n",
    "            left_hand_array =  tf.zeros(shape=(CFG.sequence_length, CFG.left_ROWS_per_frame,2),dtype=tf.float32)\n",
    "        \n",
    "        right_df = video_df[video_df.type=='right_hand']\n",
    "        right_values = right_df[['x','y']].values\n",
    "        right_values = right_values.reshape(-1,CFG.left_ROWS_per_frame,2)\n",
    "        if len(right_values) != 0:\n",
    "            right_values[:,:,0] = (right_values[:,:,0]- np.min(right_values[:,:,0]))/(right_values[:,:,0].max()- right_values[:,:,0].min())\n",
    "            right_values[:,:,1] = (right_values[:,:,1]- np.min(right_values[:,:,1]))/(right_values[:,:,1].max()- right_values[:,:,1].min())\n",
    "            right_hand_array =  tf.image.resize(right_values, (CFG.sequence_length, CFG.left_ROWS_per_frame))\n",
    "        else:\n",
    "            right_hand_array =  tf.zeros(shape=(CFG.sequence_length, CFG.left_ROWS_per_frame,2),dtype=tf.float32)\n",
    "        return left_hand_array, right_hand_array\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)//self.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_file(file_path):\n",
    "    \"\"\"Read a JSON file and parse it into a Python object.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the JSON file to read.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary object representing the JSON data.\n",
    "        \n",
    "    Raises:\n",
    "        FileNotFoundError: If the specified file path does not exist.\n",
    "        ValueError: If the specified file path does not contain valid JSON data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Open the file and load the JSON data into a Python object\n",
    "        with open(file_path, 'r') as file:\n",
    "            json_data = json.load(file)\n",
    "        return json_data\n",
    "    except FileNotFoundError:\n",
    "        # Raise an error if the file path does not exist\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    except ValueError:\n",
    "        # Raise an error if the file does not contain valid JSON data\n",
    "        raise ValueError(f\"Invalid JSON data in file: {file_path}\")\n",
    "p2s_map = {v:k for k,v in read_json_file(\"sign_to_prediction_index_map.json\").items()}\n",
    "encoder = lambda x: s2p_map.get(x.lower())\n",
    "decoder = lambda x: p2s_map.get(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('models/030923_00_26.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_df['pred'] = None\n",
    "test_df['pred'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_df['topk'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topk(label,preds,k=3):\n",
    "    pred_labels = [decoder(j) for j in np.argsort(preds)[::-1][:k]]\n",
    "    if label in pred_labels:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2952/2952 [21:16<00:00,  2.31it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm.tqdm(range(len(extended_df)//32)):\n",
    "    batched_df = extended_df[i*32:(i+1)*32]\n",
    "    left_hand_input = np.zeros(shape=(32,CFG.sequence_length,CFG.left_ROWS_per_frame,2))\n",
    "    right_hand_input = np.zeros(shape=(32,CFG.sequence_length,CFG.left_ROWS_per_frame,2))\n",
    "    for index,path in enumerate(batched_df.path.to_list()):\n",
    "        left_hand_array, right_hand_array = train_datagen.load_video(path.replace('/kaggle/input/asl-signs/',''))\n",
    "        left_hand_input[index,:] = left_hand_array\n",
    "        right_hand_input[index,:] = right_hand_array\n",
    "    preds = model.predict([left_hand_input,right_hand_input],verbose=0)\n",
    "    pred_labels = [topk(j,k) for j,k in zip(batched_df.sign.to_list(),preds)]\n",
    "    extended_df.topk.loc[i*32:(i+1)*32-1]=pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "beside    43\n",
       "there     38\n",
       "before    38\n",
       "fast      37\n",
       "give      35\n",
       "          ..\n",
       "aunt       4\n",
       "uncle      4\n",
       "gum        3\n",
       "clown      3\n",
       "flower     2\n",
       "Name: sign, Length: 250, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extended_df[(extended_df.sign != extended_df.pred) & (extended_df.topk == False)].sign.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9565502714946494"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(extended_df[extended_df.topk == True])/len(extended_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = CustomData(train_df,num_frames=CFG.sequence_length,batch_size=CFG.batch_size)\n",
    "test_datagen = CustomData(test_df,num_frames=CFG.sequence_length,batch_size=CFG.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = extended_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cd = sklearn.metrics.confusion_matrix(new_df.sign.to_list(), new_df.pred.to_list(),labels=extended_df.sign.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 3.5007 - accuracy: 0.2448\n",
      "Epoch 1: val_accuracy improved from -inf to 0.38692, saving model to models/030923_00_26.h5\n",
      "2361/2361 [==============================] - 1125s 472ms/step - loss: 3.5007 - accuracy: 0.2448 - val_loss: 2.6387 - val_accuracy: 0.3869 - lr: 3.3300e-04\n",
      "Epoch 2/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 2.4072 - accuracy: 0.4283\n",
      "Epoch 2: val_accuracy improved from 0.38692 to 0.46944, saving model to models/030923_00_26.h5\n",
      "2361/2361 [==============================] - 1105s 468ms/step - loss: 2.4072 - accuracy: 0.4283 - val_loss: 2.2377 - val_accuracy: 0.4694 - lr: 3.3300e-04\n",
      "Epoch 3/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 2.0382 - accuracy: 0.5058\n",
      "Epoch 3: val_accuracy improved from 0.46944 to 0.51404, saving model to models/030923_00_26.h5\n",
      "2361/2361 [==============================] - 1107s 469ms/step - loss: 2.0382 - accuracy: 0.5058 - val_loss: 2.0206 - val_accuracy: 0.5140 - lr: 3.3300e-04\n",
      "Epoch 4/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 1.8143 - accuracy: 0.5522\n",
      "Epoch 4: val_accuracy improved from 0.51404 to 0.54386, saving model to models/030923_00_26.h5\n",
      "2361/2361 [==============================] - 1104s 467ms/step - loss: 1.8143 - accuracy: 0.5522 - val_loss: 1.8888 - val_accuracy: 0.5439 - lr: 3.3300e-04\n",
      "Epoch 5/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 1.6434 - accuracy: 0.5872\n",
      "Epoch 5: val_accuracy improved from 0.54386 to 0.56112, saving model to models/030923_00_26.h5\n",
      "2361/2361 [==============================] - 1105s 468ms/step - loss: 1.6434 - accuracy: 0.5872 - val_loss: 1.8016 - val_accuracy: 0.5611 - lr: 3.3300e-04\n",
      "Epoch 6/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 1.5024 - accuracy: 0.6164\n",
      "Epoch 6: val_accuracy improved from 0.56112 to 0.57436, saving model to models/030923_00_26.h5\n",
      "2361/2361 [==============================] - 1104s 468ms/step - loss: 1.5024 - accuracy: 0.6164 - val_loss: 1.7557 - val_accuracy: 0.5744 - lr: 3.3300e-04\n",
      "Epoch 7/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 1.3918 - accuracy: 0.6389\n",
      "Epoch 7: val_accuracy improved from 0.57436 to 0.59248, saving model to models/030923_00_26.h5\n",
      "2361/2361 [==============================] - 1103s 467ms/step - loss: 1.3918 - accuracy: 0.6389 - val_loss: 1.6805 - val_accuracy: 0.5925 - lr: 3.3300e-04\n",
      "Epoch 8/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 1.2918 - accuracy: 0.6606\n",
      "Epoch 8: val_accuracy improved from 0.59248 to 0.59576, saving model to models/030923_00_26.h5\n",
      "2361/2361 [==============================] - 1105s 468ms/step - loss: 1.2918 - accuracy: 0.6606 - val_loss: 1.6884 - val_accuracy: 0.5958 - lr: 3.3300e-04\n",
      "Epoch 9/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 1.2085 - accuracy: 0.6801\n",
      "Epoch 9: val_accuracy improved from 0.59576 to 0.60201, saving model to models/030923_00_26.h5\n",
      "2361/2361 [==============================] - 1103s 467ms/step - loss: 1.2085 - accuracy: 0.6801 - val_loss: 1.6620 - val_accuracy: 0.6020 - lr: 3.3300e-04\n",
      "Epoch 10/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 1.1280 - accuracy: 0.7000\n",
      "Epoch 10: val_accuracy improved from 0.60201 to 0.60583, saving model to models/030923_00_26.h5\n",
      "2361/2361 [==============================] - 1104s 468ms/step - loss: 1.1280 - accuracy: 0.7000 - val_loss: 1.6604 - val_accuracy: 0.6058 - lr: 3.3300e-04\n",
      "Epoch 11/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 1.0657 - accuracy: 0.7133\n",
      "Epoch 11: val_accuracy improved from 0.60583 to 0.61139, saving model to models/030923_00_26.h5\n",
      "2361/2361 [==============================] - 1107s 469ms/step - loss: 1.0657 - accuracy: 0.7133 - val_loss: 1.6402 - val_accuracy: 0.6114 - lr: 3.3300e-04\n",
      "Epoch 12/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 1.0046 - accuracy: 0.7266\n",
      "Epoch 12: val_accuracy improved from 0.61139 to 0.61626, saving model to models/030923_00_26.h5\n",
      "2361/2361 [==============================] - 1105s 468ms/step - loss: 1.0046 - accuracy: 0.7266 - val_loss: 1.6410 - val_accuracy: 0.6163 - lr: 3.3300e-04\n",
      "Epoch 13/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 0.9433 - accuracy: 0.7413\n",
      "Epoch 13: val_accuracy improved from 0.61626 to 0.61843, saving model to models/030923_00_26.h5\n",
      "2361/2361 [==============================] - 1105s 468ms/step - loss: 0.9433 - accuracy: 0.7413 - val_loss: 1.6309 - val_accuracy: 0.6184 - lr: 3.3300e-04\n",
      "Epoch 14/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 0.8958 - accuracy: 0.7521\n",
      "Epoch 14: val_accuracy improved from 0.61843 to 0.61992, saving model to models/030923_00_26.h5\n",
      "2361/2361 [==============================] - 1105s 468ms/step - loss: 0.8958 - accuracy: 0.7521 - val_loss: 1.6472 - val_accuracy: 0.6199 - lr: 3.3300e-04\n",
      "Epoch 15/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 0.8493 - accuracy: 0.7636\n",
      "Epoch 15: val_accuracy improved from 0.61992 to 0.62193, saving model to models/030923_00_26.h5\n",
      "2361/2361 [==============================] - 1105s 468ms/step - loss: 0.8493 - accuracy: 0.7636 - val_loss: 1.6497 - val_accuracy: 0.6219 - lr: 3.3300e-04\n",
      "Epoch 16/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 0.8080 - accuracy: 0.7735\n",
      "Epoch 16: val_accuracy improved from 0.62193 to 0.62680, saving model to models/030923_00_26.h5\n",
      "2361/2361 [==============================] - 1108s 469ms/step - loss: 0.8080 - accuracy: 0.7735 - val_loss: 1.6387 - val_accuracy: 0.6268 - lr: 3.3300e-04\n",
      "Epoch 17/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 0.7659 - accuracy: 0.7831\n",
      "Epoch 17: val_accuracy did not improve from 0.62680\n",
      "2361/2361 [==============================] - 1112s 471ms/step - loss: 0.7659 - accuracy: 0.7831 - val_loss: 1.6805 - val_accuracy: 0.6222 - lr: 3.3300e-04\n",
      "Epoch 18/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 0.7369 - accuracy: 0.7886\n",
      "Epoch 18: val_accuracy did not improve from 0.62680\n",
      "2361/2361 [==============================] - 1107s 469ms/step - loss: 0.7369 - accuracy: 0.7886 - val_loss: 1.6697 - val_accuracy: 0.6212 - lr: 3.3300e-04\n",
      "Epoch 19/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 0.7037 - accuracy: 0.7973\n",
      "Epoch 19: val_accuracy improved from 0.62680 to 0.62722, saving model to models/030923_00_26.h5\n",
      "2361/2361 [==============================] - 1107s 469ms/step - loss: 0.7037 - accuracy: 0.7973 - val_loss: 1.6685 - val_accuracy: 0.6272 - lr: 3.3300e-04\n",
      "Epoch 20/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 0.6732 - accuracy: 0.8055\n",
      "Epoch 20: val_accuracy improved from 0.62722 to 0.62733, saving model to models/030923_00_26.h5\n",
      "2361/2361 [==============================] - 1105s 468ms/step - loss: 0.6732 - accuracy: 0.8055 - val_loss: 1.6847 - val_accuracy: 0.6273 - lr: 3.3300e-04\n",
      "Epoch 21/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 0.6506 - accuracy: 0.8111\n",
      "Epoch 21: val_accuracy improved from 0.62733 to 0.62887, saving model to models/030923_00_26.h5\n",
      "2361/2361 [==============================] - 1105s 468ms/step - loss: 0.6506 - accuracy: 0.8111 - val_loss: 1.6890 - val_accuracy: 0.6289 - lr: 3.3300e-04\n",
      "Epoch 22/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 0.6254 - accuracy: 0.8180\n",
      "Epoch 22: val_accuracy improved from 0.62887 to 0.63437, saving model to models/030923_00_26.h5\n",
      "2361/2361 [==============================] - 1106s 469ms/step - loss: 0.6254 - accuracy: 0.8180 - val_loss: 1.6823 - val_accuracy: 0.6344 - lr: 3.3300e-04\n",
      "Epoch 23/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 0.5985 - accuracy: 0.8237\n",
      "Epoch 23: val_accuracy did not improve from 0.63437\n",
      "2361/2361 [==============================] - 1108s 469ms/step - loss: 0.5985 - accuracy: 0.8237 - val_loss: 1.7218 - val_accuracy: 0.6295 - lr: 3.3300e-04\n",
      "Epoch 24/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 0.5825 - accuracy: 0.8274\n",
      "Epoch 24: val_accuracy did not improve from 0.63437\n",
      "2361/2361 [==============================] - 1106s 468ms/step - loss: 0.5825 - accuracy: 0.8274 - val_loss: 1.7224 - val_accuracy: 0.6298 - lr: 3.3300e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 0.5595 - accuracy: 0.8325\n",
      "Epoch 25: val_accuracy did not improve from 0.63437\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 3.330000035930425e-05.\n",
      "2361/2361 [==============================] - 1105s 468ms/step - loss: 0.5595 - accuracy: 0.8325 - val_loss: 1.7206 - val_accuracy: 0.6333 - lr: 3.3300e-04\n",
      "Epoch 26/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 0.4033 - accuracy: 0.8845\n",
      "Epoch 26: val_accuracy improved from 0.63437 to 0.65095, saving model to models/030923_00_26.h5\n",
      "2361/2361 [==============================] - 1106s 468ms/step - loss: 0.4033 - accuracy: 0.8845 - val_loss: 1.6577 - val_accuracy: 0.6510 - lr: 3.3300e-05\n",
      "Epoch 27/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 0.3587 - accuracy: 0.8986\n",
      "Epoch 27: val_accuracy improved from 0.65095 to 0.65445, saving model to models/030923_00_26.h5\n",
      "2361/2361 [==============================] - 1106s 468ms/step - loss: 0.3587 - accuracy: 0.8986 - val_loss: 1.6522 - val_accuracy: 0.6544 - lr: 3.3300e-05\n",
      "Epoch 28/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 0.3427 - accuracy: 0.9042\n",
      "Epoch 28: val_accuracy improved from 0.65445 to 0.65546, saving model to models/030923_00_26.h5\n",
      "2361/2361 [==============================] - 1107s 469ms/step - loss: 0.3427 - accuracy: 0.9042 - val_loss: 1.6414 - val_accuracy: 0.6555 - lr: 3.3300e-05\n",
      "Epoch 29/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 0.3348 - accuracy: 0.9063\n",
      "Epoch 29: val_accuracy improved from 0.65546 to 0.65720, saving model to models/030923_00_26.h5\n",
      "2361/2361 [==============================] - 1103s 467ms/step - loss: 0.3348 - accuracy: 0.9063 - val_loss: 1.6425 - val_accuracy: 0.6572 - lr: 3.3300e-05\n",
      "Epoch 30/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 0.3207 - accuracy: 0.9119\n",
      "Epoch 30: val_accuracy did not improve from 0.65720\n",
      "2361/2361 [==============================] - 1108s 469ms/step - loss: 0.3207 - accuracy: 0.9119 - val_loss: 1.6438 - val_accuracy: 0.6553 - lr: 3.3300e-05\n"
     ]
    }
   ],
   "source": [
    "file_name = \"models/030923_00_26.h5\"\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        file_name, \n",
    "        save_best_only=True, \n",
    "        monitor=\"val_accuracy\",\n",
    "        mode=\"max\",\n",
    "        verbose = 1\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', factor=0.1,mode='max',verbose=1,\n",
    "                              patience=3, min_lr=0.000001)\n",
    "]\n",
    "model.fit(train_datagen,validation_data=test_datagen,\\\n",
    "          epochs=30, callbacks=callbacks)\n",
    "model = tf.keras.models.load_model(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('models/030523_00_09.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
