{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import tensorflow_io as tfio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_it_all(seed=7):\n",
    "    \"\"\" Attempt to be Reproducible \"\"\"\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "seed_it_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    left_ROWS_per_frame = 21\n",
    "    sequence_length = 20\n",
    "    batch_size = 32\n",
    "\n",
    "labels  = json.load(open('sign_to_prediction_index_map.json','r'))\n",
    "complete_df = pd.read_csv('train.csv')\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(complete_df, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_loader(with_labels=True):\n",
    "    def load_video(video_path):\n",
    "        #print('herer')\n",
    "        video_df = tfio.IODataset.from_parquet(video_path)\n",
    "        #video_df = pd.read_parquet(video_path, engine='pyarrow')\n",
    "        #video_df.fillna(0,inplace=True)\n",
    "        left_df = video_df[video_df.type=='left_hand']\n",
    "        left_values = left_df[['x','y','z']].values\n",
    "        left_values = left_values.reshape(-1,CFG.left_ROWS_per_frame,3)\n",
    "        left_hand_array =  tf.image.resize(left_values, (CFG.sequence_length, CFG.left_ROWS_per_frame))\n",
    "        right_df = video_df[video_df.type=='right_hand']\n",
    "        right_values = right_df[['x','y','z']].values\n",
    "        right_values = right_values.reshape(-1,CFG.left_ROWS_per_frame,3)\n",
    "        right_hand_array =  tf.image.resize(right_values, (CFG.sequence_length, CFG.left_ROWS_per_frame))\n",
    "        return [left_hand_array, right_hand_array]\n",
    "    \n",
    "    def load_video_with_labels(path, label):\n",
    "        return load_video(path), labels[label]\n",
    "    \n",
    "    return load_video_with_labels if with_labels else load_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomData(tf.keras.utils.Sequence):\n",
    "    def __init__(self,df,num_frames=20,batch_size=8,shuffle=True,\\\n",
    "                 labels_path='sign_to_prediction_index_map.json'):\n",
    "        self.df = df\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.num_frames = num_frames\n",
    "        self.labels  = json.load(open('sign_to_prediction_index_map.json','r'))\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.df))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        batches = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        left_hand_input = np.zeros(shape=(self.batch_size,self.num_frames,CFG.left_ROWS_per_frame,2))\n",
    "        right_hand_input = np.zeros(shape=(self.batch_size,self.num_frames,CFG.left_ROWS_per_frame,2))\n",
    "        labels = []\n",
    "        for i,row_val in enumerate(batches):\n",
    "            row = self.df.iloc[row_val]\n",
    "            left_hand,right_hand = self.load_video(row['path'])\n",
    "            left_hand_input[i,:] = left_hand\n",
    "            right_hand_input[i,:] = right_hand\n",
    "            labels.append(self.labels[row['sign']])\n",
    "        return [left_hand_input,right_hand_input],np.asarray(labels)\n",
    "            \n",
    "    def load_video(self,video_path):\n",
    "        video_df = pd.read_parquet(video_path, engine='pyarrow')\n",
    "        video_df.dropna(inplace=True)\n",
    "        left_df = video_df[video_df.type=='left_hand']\n",
    "        left_values = left_df[['x','y']].values\n",
    "        left_values = left_values.reshape(-1,CFG.left_ROWS_per_frame,2)\n",
    "        if len(left_values)!=0:\n",
    "            left_values[:,:,0] = (left_values[:,:,0]- np.min(left_values[:,:,0]))/(left_values[:,:,0].max()- left_values[:,:,0].min())\n",
    "            left_values[:,:,1] = (left_values[:,:,1]- np.min(left_values[:,:,1]))/(left_values[:,:,1].max()- left_values[:,:,1].min())\n",
    "            left_hand_array =  tf.image.resize(left_values, (CFG.sequence_length, CFG.left_ROWS_per_frame))\n",
    "        else:\n",
    "            left_hand_array =  tf.zeros(shape=(CFG.sequence_length, CFG.left_ROWS_per_frame,2),dtype=tf.float32)\n",
    "        \n",
    "        right_df = video_df[video_df.type=='right_hand']\n",
    "        right_values = right_df[['x','y']].values\n",
    "        right_values = right_values.reshape(-1,CFG.left_ROWS_per_frame,2)\n",
    "        if len(right_values) != 0:\n",
    "            right_values[:,:,0] = (right_values[:,:,0]- np.min(right_values[:,:,0]))/(right_values[:,:,0].max()- right_values[:,:,0].min())\n",
    "            right_values[:,:,1] = (right_values[:,:,1]- np.min(right_values[:,:,1]))/(right_values[:,:,1].max()- right_values[:,:,1].min())\n",
    "            right_hand_array =  tf.image.resize(right_values, (CFG.sequence_length, CFG.left_ROWS_per_frame))\n",
    "        else:\n",
    "            right_hand_array =  tf.zeros(shape=(CFG.sequence_length, CFG.left_ROWS_per_frame,2),dtype=tf.float32)\n",
    "        return left_hand_array, right_hand_array\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)//self.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_df = pd.read_csv('extended_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>participant_id</th>\n",
       "      <th>sequence_id</th>\n",
       "      <th>sign</th>\n",
       "      <th>start_frame</th>\n",
       "      <th>end_frame</th>\n",
       "      <th>total_frames</th>\n",
       "      <th>face_count</th>\n",
       "      <th>face_nan_count</th>\n",
       "      <th>pose_count</th>\n",
       "      <th>...</th>\n",
       "      <th>left_hand_count</th>\n",
       "      <th>left_hand_nan_count</th>\n",
       "      <th>right_hand_count</th>\n",
       "      <th>right_hand_nan_count</th>\n",
       "      <th>x_min</th>\n",
       "      <th>x_max</th>\n",
       "      <th>y_min</th>\n",
       "      <th>y_max</th>\n",
       "      <th>z_min</th>\n",
       "      <th>z_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/kaggle/input/asl-signs/train_landmark_files/2...</td>\n",
       "      <td>26734</td>\n",
       "      <td>1000035562</td>\n",
       "      <td>blow</td>\n",
       "      <td>20</td>\n",
       "      <td>42</td>\n",
       "      <td>23</td>\n",
       "      <td>10764</td>\n",
       "      <td>0</td>\n",
       "      <td>759</td>\n",
       "      <td>...</td>\n",
       "      <td>483</td>\n",
       "      <td>483</td>\n",
       "      <td>483</td>\n",
       "      <td>252</td>\n",
       "      <td>-0.031811</td>\n",
       "      <td>1.294350</td>\n",
       "      <td>-2.147826</td>\n",
       "      <td>-0.224151</td>\n",
       "      <td>-2.782624</td>\n",
       "      <td>1.910815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/kaggle/input/asl-signs/train_landmark_files/2...</td>\n",
       "      <td>28656</td>\n",
       "      <td>1000106739</td>\n",
       "      <td>wait</td>\n",
       "      <td>29</td>\n",
       "      <td>39</td>\n",
       "      <td>11</td>\n",
       "      <td>5148</td>\n",
       "      <td>0</td>\n",
       "      <td>363</td>\n",
       "      <td>...</td>\n",
       "      <td>231</td>\n",
       "      <td>231</td>\n",
       "      <td>231</td>\n",
       "      <td>189</td>\n",
       "      <td>-0.017062</td>\n",
       "      <td>1.015931</td>\n",
       "      <td>-2.222732</td>\n",
       "      <td>-0.329273</td>\n",
       "      <td>-2.543970</td>\n",
       "      <td>1.627621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/kaggle/input/asl-signs/train_landmark_files/1...</td>\n",
       "      <td>16069</td>\n",
       "      <td>100015657</td>\n",
       "      <td>cloud</td>\n",
       "      <td>103</td>\n",
       "      <td>207</td>\n",
       "      <td>105</td>\n",
       "      <td>49140</td>\n",
       "      <td>0</td>\n",
       "      <td>3465</td>\n",
       "      <td>...</td>\n",
       "      <td>2205</td>\n",
       "      <td>1617</td>\n",
       "      <td>2205</td>\n",
       "      <td>2205</td>\n",
       "      <td>-0.042923</td>\n",
       "      <td>1.197836</td>\n",
       "      <td>-2.591290</td>\n",
       "      <td>-0.248094</td>\n",
       "      <td>-2.838325</td>\n",
       "      <td>1.587503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/kaggle/input/asl-signs/train_landmark_files/2...</td>\n",
       "      <td>25571</td>\n",
       "      <td>1000210073</td>\n",
       "      <td>bird</td>\n",
       "      <td>17</td>\n",
       "      <td>28</td>\n",
       "      <td>12</td>\n",
       "      <td>5616</td>\n",
       "      <td>0</td>\n",
       "      <td>396</td>\n",
       "      <td>...</td>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.129268</td>\n",
       "      <td>1.156573</td>\n",
       "      <td>-2.294936</td>\n",
       "      <td>-0.310272</td>\n",
       "      <td>-3.018237</td>\n",
       "      <td>2.196296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/kaggle/input/asl-signs/train_landmark_files/6...</td>\n",
       "      <td>62590</td>\n",
       "      <td>1000240708</td>\n",
       "      <td>owie</td>\n",
       "      <td>22</td>\n",
       "      <td>39</td>\n",
       "      <td>18</td>\n",
       "      <td>8424</td>\n",
       "      <td>0</td>\n",
       "      <td>594</td>\n",
       "      <td>...</td>\n",
       "      <td>378</td>\n",
       "      <td>378</td>\n",
       "      <td>378</td>\n",
       "      <td>0</td>\n",
       "      <td>0.072425</td>\n",
       "      <td>1.165405</td>\n",
       "      <td>-2.264609</td>\n",
       "      <td>-0.405282</td>\n",
       "      <td>-2.970331</td>\n",
       "      <td>1.360011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path  participant_id  \\\n",
       "0  /kaggle/input/asl-signs/train_landmark_files/2...           26734   \n",
       "1  /kaggle/input/asl-signs/train_landmark_files/2...           28656   \n",
       "2  /kaggle/input/asl-signs/train_landmark_files/1...           16069   \n",
       "3  /kaggle/input/asl-signs/train_landmark_files/2...           25571   \n",
       "4  /kaggle/input/asl-signs/train_landmark_files/6...           62590   \n",
       "\n",
       "   sequence_id   sign  start_frame  end_frame  total_frames  face_count  \\\n",
       "0   1000035562   blow           20         42            23       10764   \n",
       "1   1000106739   wait           29         39            11        5148   \n",
       "2    100015657  cloud          103        207           105       49140   \n",
       "3   1000210073   bird           17         28            12        5616   \n",
       "4   1000240708   owie           22         39            18        8424   \n",
       "\n",
       "   face_nan_count  pose_count  ...  left_hand_count  left_hand_nan_count  \\\n",
       "0               0         759  ...              483                  483   \n",
       "1               0         363  ...              231                  231   \n",
       "2               0        3465  ...             2205                 1617   \n",
       "3               0         396  ...              252                  252   \n",
       "4               0         594  ...              378                  378   \n",
       "\n",
       "   right_hand_count  right_hand_nan_count     x_min     x_max     y_min  \\\n",
       "0               483                   252 -0.031811  1.294350 -2.147826   \n",
       "1               231                   189 -0.017062  1.015931 -2.222732   \n",
       "2              2205                  2205 -0.042923  1.197836 -2.591290   \n",
       "3               252                     0 -0.129268  1.156573 -2.294936   \n",
       "4               378                     0  0.072425  1.165405 -2.264609   \n",
       "\n",
       "      y_max     z_min     z_max  \n",
       "0 -0.224151 -2.782624  1.910815  \n",
       "1 -0.329273 -2.543970  1.627621  \n",
       "2 -0.248094 -2.838325  1.587503  \n",
       "3 -0.310272 -3.018237  2.196296  \n",
       "4 -0.405282 -2.970331  1.360011  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extended_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,row in extended_df.iterrows():\n",
    "    left_hand_array, right_hand_array = train_datagen.load_video(row['path'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = CustomData(train_df,num_frames=CFG.sequence_length,batch_size=CFG.batch_size)\n",
    "test_datagen = CustomData(test_df,num_frames=CFG.sequence_length,batch_size=CFG.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv1d_lstm_block(inputs, filters):\n",
    "    vector = tf.keras.layers.ConvLSTM1D(filters=32, kernel_size=3,return_sequences=True)(inputs)\n",
    "    vector = tf.keras.layers.Dropout(0.2)(vector)\n",
    "    vector = tf.keras.layers.BatchNormalization(axis=-1)(vector)\n",
    "    vector = tf.keras.layers.ConvLSTM1D(filters=64, kernel_size=3,return_sequences=True)(vector)\n",
    "    vector = tf.keras.layers.Dropout(0.2)(vector)\n",
    "    vector = tf.keras.layers.BatchNormalization(axis=-1)(vector)\n",
    "    vector = tf.keras.layers.ConvLSTM1D(filters=64, kernel_size=3)(vector)\n",
    "    vector = tf.keras.layers.Dropout(0.2)(vector)\n",
    "    return vector\n",
    "\n",
    "def get_model():\n",
    "    input1 = tf.keras.Input((CFG.sequence_length, CFG.left_ROWS_per_frame, 2), dtype=tf.float32)\n",
    "    input2 = tf.keras.Input((CFG.sequence_length, CFG.left_ROWS_per_frame, 2), dtype=tf.float32)\n",
    "    left_hand_vector = conv1d_lstm_block(input1, [64])\n",
    "    right_hand_vector = conv1d_lstm_block(input2, [64])\n",
    "    vector = tf.keras.layers.Concatenate(axis=1)([left_hand_vector, right_hand_vector])\n",
    "    vector = tf.keras.layers.Flatten()(vector)\n",
    "    output = tf.keras.layers.Dense(250, activation=\"softmax\")(vector)\n",
    "    model = tf.keras.Model(inputs=[input1,input2], outputs=output)\n",
    "    model.compile(tf.keras.optimizers.Adam(0.000333), \"sparse_categorical_crossentropy\", metrics=\"accuracy\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"conv_lstm1d_2\" (type ConvLSTM1D).\n\nNegative dimension size caused by subtracting 9 from 5 for '{{node conv_lstm1d_2/convolution}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](conv_lstm1d_2/convolution/ExpandDims, conv_lstm1d_2/convolution/ExpandDims_1)' with input shapes: [?,1,5,64], [1,9,64,64].\n\nCall arguments received by layer \"conv_lstm1d_2\" (type ConvLSTM1D):\n  • inputs=tf.Tensor(shape=(None, 20, 5, 64), dtype=float32)\n  • mask=None\n  • training=None\n  • initial_state=None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mget_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [5], line 15\u001b[0m, in \u001b[0;36mget_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m input1 \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mInput((CFG\u001b[38;5;241m.\u001b[39msequence_length, CFG\u001b[38;5;241m.\u001b[39mleft_ROWS_per_frame, \u001b[38;5;241m2\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     14\u001b[0m input2 \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mInput((CFG\u001b[38;5;241m.\u001b[39msequence_length, CFG\u001b[38;5;241m.\u001b[39mleft_ROWS_per_frame, \u001b[38;5;241m2\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m---> 15\u001b[0m left_hand_vector \u001b[38;5;241m=\u001b[39m \u001b[43mconv1d_lstm_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m right_hand_vector \u001b[38;5;241m=\u001b[39m conv1d_lstm_block(input2, [\u001b[38;5;241m64\u001b[39m])\n\u001b[1;32m     17\u001b[0m vector \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mConcatenate(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)([left_hand_vector, right_hand_vector])\n",
      "Cell \u001b[0;32mIn [5], line 8\u001b[0m, in \u001b[0;36mconv1d_lstm_block\u001b[0;34m(inputs, filters)\u001b[0m\n\u001b[1;32m      6\u001b[0m vector \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDropout(\u001b[38;5;241m0.2\u001b[39m)(vector)\n\u001b[1;32m      7\u001b[0m vector \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mBatchNormalization(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)(vector)\n\u001b[0;32m----> 8\u001b[0m vector \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConvLSTM1D\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m9\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvector\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m vector \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDropout(\u001b[38;5;241m0.2\u001b[39m)(vector)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m vector\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/layers/rnn/base_rnn.py:556\u001b[0m, in \u001b[0;36mRNN.__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    551\u001b[0m inputs, initial_state, constants \u001b[38;5;241m=\u001b[39m rnn_utils\u001b[38;5;241m.\u001b[39mstandardize_args(\n\u001b[1;32m    552\u001b[0m     inputs, initial_state, constants, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_constants\n\u001b[1;32m    553\u001b[0m )\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m initial_state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m constants \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 556\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;66;03m# If any of `initial_state` or `constants` are specified and are Keras\u001b[39;00m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;66;03m# tensors, then add them to the inputs and temporarily modify the\u001b[39;00m\n\u001b[1;32m    560\u001b[0m \u001b[38;5;66;03m# input_spec to include them.\u001b[39;00m\n\u001b[1;32m    562\u001b[0m additional_inputs \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/backend.py:5994\u001b[0m, in \u001b[0;36mconv1d\u001b[0;34m(x, kernel, strides, padding, data_format, dilation_rate)\u001b[0m\n\u001b[1;32m   5991\u001b[0m padding \u001b[38;5;241m=\u001b[39m _preprocess_padding(padding)\n\u001b[1;32m   5993\u001b[0m x, tf_data_format \u001b[38;5;241m=\u001b[39m _preprocess_conv1d_input(x, data_format)\n\u001b[0;32m-> 5994\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mv1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvolution\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5995\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5996\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5997\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdilation_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdilation_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5998\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrides\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5999\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   6000\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf_data_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   6001\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6002\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data_format \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchannels_first\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m tf_data_format \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNWC\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   6003\u001b[0m     x \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mtranspose(x, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m))  \u001b[38;5;66;03m# NWC -> NCW\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer \"conv_lstm1d_2\" (type ConvLSTM1D).\n\nNegative dimension size caused by subtracting 9 from 5 for '{{node conv_lstm1d_2/convolution}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](conv_lstm1d_2/convolution/ExpandDims, conv_lstm1d_2/convolution/ExpandDims_1)' with input shapes: [?,1,5,64], [1,9,64,64].\n\nCall arguments received by layer \"conv_lstm1d_2\" (type ConvLSTM1D):\n  • inputs=tf.Tensor(shape=(None, 20, 5, 64), dtype=float32)\n  • mask=None\n  • training=None\n  • initial_state=None"
     ]
    }
   ],
   "source": [
    "model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 20, 21, 2)]  0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 20, 21, 2)]  0           []                               \n",
      "                                                                                                  \n",
      " conv_lstm1d (ConvLSTM1D)       (None, 20, 19, 32)   13184       ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv_lstm1d_3 (ConvLSTM1D)     (None, 20, 19, 32)   13184       ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 20, 19, 32)   0           ['conv_lstm1d[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 20, 19, 32)   0           ['conv_lstm1d_3[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 20, 19, 32)  128         ['dropout[0][0]']                \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 20, 19, 32)  128         ['dropout_3[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv_lstm1d_1 (ConvLSTM1D)     (None, 20, 17, 64)   73984       ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " conv_lstm1d_4 (ConvLSTM1D)     (None, 20, 17, 64)   73984       ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 20, 17, 64)   0           ['conv_lstm1d_1[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 20, 17, 64)   0           ['conv_lstm1d_4[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 20, 17, 64)  256         ['dropout_1[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 20, 17, 64)  256         ['dropout_4[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv_lstm1d_2 (ConvLSTM1D)     (None, 15, 64)       98560       ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " conv_lstm1d_5 (ConvLSTM1D)     (None, 15, 64)       98560       ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 15, 64)       0           ['conv_lstm1d_2[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 15, 64)       0           ['conv_lstm1d_5[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 30, 64)       0           ['dropout_2[0][0]',              \n",
      "                                                                  'dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 1920)         0           ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 250)          480250      ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 852,474\n",
      "Trainable params: 852,090\n",
      "Non-trainable params: 384\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 3.5007 - accuracy: 0.2448\n",
      "Epoch 1: val_accuracy improved from -inf to 0.38692, saving model to models/030923_00_26.h5\n",
      "2361/2361 [==============================] - 1125s 472ms/step - loss: 3.5007 - accuracy: 0.2448 - val_loss: 2.6387 - val_accuracy: 0.3869 - lr: 3.3300e-04\n",
      "Epoch 2/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 2.4072 - accuracy: 0.4283\n",
      "Epoch 2: val_accuracy improved from 0.38692 to 0.46944, saving model to models/030923_00_26.h5\n",
      "2361/2361 [==============================] - 1105s 468ms/step - loss: 2.4072 - accuracy: 0.4283 - val_loss: 2.2377 - val_accuracy: 0.4694 - lr: 3.3300e-04\n",
      "Epoch 3/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 2.0382 - accuracy: 0.5058\n",
      "Epoch 3: val_accuracy improved from 0.46944 to 0.51404, saving model to models/030923_00_26.h5\n",
      "2361/2361 [==============================] - 1107s 469ms/step - loss: 2.0382 - accuracy: 0.5058 - val_loss: 2.0206 - val_accuracy: 0.5140 - lr: 3.3300e-04\n",
      "Epoch 4/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 1.8143 - accuracy: 0.5522\n",
      "Epoch 4: val_accuracy improved from 0.51404 to 0.54386, saving model to models/030923_00_26.h5\n",
      "2361/2361 [==============================] - 1104s 467ms/step - loss: 1.8143 - accuracy: 0.5522 - val_loss: 1.8888 - val_accuracy: 0.5439 - lr: 3.3300e-04\n",
      "Epoch 5/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 1.6434 - accuracy: 0.5872\n",
      "Epoch 5: val_accuracy improved from 0.54386 to 0.56112, saving model to models/030923_00_26.h5\n",
      "2361/2361 [==============================] - 1105s 468ms/step - loss: 1.6434 - accuracy: 0.5872 - val_loss: 1.8016 - val_accuracy: 0.5611 - lr: 3.3300e-04\n",
      "Epoch 6/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 1.5024 - accuracy: 0.6164\n",
      "Epoch 6: val_accuracy improved from 0.56112 to 0.57436, saving model to models/030923_00_26.h5\n",
      "2361/2361 [==============================] - 1104s 468ms/step - loss: 1.5024 - accuracy: 0.6164 - val_loss: 1.7557 - val_accuracy: 0.5744 - lr: 3.3300e-04\n",
      "Epoch 7/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 1.3918 - accuracy: 0.6389\n",
      "Epoch 7: val_accuracy improved from 0.57436 to 0.59248, saving model to models/030923_00_26.h5\n",
      "2361/2361 [==============================] - 1103s 467ms/step - loss: 1.3918 - accuracy: 0.6389 - val_loss: 1.6805 - val_accuracy: 0.5925 - lr: 3.3300e-04\n",
      "Epoch 8/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 1.2918 - accuracy: 0.6606\n",
      "Epoch 8: val_accuracy improved from 0.59248 to 0.59576, saving model to models/030923_00_26.h5\n",
      "2361/2361 [==============================] - 1105s 468ms/step - loss: 1.2918 - accuracy: 0.6606 - val_loss: 1.6884 - val_accuracy: 0.5958 - lr: 3.3300e-04\n",
      "Epoch 9/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 1.2085 - accuracy: 0.6801\n",
      "Epoch 9: val_accuracy improved from 0.59576 to 0.60201, saving model to models/030923_00_26.h5\n",
      "2361/2361 [==============================] - 1103s 467ms/step - loss: 1.2085 - accuracy: 0.6801 - val_loss: 1.6620 - val_accuracy: 0.6020 - lr: 3.3300e-04\n",
      "Epoch 10/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 1.1280 - accuracy: 0.7000\n",
      "Epoch 10: val_accuracy improved from 0.60201 to 0.60583, saving model to models/030923_00_26.h5\n",
      "2361/2361 [==============================] - 1104s 468ms/step - loss: 1.1280 - accuracy: 0.7000 - val_loss: 1.6604 - val_accuracy: 0.6058 - lr: 3.3300e-04\n",
      "Epoch 11/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 1.0657 - accuracy: 0.7133\n",
      "Epoch 11: val_accuracy improved from 0.60583 to 0.61139, saving model to models/030923_00_26.h5\n",
      "2361/2361 [==============================] - 1107s 469ms/step - loss: 1.0657 - accuracy: 0.7133 - val_loss: 1.6402 - val_accuracy: 0.6114 - lr: 3.3300e-04\n",
      "Epoch 12/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 1.0046 - accuracy: 0.7266\n",
      "Epoch 12: val_accuracy improved from 0.61139 to 0.61626, saving model to models/030923_00_26.h5\n",
      "2361/2361 [==============================] - 1105s 468ms/step - loss: 1.0046 - accuracy: 0.7266 - val_loss: 1.6410 - val_accuracy: 0.6163 - lr: 3.3300e-04\n",
      "Epoch 13/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 0.9433 - accuracy: 0.7413\n",
      "Epoch 13: val_accuracy improved from 0.61626 to 0.61843, saving model to models/030923_00_26.h5\n",
      "2361/2361 [==============================] - 1105s 468ms/step - loss: 0.9433 - accuracy: 0.7413 - val_loss: 1.6309 - val_accuracy: 0.6184 - lr: 3.3300e-04\n",
      "Epoch 14/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 0.8958 - accuracy: 0.7521\n",
      "Epoch 14: val_accuracy improved from 0.61843 to 0.61992, saving model to models/030923_00_26.h5\n",
      "2361/2361 [==============================] - 1105s 468ms/step - loss: 0.8958 - accuracy: 0.7521 - val_loss: 1.6472 - val_accuracy: 0.6199 - lr: 3.3300e-04\n",
      "Epoch 15/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 0.8493 - accuracy: 0.7636\n",
      "Epoch 15: val_accuracy improved from 0.61992 to 0.62193, saving model to models/030923_00_26.h5\n",
      "2361/2361 [==============================] - 1105s 468ms/step - loss: 0.8493 - accuracy: 0.7636 - val_loss: 1.6497 - val_accuracy: 0.6219 - lr: 3.3300e-04\n",
      "Epoch 16/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 0.8080 - accuracy: 0.7735\n",
      "Epoch 16: val_accuracy improved from 0.62193 to 0.62680, saving model to models/030923_00_26.h5\n",
      "2361/2361 [==============================] - 1108s 469ms/step - loss: 0.8080 - accuracy: 0.7735 - val_loss: 1.6387 - val_accuracy: 0.6268 - lr: 3.3300e-04\n",
      "Epoch 17/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 0.7659 - accuracy: 0.7831\n",
      "Epoch 17: val_accuracy did not improve from 0.62680\n",
      "2361/2361 [==============================] - 1112s 471ms/step - loss: 0.7659 - accuracy: 0.7831 - val_loss: 1.6805 - val_accuracy: 0.6222 - lr: 3.3300e-04\n",
      "Epoch 18/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 0.7369 - accuracy: 0.7886\n",
      "Epoch 18: val_accuracy did not improve from 0.62680\n",
      "2361/2361 [==============================] - 1107s 469ms/step - loss: 0.7369 - accuracy: 0.7886 - val_loss: 1.6697 - val_accuracy: 0.6212 - lr: 3.3300e-04\n",
      "Epoch 19/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 0.7037 - accuracy: 0.7973\n",
      "Epoch 19: val_accuracy improved from 0.62680 to 0.62722, saving model to models/030923_00_26.h5\n",
      "2361/2361 [==============================] - 1107s 469ms/step - loss: 0.7037 - accuracy: 0.7973 - val_loss: 1.6685 - val_accuracy: 0.6272 - lr: 3.3300e-04\n",
      "Epoch 20/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 0.6732 - accuracy: 0.8055\n",
      "Epoch 20: val_accuracy improved from 0.62722 to 0.62733, saving model to models/030923_00_26.h5\n",
      "2361/2361 [==============================] - 1105s 468ms/step - loss: 0.6732 - accuracy: 0.8055 - val_loss: 1.6847 - val_accuracy: 0.6273 - lr: 3.3300e-04\n",
      "Epoch 21/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 0.6506 - accuracy: 0.8111\n",
      "Epoch 21: val_accuracy improved from 0.62733 to 0.62887, saving model to models/030923_00_26.h5\n",
      "2361/2361 [==============================] - 1105s 468ms/step - loss: 0.6506 - accuracy: 0.8111 - val_loss: 1.6890 - val_accuracy: 0.6289 - lr: 3.3300e-04\n",
      "Epoch 22/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 0.6254 - accuracy: 0.8180\n",
      "Epoch 22: val_accuracy improved from 0.62887 to 0.63437, saving model to models/030923_00_26.h5\n",
      "2361/2361 [==============================] - 1106s 469ms/step - loss: 0.6254 - accuracy: 0.8180 - val_loss: 1.6823 - val_accuracy: 0.6344 - lr: 3.3300e-04\n",
      "Epoch 23/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 0.5985 - accuracy: 0.8237\n",
      "Epoch 23: val_accuracy did not improve from 0.63437\n",
      "2361/2361 [==============================] - 1108s 469ms/step - loss: 0.5985 - accuracy: 0.8237 - val_loss: 1.7218 - val_accuracy: 0.6295 - lr: 3.3300e-04\n",
      "Epoch 24/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 0.5825 - accuracy: 0.8274\n",
      "Epoch 24: val_accuracy did not improve from 0.63437\n",
      "2361/2361 [==============================] - 1106s 468ms/step - loss: 0.5825 - accuracy: 0.8274 - val_loss: 1.7224 - val_accuracy: 0.6298 - lr: 3.3300e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 0.5595 - accuracy: 0.8325\n",
      "Epoch 25: val_accuracy did not improve from 0.63437\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 3.330000035930425e-05.\n",
      "2361/2361 [==============================] - 1105s 468ms/step - loss: 0.5595 - accuracy: 0.8325 - val_loss: 1.7206 - val_accuracy: 0.6333 - lr: 3.3300e-04\n",
      "Epoch 26/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 0.4033 - accuracy: 0.8845\n",
      "Epoch 26: val_accuracy improved from 0.63437 to 0.65095, saving model to models/030923_00_26.h5\n",
      "2361/2361 [==============================] - 1106s 468ms/step - loss: 0.4033 - accuracy: 0.8845 - val_loss: 1.6577 - val_accuracy: 0.6510 - lr: 3.3300e-05\n",
      "Epoch 27/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 0.3587 - accuracy: 0.8986\n",
      "Epoch 27: val_accuracy improved from 0.65095 to 0.65445, saving model to models/030923_00_26.h5\n",
      "2361/2361 [==============================] - 1106s 468ms/step - loss: 0.3587 - accuracy: 0.8986 - val_loss: 1.6522 - val_accuracy: 0.6544 - lr: 3.3300e-05\n",
      "Epoch 28/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 0.3427 - accuracy: 0.9042\n",
      "Epoch 28: val_accuracy improved from 0.65445 to 0.65546, saving model to models/030923_00_26.h5\n",
      "2361/2361 [==============================] - 1107s 469ms/step - loss: 0.3427 - accuracy: 0.9042 - val_loss: 1.6414 - val_accuracy: 0.6555 - lr: 3.3300e-05\n",
      "Epoch 29/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 0.3348 - accuracy: 0.9063\n",
      "Epoch 29: val_accuracy improved from 0.65546 to 0.65720, saving model to models/030923_00_26.h5\n",
      "2361/2361 [==============================] - 1103s 467ms/step - loss: 0.3348 - accuracy: 0.9063 - val_loss: 1.6425 - val_accuracy: 0.6572 - lr: 3.3300e-05\n",
      "Epoch 30/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 0.3207 - accuracy: 0.9119\n",
      "Epoch 30: val_accuracy did not improve from 0.65720\n",
      "2361/2361 [==============================] - 1108s 469ms/step - loss: 0.3207 - accuracy: 0.9119 - val_loss: 1.6438 - val_accuracy: 0.6553 - lr: 3.3300e-05\n"
     ]
    }
   ],
   "source": [
    "file_name = \"models/030923_00_26.h5\"\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        file_name, \n",
    "        save_best_only=True, \n",
    "        monitor=\"val_accuracy\",\n",
    "        mode=\"max\",\n",
    "        verbose = 1\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', factor=0.1,mode='max',verbose=1,\n",
    "                              patience=3, min_lr=0.000001)\n",
    "]\n",
    "model.fit(train_datagen,validation_data=test_datagen,\\\n",
    "          epochs=30, callbacks=callbacks)\n",
    "model = tf.keras.models.load_model(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('models/030523_00_09.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
