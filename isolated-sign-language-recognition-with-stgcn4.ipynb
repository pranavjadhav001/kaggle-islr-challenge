{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.008117,
     "end_time": "2023-03-02T08:44:33.967124",
     "exception": false,
     "start_time": "2023-03-02T08:44:33.959007",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Isolated Sign Language Recognition with STGCN\n",
    "\n",
    "In this notebook, I will create Sign Language Recognition model using STGCN. To build an efficient training pipeline, I will use TFRecord Dataset from https://www.kaggle.com/datasets/lonnieqin/islr-12-time-steps-tfrecords created by notebook https://www.kaggle.com/code/lonnieqin/islr-create-tfrecord for training.\n",
    "The ST-GCN model archetecture was adapated from https://github.com/kdkalvik/ST-GCN\n",
    "It will take about 1 hour to finish runing this notebook using GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.006539,
     "end_time": "2023-03-02T08:44:33.994281",
     "exception": false,
     "start_time": "2023-03-02T08:44:33.987742",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-28T10:00:12.442535Z",
     "iopub.status.busy": "2023-04-28T10:00:12.442080Z",
     "iopub.status.idle": "2023-04-28T10:00:12.470153Z",
     "shell.execute_reply": "2023-04-28T10:00:12.469255Z",
     "shell.execute_reply.started": "2023-04-28T10:00:12.442503Z"
    },
    "papermill": {
     "duration": 0.022144,
     "end_time": "2023-03-02T08:44:34.023146",
     "exception": false,
     "start_time": "2023-03-02T08:44:34.001002",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    data_path = \"\"\n",
    "    tf_record_path = \"/kaggle/input/islr-12-time-steps-tfrecords/\"\n",
    "    sequence_length = 12\n",
    "    rows_per_frame = 543\n",
    "    is_training = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.006464,
     "end_time": "2023-03-02T08:44:34.036262",
     "exception": false,
     "start_time": "2023-03-02T08:44:34.029798",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-28T10:00:12.472531Z",
     "iopub.status.busy": "2023-04-28T10:00:12.472164Z",
     "iopub.status.idle": "2023-04-28T10:00:21.151336Z",
     "shell.execute_reply": "2023-04-28T10:00:21.150218Z",
     "shell.execute_reply.started": "2023-04-28T10:00:12.472493Z"
    },
    "papermill": {
     "duration": 7.496016,
     "end_time": "2023-03-02T08:44:41.539085",
     "exception": false,
     "start_time": "2023-03-02T08:44:34.043069",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import math\n",
    "np.random.seed(16)\n",
    "tf.random.set_seed(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.006496,
     "end_time": "2023-03-02T08:44:41.597734",
     "exception": false,
     "start_time": "2023-03-02T08:44:41.591238",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-28T10:00:28.032382Z",
     "iopub.status.busy": "2023-04-28T10:00:28.031796Z",
     "iopub.status.idle": "2023-04-28T10:00:28.042057Z",
     "shell.execute_reply": "2023-04-28T10:00:28.040895Z",
     "shell.execute_reply.started": "2023-04-28T10:00:28.032346Z"
    },
    "papermill": {
     "duration": 0.017828,
     "end_time": "2023-03-02T08:44:41.622261",
     "exception": false,
     "start_time": "2023-03-02T08:44:41.604433",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ROWS_PER_FRAME = 543  # number of landmarks per frame\n",
    "\n",
    "def load_relevant_data_subset_with_imputation(pq_path):\n",
    "    data_columns = ['x', 'y']\n",
    "    data = pd.read_parquet(pq_path, columns=data_columns)\n",
    "    data.replace(np.nan, 0, inplace=True)\n",
    "    n_frames = int(len(data) / ROWS_PER_FRAME)\n",
    "    data = data.values.reshape(n_frames, ROWS_PER_FRAME, len(data_columns))\n",
    "    return data.astype(np.float16)\n",
    "\n",
    "def load_relevant_data_subset(pq_path):\n",
    "    data_columns = ['x', 'y']\n",
    "    data = pd.read_parquet(pq_path, columns=data_columns)\n",
    "    n_frames = int(len(data) / ROWS_PER_FRAME)\n",
    "    data = data.values.reshape(n_frames, ROWS_PER_FRAME, len(data_columns))\n",
    "    return data.astype(np.float32)\n",
    "\n",
    "def read_dict(file_path):\n",
    "    path = os.path.expanduser(file_path)\n",
    "    with open(path, \"r\") as f:\n",
    "        dic = json.load(f)\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.006542,
     "end_time": "2023-03-02T08:44:41.635478",
     "exception": false,
     "start_time": "2023-03-02T08:44:41.628936",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-28T10:00:28.576086Z",
     "iopub.status.busy": "2023-04-28T10:00:28.575721Z",
     "iopub.status.idle": "2023-04-28T10:00:28.762511Z",
     "shell.execute_reply": "2023-04-28T10:00:28.761424Z",
     "shell.execute_reply.started": "2023-04-28T10:00:28.576053Z"
    },
    "papermill": {
     "duration": 0.206559,
     "end_time": "2023-03-02T08:44:41.848795",
     "exception": false,
     "start_time": "2023-03-02T08:44:41.642236",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>participant_id</th>\n",
       "      <th>sequence_id</th>\n",
       "      <th>sign</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_landmark_files/26734/1000035562.parquet</td>\n",
       "      <td>26734</td>\n",
       "      <td>1000035562</td>\n",
       "      <td>blow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_landmark_files/28656/1000106739.parquet</td>\n",
       "      <td>28656</td>\n",
       "      <td>1000106739</td>\n",
       "      <td>wait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_landmark_files/16069/100015657.parquet</td>\n",
       "      <td>16069</td>\n",
       "      <td>100015657</td>\n",
       "      <td>cloud</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_landmark_files/25571/1000210073.parquet</td>\n",
       "      <td>25571</td>\n",
       "      <td>1000210073</td>\n",
       "      <td>bird</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_landmark_files/62590/1000240708.parquet</td>\n",
       "      <td>62590</td>\n",
       "      <td>1000240708</td>\n",
       "      <td>owie</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            path  participant_id  sequence_id  \\\n",
       "0  train_landmark_files/26734/1000035562.parquet           26734   1000035562   \n",
       "1  train_landmark_files/28656/1000106739.parquet           28656   1000106739   \n",
       "2   train_landmark_files/16069/100015657.parquet           16069    100015657   \n",
       "3  train_landmark_files/25571/1000210073.parquet           25571   1000210073   \n",
       "4  train_landmark_files/62590/1000240708.parquet           62590   1000240708   \n",
       "\n",
       "    sign  \n",
       "0   blow  \n",
       "1   wait  \n",
       "2  cloud  \n",
       "3   bird  \n",
       "4   owie  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(f\"{CFG.data_path}train.csv\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.007434,
     "end_time": "2023-03-02T08:44:41.8635",
     "exception": false,
     "start_time": "2023-03-02T08:44:41.856066",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "There are 21 participants. Each of them created about 3000 to 5000 training records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-28T10:00:21.429554Z",
     "iopub.status.busy": "2023-04-28T10:00:21.428647Z",
     "iopub.status.idle": "2023-04-28T10:00:21.441674Z",
     "shell.execute_reply": "2023-04-28T10:00:21.440628Z",
     "shell.execute_reply.started": "2023-04-28T10:00:21.429504Z"
    },
    "papermill": {
     "duration": 0.024082,
     "end_time": "2023-03-02T08:44:41.894435",
     "exception": false,
     "start_time": "2023-03-02T08:44:41.870353",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.participant_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-28T10:00:21.443808Z",
     "iopub.status.busy": "2023-04-28T10:00:21.443178Z",
     "iopub.status.idle": "2023-04-28T10:00:21.780692Z",
     "shell.execute_reply": "2023-04-28T10:00:21.779677Z",
     "shell.execute_reply.started": "2023-04-28T10:00:21.443772Z"
    },
    "papermill": {
     "duration": 0.339293,
     "end_time": "2023-03-02T08:44:42.240816",
     "exception": false,
     "start_time": "2023-03-02T08:44:41.901523",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot: >"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.participant_id.value_counts().plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.007382,
     "end_time": "2023-03-02T08:44:42.255978",
     "exception": false,
     "start_time": "2023-03-02T08:44:42.248596",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "There are 94477 training samples in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-28T10:00:33.098748Z",
     "iopub.status.busy": "2023-04-28T10:00:33.098364Z",
     "iopub.status.idle": "2023-04-28T10:00:33.105587Z",
     "shell.execute_reply": "2023-04-28T10:00:33.104499Z",
     "shell.execute_reply.started": "2023-04-28T10:00:33.098691Z"
    },
    "papermill": {
     "duration": 0.016757,
     "end_time": "2023-03-02T08:44:42.280157",
     "exception": false,
     "start_time": "2023-03-02T08:44:42.2634",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94477"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.007305,
     "end_time": "2023-03-02T08:44:42.294963",
     "exception": false,
     "start_time": "2023-03-02T08:44:42.287658",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "There are 250 kinds of sign languages that we need to make prediction on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-28T10:00:34.919532Z",
     "iopub.status.busy": "2023-04-28T10:00:34.918776Z",
     "iopub.status.idle": "2023-04-28T10:00:34.974299Z",
     "shell.execute_reply": "2023-04-28T10:00:34.973172Z",
     "shell.execute_reply.started": "2023-04-28T10:00:34.919491Z"
    },
    "papermill": {
     "duration": 0.059155,
     "end_time": "2023-03-02T08:44:42.361615",
     "exception": false,
     "start_time": "2023-03-02T08:44:42.30246",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'TV': 0, 'after': 1, 'airplane': 2, 'all': 3, 'alligator': 4, 'animal': 5, 'another': 6, 'any': 7, 'apple': 8, 'arm': 9, 'aunt': 10, 'awake': 11, 'backyard': 12, 'bad': 13, 'balloon': 14, 'bath': 15, 'because': 16, 'bed': 17, 'bedroom': 18, 'bee': 19, 'before': 20, 'beside': 21, 'better': 22, 'bird': 23, 'black': 24, 'blow': 25, 'blue': 26, 'boat': 27, 'book': 28, 'boy': 29, 'brother': 30, 'brown': 31, 'bug': 32, 'bye': 33, 'callonphone': 34, 'can': 35, 'car': 36, 'carrot': 37, 'cat': 38, 'cereal': 39, 'chair': 40, 'cheek': 41, 'child': 42, 'chin': 43, 'chocolate': 44, 'clean': 45, 'close': 46, 'closet': 47, 'cloud': 48, 'clown': 49, 'cow': 50, 'cowboy': 51, 'cry': 52, 'cut': 53, 'cute': 54, 'dad': 55, 'dance': 56, 'dirty': 57, 'dog': 58, 'doll': 59, 'donkey': 60, 'down': 61, 'drawer': 62, 'drink': 63, 'drop': 64, 'dry': 65, 'dryer': 66, 'duck': 67, 'ear': 68, 'elephant': 69, 'empty': 70, 'every': 71, 'eye': 72, 'face': 73, 'fall': 74, 'farm': 75, 'fast': 76, 'feet': 77, 'find': 78, 'fine': 79, 'finger': 80, 'finish': 81, 'fireman': 82, 'first': 83, 'fish': 84, 'flag': 85, 'flower': 86, 'food': 87, 'for': 88, 'frenchfries': 89, 'frog': 90, 'garbage': 91, 'gift': 92, 'giraffe': 93, 'girl': 94, 'give': 95, 'glasswindow': 96, 'go': 97, 'goose': 98, 'grandma': 99, 'grandpa': 100, 'grass': 101, 'green': 102, 'gum': 103, 'hair': 104, 'happy': 105, 'hat': 106, 'hate': 107, 'have': 108, 'haveto': 109, 'head': 110, 'hear': 111, 'helicopter': 112, 'hello': 113, 'hen': 114, 'hesheit': 115, 'hide': 116, 'high': 117, 'home': 118, 'horse': 119, 'hot': 120, 'hungry': 121, 'icecream': 122, 'if': 123, 'into': 124, 'jacket': 125, 'jeans': 126, 'jump': 127, 'kiss': 128, 'kitty': 129, 'lamp': 130, 'later': 131, 'like': 132, 'lion': 133, 'lips': 134, 'listen': 135, 'look': 136, 'loud': 137, 'mad': 138, 'make': 139, 'man': 140, 'many': 141, 'milk': 142, 'minemy': 143, 'mitten': 144, 'mom': 145, 'moon': 146, 'morning': 147, 'mouse': 148, 'mouth': 149, 'nap': 150, 'napkin': 151, 'night': 152, 'no': 153, 'noisy': 154, 'nose': 155, 'not': 156, 'now': 157, 'nuts': 158, 'old': 159, 'on': 160, 'open': 161, 'orange': 162, 'outside': 163, 'owie': 164, 'owl': 165, 'pajamas': 166, 'pen': 167, 'pencil': 168, 'penny': 169, 'person': 170, 'pig': 171, 'pizza': 172, 'please': 173, 'police': 174, 'pool': 175, 'potty': 176, 'pretend': 177, 'pretty': 178, 'puppy': 179, 'puzzle': 180, 'quiet': 181, 'radio': 182, 'rain': 183, 'read': 184, 'red': 185, 'refrigerator': 186, 'ride': 187, 'room': 188, 'sad': 189, 'same': 190, 'say': 191, 'scissors': 192, 'see': 193, 'shhh': 194, 'shirt': 195, 'shoe': 196, 'shower': 197, 'sick': 198, 'sleep': 199, 'sleepy': 200, 'smile': 201, 'snack': 202, 'snow': 203, 'stairs': 204, 'stay': 205, 'sticky': 206, 'store': 207, 'story': 208, 'stuck': 209, 'sun': 210, 'table': 211, 'talk': 212, 'taste': 213, 'thankyou': 214, 'that': 215, 'there': 216, 'think': 217, 'thirsty': 218, 'tiger': 219, 'time': 220, 'tomorrow': 221, 'tongue': 222, 'tooth': 223, 'toothbrush': 224, 'touch': 225, 'toy': 226, 'tree': 227, 'uncle': 228, 'underwear': 229, 'up': 230, 'vacuum': 231, 'wait': 232, 'wake': 233, 'water': 234, 'wet': 235, 'weus': 236, 'where': 237, 'white': 238, 'who': 239, 'why': 240, 'will': 241, 'wolf': 242, 'yellow': 243, 'yes': 244, 'yesterday': 245, 'yourself': 246, 'yucky': 247, 'zebra': 248, 'zipper': 249}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>participant_id</th>\n",
       "      <th>sequence_id</th>\n",
       "      <th>sign</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_landmark_files/26734/1000035562.parquet</td>\n",
       "      <td>26734</td>\n",
       "      <td>1000035562</td>\n",
       "      <td>blow</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_landmark_files/28656/1000106739.parquet</td>\n",
       "      <td>28656</td>\n",
       "      <td>1000106739</td>\n",
       "      <td>wait</td>\n",
       "      <td>232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_landmark_files/16069/100015657.parquet</td>\n",
       "      <td>16069</td>\n",
       "      <td>100015657</td>\n",
       "      <td>cloud</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_landmark_files/25571/1000210073.parquet</td>\n",
       "      <td>25571</td>\n",
       "      <td>1000210073</td>\n",
       "      <td>bird</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_landmark_files/62590/1000240708.parquet</td>\n",
       "      <td>62590</td>\n",
       "      <td>1000240708</td>\n",
       "      <td>owie</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            path  participant_id  sequence_id  \\\n",
       "0  train_landmark_files/26734/1000035562.parquet           26734   1000035562   \n",
       "1  train_landmark_files/28656/1000106739.parquet           28656   1000106739   \n",
       "2   train_landmark_files/16069/100015657.parquet           16069    100015657   \n",
       "3  train_landmark_files/25571/1000210073.parquet           25571   1000210073   \n",
       "4  train_landmark_files/62590/1000240708.parquet           62590   1000240708   \n",
       "\n",
       "    sign  label  \n",
       "0   blow     25  \n",
       "1   wait    232  \n",
       "2  cloud     48  \n",
       "3   bird     23  \n",
       "4   owie    164  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_index = read_dict(f\"{CFG.data_path}sign_to_prediction_index_map.json\")\n",
    "index_label = dict([(label_index[key], key) for key in label_index])\n",
    "print(label_index)\n",
    "train[\"label\"] = train[\"sign\"].map(lambda sign: label_index[sign])\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.007629,
     "end_time": "2023-03-02T08:44:42.377",
     "exception": false,
     "start_time": "2023-03-02T08:44:42.369371",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Create Tensorflow Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-28T10:00:37.735320Z",
     "iopub.status.busy": "2023-04-28T10:00:37.734393Z",
     "iopub.status.idle": "2023-04-28T10:00:37.745983Z",
     "shell.execute_reply": "2023-04-28T10:00:37.744961Z",
     "shell.execute_reply.started": "2023-04-28T10:00:37.735279Z"
    },
    "papermill": {
     "duration": 0.019748,
     "end_time": "2023-03-02T08:44:42.404541",
     "exception": false,
     "start_time": "2023-03-02T08:44:42.384793",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def decode_function(record_bytes):\n",
    "    return tf.io.parse_single_example(\n",
    "          # Data\n",
    "          record_bytes,\n",
    "          # Schema\n",
    "          {\n",
    "              \"feature\": tf.io.FixedLenFeature([12 * 543 * 3], dtype=tf.float32),\n",
    "              \"label\": tf.io.FixedLenFeature([], dtype=tf.int64)\n",
    "          }\n",
    "      )\n",
    "def preprocess(item):\n",
    "    features = item[\"feature\"]\n",
    "#     features = tf.reshape(features, (1,CFG.sequence_length, 543,3))\n",
    "    features=tf.reshape(features, (1,12, 543, 3))\n",
    "#         \"face\"       : np.arange(0, 468),\n",
    "#     \"left_hand\"  : np.arange(468, 489),\n",
    "#     \"pose\"       : np.arange(489, 522),\n",
    "#     \"right_hand\" : np.arange(522, 543),\n",
    "    features=tf.transpose(features, perm=[3, 1, 2, 0])\n",
    "    features1=features[:,-5:,468:489,:]\n",
    "    features2=features[:,-5:,522:543,:]\n",
    "    features=tf.concat([features1, features2],2)\n",
    "    print(features.shape)\n",
    "    return features, item[\"label\"]         \n",
    "def make_dataset(file_paths, batch_size=128, mode=\"train\"):\n",
    "    ds = tf.data.TFRecordDataset(file_paths)\n",
    "    ds = ds.map(decode_function)\n",
    "    ds = ds.map(preprocess)\n",
    "    options = tf.data.Options()\n",
    "    if mode == \"train\":\n",
    "        ds = ds.shuffle(1024)\n",
    "        options.experimental_deterministic = False\n",
    "    ds = ds.batch(batch_size, drop_remainder=True)\n",
    "    ds = ds.with_options(options) \n",
    "    ds = ds.cache().prefetch(tf.data.AUTOTUNE)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_TYPES = ['left_hand', 'pose', 'right_hand']\n",
    "START_IDX = 468\n",
    "LIPS_IDXS0 = np.array([\n",
    "        61, 185, 40, 39, 37, 0, 267, 269, 270, 409,\n",
    "        291, 146, 91, 181, 84, 17, 314, 405, 321, 375,\n",
    "        78, 191, 80, 81, 82, 13, 312, 311, 310, 415,\n",
    "        95, 88, 178, 87, 14, 317, 402, 318, 324, 308,\n",
    "    ])\n",
    "# Landmark indices in original data\n",
    "LEFT_HAND_IDXS0 = np.arange(468,489)\n",
    "RIGHT_HAND_IDXS0 = np.arange(522,543)\n",
    "LEFT_POSE_IDXS0 = np.array([502, 504, 506, 508, 510])\n",
    "RIGHT_POSE_IDXS0 = np.array([503, 505, 507, 509, 511])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.load('X_train_20x61_left.npy')\n",
    "y_train = np.load('y_train_20x61_left.npy')\n",
    "x_test = np.load('X_test_20x61_left.npy')\n",
    "y_test = np.load('y_test_20x61_left.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIPS_reset = np.arange(len(LIPS_IDXS0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "       34, 35, 36, 37, 38, 39])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LIPS_reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lips_index = []\n",
    "for link in FACEMESH_LIPS:\n",
    "    i,j = link\n",
    "    lips_index.append((21+LIPS_reset[np.where(LIPS_IDXS0 == i)[0][0]],21+LIPS_reset[np.where(LIPS_IDXS0 == j)[0][0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "FACEMESH_LIPS = frozenset([(61, 146), (146, 91), (91, 181), (181, 84), (84, 17),\n",
    "                           (17, 314), (314, 405), (405, 321), (321, 375),\n",
    "                           (375, 291), (61, 185), (185, 40), (40, 39), (39, 37),\n",
    "                           (37, 0), (0, 267),\n",
    "                           (267, 269), (269, 270), (270, 409), (409, 291),\n",
    "                           (78, 95), (95, 88), (88, 178), (178, 87), (87, 14),\n",
    "                           (14, 317), (317, 402), (402, 318), (318, 324),\n",
    "                           (324, 308), (78, 191), (191, 80), (80, 81), (81, 82),\n",
    "                           (82, 13), (13, 312), (312, 311), (311, 310),\n",
    "                           (310, 415), (415, 308)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(29, 30),\n",
       " (56, 57),\n",
       " (44, 45),\n",
       " (33, 34),\n",
       " (25, 26),\n",
       " (35, 36),\n",
       " (28, 29),\n",
       " (39, 40),\n",
       " (58, 59),\n",
       " (47, 48),\n",
       " (50, 60),\n",
       " (36, 37),\n",
       " (21, 32),\n",
       " (41, 51),\n",
       " (26, 27),\n",
       " (45, 46),\n",
       " (37, 38),\n",
       " (53, 54),\n",
       " (27, 28),\n",
       " (21, 22),\n",
       " (55, 56),\n",
       " (52, 53),\n",
       " (22, 23),\n",
       " (38, 39),\n",
       " (46, 47),\n",
       " (59, 60),\n",
       " (30, 31),\n",
       " (32, 33),\n",
       " (54, 55),\n",
       " (41, 42),\n",
       " (51, 52),\n",
       " (48, 49),\n",
       " (24, 25),\n",
       " (23, 24),\n",
       " (57, 58),\n",
       " (42, 43),\n",
       " (43, 44),\n",
       " (49, 50),\n",
       " (34, 35),\n",
       " (40, 31)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lips_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train = x_train[:,:,:21,:]\n",
    "#x_test = x_test[:,:,:21,:]\n",
    "\n",
    "x_train = np.transpose(x_train,(0, 3,1, 2))\n",
    "x_test = np.transpose(x_test,(0,3, 1, 2))\n",
    "x_train = np.expand_dims(x_train,axis=-1)\n",
    "x_test = np.expand_dims(x_test,axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-28T10:00:39.323274Z",
     "iopub.status.busy": "2023-04-28T10:00:39.322502Z",
     "iopub.status.idle": "2023-04-28T10:00:42.286313Z",
     "shell.execute_reply": "2023-04-28T10:00:42.285220Z",
     "shell.execute_reply.started": "2023-04-28T10:00:39.323222Z"
    },
    "papermill": {
     "duration": 0.017938,
     "end_time": "2023-03-02T08:44:42.43015",
     "exception": false,
     "start_time": "2023-03-02T08:44:42.412212",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "unique_ids = np.array(sorted(train.participant_id.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-28T10:01:29.918426Z",
     "iopub.status.busy": "2023-04-28T10:01:29.917624Z",
     "iopub.status.idle": "2023-04-28T10:01:29.928310Z",
     "shell.execute_reply": "2023-04-28T10:01:29.926731Z",
     "shell.execute_reply.started": "2023-04-28T10:01:29.918383Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def edge2mat(link, num_node):\n",
    "    A = np.zeros((num_node, num_node))\n",
    "    for i, j in link:\n",
    "        A[j, i] = 1\n",
    "    return A\n",
    "\n",
    "\n",
    "def normalize_digraph(A):  # 除以每列的和\n",
    "    Dl = np.sum(A, 0)\n",
    "    h, w = A.shape\n",
    "    Dn = np.zeros((w, w))\n",
    "    for i in range(w):\n",
    "        if Dl[i] > 0:\n",
    "            Dn[i, i] = Dl[i] ** (-1)\n",
    "    AD = np.dot(A, Dn)\n",
    "    return AD\n",
    "\n",
    "\n",
    "def get_spatial_graph(num_node, self_link, inward, outward):\n",
    "    I = edge2mat(self_link, num_node)\n",
    "    In = normalize_digraph(edge2mat(inward, num_node))\n",
    "    Out = normalize_digraph(edge2mat(outward, num_node))\n",
    "    A = np.stack((I, In, Out))\n",
    "    return A\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![handlandmark](https://developers.google.com/static/mediapipe/images/solutions/hand-landmarks.png) create the node graph for hand landmarks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-28T10:01:35.614777Z",
     "iopub.status.busy": "2023-04-28T10:01:35.614391Z",
     "iopub.status.idle": "2023-04-28T10:01:35.626798Z",
     "shell.execute_reply": "2023-04-28T10:01:35.624837Z",
     "shell.execute_reply.started": "2023-04-28T10:01:35.614740Z"
    }
   },
   "outputs": [],
   "source": [
    "num_node = 61\n",
    "self_link = [(i, i) for i in range(num_node)]\n",
    "inward_ori_index = [(1, 2), (2, 3), (3, 4), (4, 5), (1, 6), (6, 7), (7, 8),\n",
    "                    (8, 9), (6, 10), (10, 11), (11, 12), (12, 13), (10, 14),\n",
    "                    (14, 15), (15, 16), (16, 17), (14, 18), (18, 19), (19, 20),\n",
    "                    (20, 21), (18, 1)]\n",
    "inward_ori_index2=[(1+21, 2+21), (2+21, 3+21), (3+21, 4+21), (4+21, 5+21), (1+21, 6), \n",
    "                   (6+21, 7+21), (7+21, 8+21), (8+21, 9+21), (6+21, 10+21), \n",
    "                    (10+21, 11+21), (11+21, 12+21), (12+21, 13+21), (10+21, 14+21),\n",
    "                    (14+21, 15+21), (15+21, 16+21), (16+21, 17+21), (14+21, 18+21), (18+21, 19+21), (19+21, 20+21),\n",
    "                    (20+21, 21+21), (18+21, 1+21)]\n",
    "inward_ori_index.extend(lips_index)\n",
    "inward = [(i - 1, j - 1) for (i, j) in inward_ori_index]\n",
    "outward = [(j, i) for (i, j) in inward]\n",
    "neighbor = inward + outward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(outward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-28T10:02:02.720112Z",
     "iopub.status.busy": "2023-04-28T10:02:02.719314Z",
     "iopub.status.idle": "2023-04-28T10:02:02.726845Z",
     "shell.execute_reply": "2023-04-28T10:02:02.725718Z",
     "shell.execute_reply.started": "2023-04-28T10:02:02.720072Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class Graph:\n",
    "    def __init__(self, labeling_mode='spatial'):\n",
    "        self.A = self.get_adjacency_matrix(labeling_mode)\n",
    "        self.num_node = num_node\n",
    "        self.self_link = self_link\n",
    "        self.inward = inward\n",
    "        self.outward = outward\n",
    "        self.neighbor = neighbor\n",
    "\n",
    "    def get_adjacency_matrix(self, labeling_mode=None):\n",
    "        if labeling_mode is None:\n",
    "            return self.A\n",
    "        if labeling_mode == 'spatial':\n",
    "            A = get_spatial_graph(num_node, self_link, inward, outward)\n",
    "        else:\n",
    "            raise ValueError()\n",
    "        return A\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.007677,
     "end_time": "2023-03-02T08:44:42.445922",
     "exception": false,
     "start_time": "2023-03-02T08:44:42.438245",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-28T10:31:59.243333Z",
     "iopub.status.busy": "2023-04-28T10:31:59.242459Z",
     "iopub.status.idle": "2023-04-28T10:31:59.271826Z",
     "shell.execute_reply": "2023-04-28T10:31:59.270679Z",
     "shell.execute_reply.started": "2023-04-28T10:31:59.243271Z"
    }
   },
   "outputs": [],
   "source": [
    "REGULARIZER = tf.keras.regularizers.l2(l=0.001)\n",
    "INITIALIZER = tf.keras.initializers.VarianceScaling(scale=2.,\n",
    "                                                    mode=\"fan_out\",\n",
    "                                                    distribution=\"truncated_normal\")\n",
    "class SGCN(tf.keras.Model):\n",
    "    def __init__(self, filters, kernel_size=3):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.conv = tf.keras.layers.Conv2D(filters*kernel_size,\n",
    "                                           kernel_size=1,\n",
    "                                           padding='same',\n",
    "                                           kernel_initializer=INITIALIZER,\n",
    "                                           data_format='channels_first',\n",
    "                                           kernel_regularizer=REGULARIZER)\n",
    "\n",
    "    # N, C, T, V\n",
    "    def call(self, x, A, training):\n",
    "        x = self.conv(x)\n",
    "\n",
    "        N = tf.shape(x)[0]\n",
    "        C = tf.shape(x)[1]\n",
    "        T = tf.shape(x)[2]\n",
    "        V = tf.shape(x)[3]\n",
    "\n",
    "        x = tf.reshape(x, [N, self.kernel_size, C//self.kernel_size, T, V])\n",
    "        x = tf.einsum('nkctv,kvw->nctw', x, A)\n",
    "        return x, A\n",
    "\n",
    "\n",
    "\"\"\"Applies a spatial temporal graph convolution over an input graph sequence.\n",
    "    Args:\n",
    "        filters (int): Number of channels produced by the convolution\n",
    "        kernel_size (tuple): Size of the temporal convolving kernel and graph convolving kernel\n",
    "        stride (int, optional): Stride of the temporal convolution. Default: 1\n",
    "        activation (activation function/name, optional): activation function to use\n",
    "        residual (bool, optional): If ``True``, applies a residual mechanism. Default: ``True``\n",
    "        downsample (bool, optional): If ``True``, applies a downsampling residual mechanism. Default: ``True``\n",
    "                                     the value is used only when residual is ``True``\n",
    "    Shape:\n",
    "        - Input[0]: Input graph sequence in :math:`(N, in_channels, T_{in}, V)` format\n",
    "        - Input[1]: Input graph adjacency matrix in :math:`(K, V, V)` format\n",
    "        - Output[0]: Outpu graph sequence in :math:`(N, out_channels, T_{out}, V)` format\n",
    "        - Output[1]: Graph adjacency matrix for output data in :math:`(K, V, V)` format\n",
    "        where\n",
    "            :math:`N` is a batch size,\n",
    "            :math:`K` is the spatial kernel size, as :math:`K == kernel_size[1]`,\n",
    "            :math:`T_{in}/T_{out}` is a length of input/output sequence,\n",
    "            :math:`V` is the number of graph nodes.\n",
    "\"\"\"\n",
    "class STGCN(tf.keras.Model):\n",
    "    def __init__(self, filters, kernel_size=[9, 3], stride=1, activation='relu',\n",
    "                 residual=True, downsample=False):\n",
    "        super().__init__()\n",
    "        self.sgcn = SGCN(filters, kernel_size=kernel_size[1])\n",
    "\n",
    "        self.tgcn = tf.keras.Sequential()\n",
    "        self.tgcn.add(tf.keras.layers.BatchNormalization(axis=1))\n",
    "        self.tgcn.add(tf.keras.layers.Activation(activation))\n",
    "        self.tgcn.add(tf.keras.layers.Conv2D(filters,\n",
    "                                                kernel_size=[kernel_size[0], 1],\n",
    "                                                strides=[stride, 1],\n",
    "                                                padding='same',\n",
    "                                                kernel_initializer=INITIALIZER,\n",
    "                                                data_format='channels_first',\n",
    "                                                kernel_regularizer=REGULARIZER))\n",
    "        self.tgcn.add(tf.keras.layers.BatchNormalization(axis=1))\n",
    "\n",
    "        self.act = tf.keras.layers.Activation(activation)\n",
    "\n",
    "        if not residual:\n",
    "            self.residual = lambda x, training=False: 0\n",
    "        elif residual and stride == 1 and not downsample:\n",
    "            self.residual = lambda x, training=False: x\n",
    "        else:\n",
    "            self.residual = tf.keras.Sequential()\n",
    "            self.residual.add(tf.keras.layers.Conv2D(filters,\n",
    "                                                        kernel_size=[1, 1],\n",
    "                                                        strides=[stride, 1],\n",
    "                                                        padding='same',\n",
    "                                                        kernel_initializer=INITIALIZER,\n",
    "                                                        data_format='channels_first',\n",
    "                                                        kernel_regularizer=REGULARIZER))\n",
    "            self.residual.add(tf.keras.layers.BatchNormalization(axis=1))\n",
    "\n",
    "    def call(self, x, A, training=True):\n",
    "        res = self.residual(x, training=training)\n",
    "        x, A = self.sgcn(x, A, training=training)\n",
    "        x = self.tgcn(x, training=training)\n",
    "        x += res\n",
    "        x = self.act(x)\n",
    "        return x, A\n",
    "\n",
    "\n",
    "\"\"\"Spatial temporal graph convolutional networks.\n",
    "    Args:\n",
    "        num_class (int): Number of classes for the classification task\n",
    "    Shape:(3, 5, 42, 1)\n",
    "        - Input: :math:`(N, in_channels, T_{in}, V_{in}, M_{in})`\n",
    "        - Output: :math:`(N, num_class)` where\n",
    "            :math:`N` is a batch size,\n",
    "            :math:`T_{in}` is a length of input sequence,\n",
    "            :math:`V_{in}` is the number of graph nodes,\n",
    "            :math:`M_{in}` is the number of instance in a frame.\n",
    "\"\"\"\n",
    "class Model(tf.keras.Model):\n",
    "    def __init__(self, num_classes=250):\n",
    "        super().__init__()\n",
    "\n",
    "        graph = Graph()\n",
    "        self.A = tf.Variable(graph.A,\n",
    "                             dtype=tf.float32,\n",
    "                             trainable=False,\n",
    "                             name='adjacency_matrix')\n",
    "\n",
    "        self.data_bn = tf.keras.layers.BatchNormalization(axis=1)\n",
    "\n",
    "        self.STGCN_layers = []\n",
    "        self.STGCN_layers.append(STGCN(64, residual=False))\n",
    "        #self.STGCN_layers.append(STGCN(64))\n",
    "        #self.STGCN_layers.append(STGCN(64))\n",
    "        #self.STGCN_layers.append(STGCN(64))\n",
    "        self.STGCN_layers.append(STGCN(128, stride=2, downsample=True))\n",
    "        #self.STGCN_layers.append(STGCN(128))\n",
    "        #self.STGCN_layers.append(STGCN(128))\n",
    "        self.STGCN_layers.append(STGCN(256, stride=2, downsample=True))\n",
    "        #self.STGCN_layers.append(STGCN(256))\n",
    "        #self.STGCN_layers.append(STGCN(256))\n",
    "\n",
    "        self.pool = tf.keras.layers.GlobalAveragePooling2D(data_format='channels_first')\n",
    "\n",
    "        self.logits = tf.keras.layers.Conv2D(num_classes,\n",
    "                                             kernel_size=1,\n",
    "                                             padding='same',\n",
    "                                             kernel_initializer=INITIALIZER,\n",
    "                                             data_format='channels_first',\n",
    "                                             kernel_regularizer=REGULARIZER)\n",
    "\n",
    "    def call(self, x, training):\n",
    "        N = tf.shape(x)[0]\n",
    "        C = tf.shape(x)[1]\n",
    "        T = tf.shape(x)[2]\n",
    "        V = tf.shape(x)[3]\n",
    "        M = tf.shape(x)[4]\n",
    "\n",
    "        x = tf.transpose(x, perm=[0, 4, 3, 1, 2])\n",
    "        x = tf.reshape(x, [N * M, V * C, T])\n",
    "        x = self.data_bn(x, training=training)\n",
    "        x = tf.reshape(x, [N, M, V, C, T])\n",
    "        x = tf.transpose(x, perm=[0, 1, 3, 4, 2])\n",
    "        x = tf.reshape(x, [N * M, C, T, V])\n",
    "\n",
    "        A = self.A\n",
    "        for layer in self.STGCN_layers:\n",
    "            x, A = layer(x, A, training=training)\n",
    "\n",
    "        # N*M,C,T,V\n",
    "        x = self.pool(x)\n",
    "        x = tf.reshape(x, [N, M, -1, 1, 1])\n",
    "        x = tf.reduce_mean(x, axis=1)\n",
    "        x = self.logits(x)\n",
    "        x = tf.reshape(x, [N, -1])\n",
    "        x = tf.nn.softmax(x,axis=-1)\n",
    "        print(x.shape)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-28T10:31:59.350200Z",
     "iopub.status.busy": "2023-04-28T10:31:59.349922Z",
     "iopub.status.idle": "2023-04-28T10:31:59.496392Z",
     "shell.execute_reply": "2023-04-28T10:31:59.495394Z",
     "shell.execute_reply.started": "2023-04-28T10:31:59.350174Z"
    }
   },
   "outputs": [],
   "source": [
    "model =  Model(num_classes=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-28T10:32:38.625175Z",
     "iopub.status.busy": "2023-04-28T10:32:38.624133Z",
     "iopub.status.idle": "2023-04-28T10:32:38.632021Z",
     "shell.execute_reply": "2023-04-28T10:32:38.630992Z",
     "shell.execute_reply.started": "2023-04-28T10:32:38.625127Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_model():\n",
    "          \n",
    "    model =  Model(num_classes=250)\n",
    "    model.build((128,3,20,61,1))\n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics=[\n",
    "            \"accuracy\",\n",
    "        ]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-28T10:33:10.431060Z",
     "iopub.status.busy": "2023-04-28T10:33:10.430388Z",
     "iopub.status.idle": "2023-04-28T10:33:11.862496Z",
     "shell.execute_reply": "2023-04-28T10:33:11.861593Z",
     "shell.execute_reply.started": "2023-04-28T10:33:10.431020Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "(128, 250)\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization_9 (Batc  multiple                 732       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " stgcn_3 (STGCN)             multiple                  38208     \n",
      "                                                                 \n",
      " stgcn_4 (STGCN)             multiple                  182400    \n",
      "                                                                 \n",
      " stgcn_5 (STGCN)             multiple                  725248    \n",
      "                                                                 \n",
      " global_average_pooling2d_1   multiple                 0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " conv2d_17 (Conv2D)          multiple                  64250     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,022,001\n",
      "Trainable params: 1,007,912\n",
      "Non-trainable params: 14,089\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/keras/initializers/initializers_v2.py:120: UserWarning: The initializer VarianceScaling is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-11T12:58:57.432016Z",
     "iopub.status.busy": "2023-04-11T12:58:57.431467Z",
     "iopub.status.idle": "2023-04-11T12:58:58.610898Z",
     "shell.execute_reply": "2023-04-11T12:58:58.609576Z",
     "shell.execute_reply.started": "2023-04-11T12:58:57.431981Z"
    },
    "papermill": {
     "duration": 3.974692,
     "end_time": "2023-03-02T08:44:46.689625",
     "exception": false,
     "start_time": "2023-03-02T08:44:42.714933",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# If True, processing data from scratch\n",
    "# If False, loads preprocessed data\n",
    "PREPROCESS_DATA = False\n",
    "TRAIN_MODEL = True\n",
    "# True: use 10% of participants as validation set\n",
    "# False: use all data for training -> gives better LB result\n",
    "USE_VAL = False\n",
    "N_ROWS = 543\n",
    "N_DIMS = 3\n",
    "DIM_NAMES = ['x', 'y', 'z']\n",
    "SEED = 42\n",
    "NUM_CLASSES = 250\n",
    "INPUT_SIZE = 64\n",
    "BATCH_ALL_SIGNS_N = 4\n",
    "BATCH_SIZE = 256\n",
    "N_EPOCHS = 100\n",
    "LR_MAX = 1e-3\n",
    "N_WARMUP_EPOCHS = 0\n",
    "WD_RATIO = 0.05\n",
    "MASK_VAL = 4237\n",
    "N_COLS = 61\n",
    "# Custom callback to update weight decay with learning rate\n",
    "class WeightDecayCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, wd_ratio=WD_RATIO):\n",
    "        self.step_counter = 0\n",
    "        self.wd_ratio = wd_ratio\n",
    "    \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        model.optimizer.weight_decay = model.optimizer.learning_rate * self.wd_ratio\n",
    "        print(f'learning rate: {model.optimizer.learning_rate.numpy():.2e}, weight decay: {model.optimizer.weight_decay.numpy():.2e}')\n",
    "\n",
    "def lrfn(current_step, num_warmup_steps, lr_max, num_cycles=0.50, num_training_steps=N_EPOCHS):\n",
    "    \n",
    "    if current_step < num_warmup_steps:\n",
    "        if WARMUP_METHOD == 'log':\n",
    "            return lr_max * 0.10 ** (num_warmup_steps - current_step)\n",
    "        else:\n",
    "            return lr_max * 2 ** -(num_warmup_steps - current_step)\n",
    "    else:\n",
    "        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n",
    "\n",
    "        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress))) * lr_max\n",
    "# Learning rate for encoder\n",
    "LR_SCHEDULE = [lrfn(step, num_warmup_steps=N_WARMUP_EPOCHS, lr_max=LR_MAX, num_cycles=0.50) for step in range(N_EPOCHS)]\n",
    "lr_callback = tf.keras.callbacks.LearningRateScheduler(lambda step: LR_SCHEDULE[step], verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization_9 (Batc  multiple                 732       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " stgcn_3 (STGCN)             multiple                  38208     \n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| sgcn_3 (SGCN)             multiple                  768       |\n",
      "||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯||\n",
      "|| conv2d_9 (Conv2D)       multiple                  768       ||\n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| sequential_5 (Sequential)  (128, 64, 20, 61)        37440     |\n",
      "||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯||\n",
      "|| batch_normalization_10 (Bat  (128, 64, 20, 61)    256       ||\n",
      "|| chNormalization)                                            ||\n",
      "||                                                             ||\n",
      "|| activation_6 (Activation)  (128, 64, 20, 61)      0         ||\n",
      "||                                                             ||\n",
      "|| conv2d_10 (Conv2D)      (128, 64, 20, 61)         36928     ||\n",
      "||                                                             ||\n",
      "|| batch_normalization_11 (Bat  (128, 64, 20, 61)    256       ||\n",
      "|| chNormalization)                                            ||\n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| activation_7 (Activation)  multiple                 0         |\n",
      "¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n",
      " stgcn_4 (STGCN)             multiple                  182400    \n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| sgcn_4 (SGCN)             multiple                  24960     |\n",
      "||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯||\n",
      "|| conv2d_11 (Conv2D)      multiple                  24960     ||\n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| sequential_6 (Sequential)  (128, 128, 10, 61)       148608    |\n",
      "||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯||\n",
      "|| batch_normalization_12 (Bat  (128, 128, 20, 61)   512       ||\n",
      "|| chNormalization)                                            ||\n",
      "||                                                             ||\n",
      "|| activation_8 (Activation)  (128, 128, 20, 61)     0         ||\n",
      "||                                                             ||\n",
      "|| conv2d_12 (Conv2D)      (128, 128, 10, 61)        147584    ||\n",
      "||                                                             ||\n",
      "|| batch_normalization_13 (Bat  (128, 128, 10, 61)   512       ||\n",
      "|| chNormalization)                                            ||\n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| activation_9 (Activation)  multiple                 0         |\n",
      "|                                                               |\n",
      "| sequential_7 (Sequential)  (128, 128, 10, 61)       8832      |\n",
      "||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯||\n",
      "|| conv2d_13 (Conv2D)      (128, 128, 10, 61)        8320      ||\n",
      "||                                                             ||\n",
      "|| batch_normalization_14 (Bat  (128, 128, 10, 61)   512       ||\n",
      "|| chNormalization)                                            ||\n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n",
      " stgcn_5 (STGCN)             multiple                  725248    \n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| sgcn_5 (SGCN)             multiple                  99072     |\n",
      "||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯||\n",
      "|| conv2d_14 (Conv2D)      multiple                  99072     ||\n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| sequential_8 (Sequential)  (128, 256, 5, 61)        592128    |\n",
      "||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯||\n",
      "|| batch_normalization_15 (Bat  (128, 256, 10, 61)   1024      ||\n",
      "|| chNormalization)                                            ||\n",
      "||                                                             ||\n",
      "|| activation_10 (Activation)  (128, 256, 10, 61)    0         ||\n",
      "||                                                             ||\n",
      "|| conv2d_15 (Conv2D)      (128, 256, 5, 61)         590080    ||\n",
      "||                                                             ||\n",
      "|| batch_normalization_16 (Bat  (128, 256, 5, 61)    1024      ||\n",
      "|| chNormalization)                                            ||\n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| activation_11 (Activation)  multiple                0         |\n",
      "|                                                               |\n",
      "| sequential_9 (Sequential)  (128, 256, 5, 61)        34048     |\n",
      "||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯||\n",
      "|| conv2d_16 (Conv2D)      (128, 256, 5, 61)         33024     ||\n",
      "||                                                             ||\n",
      "|| batch_normalization_17 (Bat  (128, 256, 5, 61)    1024      ||\n",
      "|| chNormalization)                                            ||\n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n",
      " global_average_pooling2d_1   multiple                 0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " conv2d_17 (Conv2D)          multiple                  64250     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,022,001\n",
      "Trainable params: 1,007,912\n",
      "Non-trainable params: 14,089\n",
      "_________________________________________________________________\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.001.\n",
      "learning rate: 1.00e-03, weight decay: 5.00e-05\n",
      "Epoch 1/100\n",
      "(128, 250)\n",
      "(128, 250)\n",
      "  5/590 [..............................] - ETA: 30s - loss: 8.0791 - accuracy: 0.0000e+00WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0041s vs `on_train_batch_end` time: 0.0392s). Check your callbacks.\n",
      "589/590 [============================>.] - ETA: 0s - loss: 5.4409 - accuracy: 0.1136(128, 250)\n",
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.08931, saving model to models/weights_stgcn4.h5\n",
      "590/590 [==============================] - 35s 53ms/step - loss: 5.4389 - accuracy: 0.1138 - val_loss: 5.1033 - val_accuracy: 0.0893 - lr: 0.0010\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.0009997532801828658.\n",
      "learning rate: 1.00e-03, weight decay: 5.00e-05\n",
      "Epoch 2/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 3.9065 - accuracy: 0.2865\n",
      "Epoch 2: val_accuracy improved from 0.08931 to 0.28328, saving model to models/weights_stgcn4.h5\n",
      "590/590 [==============================] - 31s 52ms/step - loss: 3.9059 - accuracy: 0.2866 - val_loss: 3.8811 - val_accuracy: 0.2833 - lr: 9.9975e-04\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.0009990133642141358.\n",
      "learning rate: 9.99e-04, weight decay: 5.00e-05\n",
      "Epoch 3/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 3.3778 - accuracy: 0.4012\n",
      "Epoch 3: val_accuracy improved from 0.28328 to 0.37869, saving model to models/weights_stgcn4.h5\n",
      "590/590 [==============================] - 31s 52ms/step - loss: 3.3778 - accuracy: 0.4012 - val_loss: 3.4695 - val_accuracy: 0.3787 - lr: 9.9901e-04\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.00099778098230154.\n",
      "learning rate: 9.98e-04, weight decay: 4.99e-05\n",
      "Epoch 4/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 3.0826 - accuracy: 0.4733\n",
      "Epoch 4: val_accuracy improved from 0.37869 to 0.43413, saving model to models/weights_stgcn4.h5\n",
      "590/590 [==============================] - 31s 52ms/step - loss: 3.0826 - accuracy: 0.4733 - val_loss: 3.2281 - val_accuracy: 0.4341 - lr: 9.9778e-04\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.000996057350657239.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate: 9.96e-04, weight decay: 4.98e-05\n",
      "Epoch 5/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.8969 - accuracy: 0.5188\n",
      "Epoch 5: val_accuracy improved from 0.43413 to 0.49283, saving model to models/weights_stgcn4.h5\n",
      "590/590 [==============================] - 31s 52ms/step - loss: 2.8968 - accuracy: 0.5188 - val_loss: 2.9949 - val_accuracy: 0.4928 - lr: 9.9606e-04\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.0009938441702975688.\n",
      "learning rate: 9.94e-04, weight decay: 4.97e-05\n",
      "Epoch 6/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 2.7738 - accuracy: 0.5517\n",
      "Epoch 6: val_accuracy improved from 0.49283 to 0.50615, saving model to models/weights_stgcn4.h5\n",
      "590/590 [==============================] - 31s 52ms/step - loss: 2.7738 - accuracy: 0.5517 - val_loss: 2.9445 - val_accuracy: 0.5062 - lr: 9.9384e-04\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.0009911436253643444.\n",
      "learning rate: 9.91e-04, weight decay: 4.96e-05\n",
      "Epoch 7/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.6813 - accuracy: 0.5748\n",
      "Epoch 7: val_accuracy did not improve from 0.50615\n",
      "590/590 [==============================] - 31s 52ms/step - loss: 2.6810 - accuracy: 0.5749 - val_loss: 3.0892 - val_accuracy: 0.4781 - lr: 9.9114e-04\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 0.0009879583809693738.\n",
      "learning rate: 9.88e-04, weight decay: 4.94e-05\n",
      "Epoch 8/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.6076 - accuracy: 0.5917\n",
      "Epoch 8: val_accuracy improved from 0.50615 to 0.53542, saving model to models/weights_stgcn4.h5\n",
      "590/590 [==============================] - 31s 52ms/step - loss: 2.6073 - accuracy: 0.5918 - val_loss: 2.8154 - val_accuracy: 0.5354 - lr: 9.8796e-04\n",
      "\n",
      "Epoch 9: LearningRateScheduler setting learning rate to 0.0009842915805643156.\n",
      "learning rate: 9.84e-04, weight decay: 4.92e-05\n",
      "Epoch 9/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.5526 - accuracy: 0.6100\n",
      "Epoch 9: val_accuracy improved from 0.53542 to 0.58685, saving model to models/weights_stgcn4.h5\n",
      "590/590 [==============================] - 31s 52ms/step - loss: 2.5521 - accuracy: 0.6101 - val_loss: 2.6787 - val_accuracy: 0.5868 - lr: 9.8429e-04\n",
      "\n",
      "Epoch 10: LearningRateScheduler setting learning rate to 0.0009801468428384716.\n",
      "learning rate: 9.80e-04, weight decay: 4.90e-05\n",
      "Epoch 10/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.5085 - accuracy: 0.6219\n",
      "Epoch 10: val_accuracy did not improve from 0.58685\n",
      "590/590 [==============================] - 31s 52ms/step - loss: 2.5088 - accuracy: 0.6218 - val_loss: 2.8587 - val_accuracy: 0.5339 - lr: 9.8015e-04\n",
      "\n",
      "Epoch 11: LearningRateScheduler setting learning rate to 0.0009755282581475768.\n",
      "learning rate: 9.76e-04, weight decay: 4.88e-05\n",
      "Epoch 11/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 2.4753 - accuracy: 0.6339\n",
      "Epoch 11: val_accuracy did not improve from 0.58685\n",
      "590/590 [==============================] - 31s 52ms/step - loss: 2.4753 - accuracy: 0.6339 - val_loss: 2.6898 - val_accuracy: 0.5811 - lr: 9.7553e-04\n",
      "\n",
      "Epoch 12: LearningRateScheduler setting learning rate to 0.0009704403844771128.\n",
      "learning rate: 9.70e-04, weight decay: 4.85e-05\n",
      "Epoch 12/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 2.4480 - accuracy: 0.6416\n",
      "Epoch 12: val_accuracy improved from 0.58685 to 0.60499, saving model to models/weights_stgcn4.h5\n",
      "590/590 [==============================] - 33s 56ms/step - loss: 2.4480 - accuracy: 0.6416 - val_loss: 2.6069 - val_accuracy: 0.6050 - lr: 9.7044e-04\n",
      "\n",
      "Epoch 13: LearningRateScheduler setting learning rate to 0.0009648882429441257.\n",
      "learning rate: 9.65e-04, weight decay: 4.82e-05\n",
      "Epoch 13/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.4170 - accuracy: 0.6490\n",
      "Epoch 13: val_accuracy did not improve from 0.60499\n",
      "590/590 [==============================] - 33s 57ms/step - loss: 2.4171 - accuracy: 0.6490 - val_loss: 2.8040 - val_accuracy: 0.5540 - lr: 9.6489e-04\n",
      "\n",
      "Epoch 14: LearningRateScheduler setting learning rate to 0.0009588773128419905.\n",
      "learning rate: 9.59e-04, weight decay: 4.79e-05\n",
      "Epoch 14/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 2.4018 - accuracy: 0.6568\n",
      "Epoch 14: val_accuracy did not improve from 0.60499\n",
      "590/590 [==============================] - 31s 52ms/step - loss: 2.4018 - accuracy: 0.6568 - val_loss: 2.6208 - val_accuracy: 0.6034 - lr: 9.5888e-04\n",
      "\n",
      "Epoch 15: LearningRateScheduler setting learning rate to 0.0009524135262330098.\n",
      "learning rate: 9.52e-04, weight decay: 4.76e-05\n",
      "Epoch 15/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 2.3804 - accuracy: 0.6633\n",
      "Epoch 15: val_accuracy improved from 0.60499 to 0.61264, saving model to models/weights_stgcn4.h5\n",
      "590/590 [==============================] - 31s 53ms/step - loss: 2.3804 - accuracy: 0.6633 - val_loss: 2.5739 - val_accuracy: 0.6126 - lr: 9.5241e-04\n",
      "\n",
      "Epoch 16: LearningRateScheduler setting learning rate to 0.0009455032620941839.\n",
      "learning rate: 9.46e-04, weight decay: 4.73e-05\n",
      "Epoch 16/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 2.3638 - accuracy: 0.6695\n",
      "Epoch 16: val_accuracy improved from 0.61264 to 0.61927, saving model to models/weights_stgcn4.h5\n",
      "590/590 [==============================] - 32s 54ms/step - loss: 2.3638 - accuracy: 0.6695 - val_loss: 2.5661 - val_accuracy: 0.6193 - lr: 9.4550e-04\n",
      "\n",
      "Epoch 17: LearningRateScheduler setting learning rate to 0.0009381533400219318.\n",
      "learning rate: 9.38e-04, weight decay: 4.69e-05\n",
      "Epoch 17/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 2.3462 - accuracy: 0.6756\n",
      "Epoch 17: val_accuracy did not improve from 0.61927\n",
      "590/590 [==============================] - 32s 55ms/step - loss: 2.3462 - accuracy: 0.6756 - val_loss: 2.6339 - val_accuracy: 0.6121 - lr: 9.3815e-04\n",
      "\n",
      "Epoch 18: LearningRateScheduler setting learning rate to 0.0009303710135019718.\n",
      "learning rate: 9.30e-04, weight decay: 4.65e-05\n",
      "Epoch 18/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 2.3333 - accuracy: 0.6815\n",
      "Epoch 18: val_accuracy did not improve from 0.61927\n",
      "590/590 [==============================] - 31s 53ms/step - loss: 2.3333 - accuracy: 0.6815 - val_loss: 2.6050 - val_accuracy: 0.6173 - lr: 9.3037e-04\n",
      "\n",
      "Epoch 19: LearningRateScheduler setting learning rate to 0.0009221639627510075.\n",
      "learning rate: 9.22e-04, weight decay: 4.61e-05\n",
      "Epoch 19/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 2.3216 - accuracy: 0.6856\n",
      "Epoch 19: val_accuracy did not improve from 0.61927\n",
      "590/590 [==============================] - 31s 52ms/step - loss: 2.3216 - accuracy: 0.6856 - val_loss: 2.7814 - val_accuracy: 0.5741 - lr: 9.2216e-04\n",
      "\n",
      "Epoch 20: LearningRateScheduler setting learning rate to 0.0009135402871372809.\n",
      "learning rate: 9.14e-04, weight decay: 4.57e-05\n",
      "Epoch 20/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.3103 - accuracy: 0.6915\n",
      "Epoch 20: val_accuracy did not improve from 0.61927\n",
      "590/590 [==============================] - 31s 52ms/step - loss: 2.3102 - accuracy: 0.6915 - val_loss: 2.6897 - val_accuracy: 0.6030 - lr: 9.1354e-04\n",
      "\n",
      "Epoch 21: LearningRateScheduler setting learning rate to 0.0009045084971874737.\n",
      "learning rate: 9.05e-04, weight decay: 4.52e-05\n",
      "Epoch 21/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 2.2997 - accuracy: 0.6959\n",
      "Epoch 21: val_accuracy did not improve from 0.61927\n",
      "590/590 [==============================] - 31s 52ms/step - loss: 2.2997 - accuracy: 0.6959 - val_loss: 2.7790 - val_accuracy: 0.5737 - lr: 9.0451e-04\n",
      "\n",
      "Epoch 22: LearningRateScheduler setting learning rate to 0.0008950775061878452.\n",
      "learning rate: 8.95e-04, weight decay: 4.48e-05\n",
      "Epoch 22/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.2869 - accuracy: 0.6991\n",
      "Epoch 22: val_accuracy improved from 0.61927 to 0.63420, saving model to models/weights_stgcn4.h5\n",
      "590/590 [==============================] - 31s 52ms/step - loss: 2.2868 - accuracy: 0.6991 - val_loss: 2.5523 - val_accuracy: 0.6342 - lr: 8.9508e-04\n",
      "\n",
      "Epoch 23: LearningRateScheduler setting learning rate to 0.0008852566213878947.\n",
      "learning rate: 8.85e-04, weight decay: 4.43e-05\n",
      "Epoch 23/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "590/590 [==============================] - ETA: 0s - loss: 2.2784 - accuracy: 0.7042\n",
      "Epoch 23: val_accuracy improved from 0.63420 to 0.63431, saving model to models/weights_stgcn4.h5\n",
      "590/590 [==============================] - 31s 53ms/step - loss: 2.2784 - accuracy: 0.7042 - val_loss: 2.5950 - val_accuracy: 0.6343 - lr: 8.8526e-04\n",
      "\n",
      "Epoch 24: LearningRateScheduler setting learning rate to 0.0008750555348152298.\n",
      "learning rate: 8.75e-04, weight decay: 4.38e-05\n",
      "Epoch 24/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 2.2681 - accuracy: 0.7098\n",
      "Epoch 24: val_accuracy improved from 0.63431 to 0.64426, saving model to models/weights_stgcn4.h5\n",
      "590/590 [==============================] - 31s 52ms/step - loss: 2.2681 - accuracy: 0.7098 - val_loss: 2.5215 - val_accuracy: 0.6443 - lr: 8.7506e-04\n",
      "\n",
      "Epoch 25: LearningRateScheduler setting learning rate to 0.0008644843137107057.\n",
      "learning rate: 8.64e-04, weight decay: 4.32e-05\n",
      "Epoch 25/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.2669 - accuracy: 0.7116\n",
      "Epoch 25: val_accuracy did not improve from 0.64426\n",
      "590/590 [==============================] - 31s 52ms/step - loss: 2.2666 - accuracy: 0.7116 - val_loss: 2.5651 - val_accuracy: 0.6318 - lr: 8.6448e-04\n",
      "\n",
      "Epoch 26: LearningRateScheduler setting learning rate to 0.0008535533905932737.\n",
      "learning rate: 8.54e-04, weight decay: 4.27e-05\n",
      "Epoch 26/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 2.2539 - accuracy: 0.7180\n",
      "Epoch 26: val_accuracy improved from 0.64426 to 0.64796, saving model to models/weights_stgcn4.h5\n",
      "590/590 [==============================] - 31s 52ms/step - loss: 2.2539 - accuracy: 0.7180 - val_loss: 2.5200 - val_accuracy: 0.6480 - lr: 8.5355e-04\n",
      "\n",
      "Epoch 27: LearningRateScheduler setting learning rate to 0.0008422735529643444.\n",
      "learning rate: 8.42e-04, weight decay: 4.21e-05\n",
      "Epoch 27/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 2.2444 - accuracy: 0.7206\n",
      "Epoch 27: val_accuracy did not improve from 0.64796\n",
      "590/590 [==============================] - 31s 52ms/step - loss: 2.2444 - accuracy: 0.7206 - val_loss: 2.5664 - val_accuracy: 0.6442 - lr: 8.4227e-04\n",
      "\n",
      "Epoch 28: LearningRateScheduler setting learning rate to 0.0008306559326618259.\n",
      "learning rate: 8.31e-04, weight decay: 4.15e-05\n",
      "Epoch 28/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 2.2388 - accuracy: 0.7236\n",
      "Epoch 28: val_accuracy did not improve from 0.64796\n",
      "590/590 [==============================] - 31s 52ms/step - loss: 2.2388 - accuracy: 0.7236 - val_loss: 2.5932 - val_accuracy: 0.6443 - lr: 8.3066e-04\n",
      "\n",
      "Epoch 29: LearningRateScheduler setting learning rate to 0.0008187119948743449.\n",
      "learning rate: 8.19e-04, weight decay: 4.09e-05\n",
      "Epoch 29/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.2386 - accuracy: 0.7272\n",
      "Epoch 29: val_accuracy did not improve from 0.64796\n",
      "590/590 [==============================] - 31s 52ms/step - loss: 2.2386 - accuracy: 0.7271 - val_loss: 2.6033 - val_accuracy: 0.6427 - lr: 8.1871e-04\n",
      "\n",
      "Epoch 30: LearningRateScheduler setting learning rate to 0.0008064535268264883.\n",
      "learning rate: 8.06e-04, weight decay: 4.03e-05\n",
      "Epoch 30/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 2.2281 - accuracy: 0.7314\n",
      "Epoch 30: val_accuracy did not improve from 0.64796\n",
      "590/590 [==============================] - 31s 52ms/step - loss: 2.2281 - accuracy: 0.7314 - val_loss: 2.6001 - val_accuracy: 0.6411 - lr: 8.0645e-04\n",
      "\n",
      "Epoch 31: LearningRateScheduler setting learning rate to 0.0007938926261462366.\n",
      "learning rate: 7.94e-04, weight decay: 3.97e-05\n",
      "Epoch 31/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 2.2221 - accuracy: 0.7352\n",
      "Epoch 31: val_accuracy improved from 0.64796 to 0.67418, saving model to models/weights_stgcn4.h5\n",
      "590/590 [==============================] - 31s 52ms/step - loss: 2.2221 - accuracy: 0.7352 - val_loss: 2.4889 - val_accuracy: 0.6742 - lr: 7.9389e-04\n",
      "\n",
      "Epoch 32: LearningRateScheduler setting learning rate to 0.0007810416889260653.\n",
      "learning rate: 7.81e-04, weight decay: 3.91e-05\n",
      "Epoch 32/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.2188 - accuracy: 0.7383\n",
      "Epoch 32: val_accuracy did not improve from 0.67418\n",
      "590/590 [==============================] - 31s 52ms/step - loss: 2.2194 - accuracy: 0.7381 - val_loss: 2.6663 - val_accuracy: 0.6324 - lr: 7.8104e-04\n",
      "\n",
      "Epoch 33: LearningRateScheduler setting learning rate to 0.0007679133974894983.\n",
      "learning rate: 7.68e-04, weight decay: 3.84e-05\n",
      "Epoch 33/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.2136 - accuracy: 0.7414\n",
      "Epoch 33: val_accuracy did not improve from 0.67418\n",
      "590/590 [==============================] - 31s 52ms/step - loss: 2.2139 - accuracy: 0.7414 - val_loss: 2.5514 - val_accuracy: 0.6566 - lr: 7.6791e-04\n",
      "\n",
      "Epoch 34: LearningRateScheduler setting learning rate to 0.0007545207078751857.\n",
      "learning rate: 7.55e-04, weight decay: 3.77e-05\n",
      "Epoch 34/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.2054 - accuracy: 0.7466\n",
      "Epoch 34: val_accuracy improved from 0.67418 to 0.68745, saving model to models/weights_stgcn4.h5\n",
      "590/590 [==============================] - 31s 52ms/step - loss: 2.2059 - accuracy: 0.7465 - val_loss: 2.4673 - val_accuracy: 0.6874 - lr: 7.5452e-04\n",
      "\n",
      "Epoch 35: LearningRateScheduler setting learning rate to 0.0007408768370508576.\n",
      "learning rate: 7.41e-04, weight decay: 3.70e-05\n",
      "Epoch 35/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 2.1988 - accuracy: 0.7509\n",
      "Epoch 35: val_accuracy did not improve from 0.68745\n",
      "590/590 [==============================] - 31s 52ms/step - loss: 2.1988 - accuracy: 0.7509 - val_loss: 2.5819 - val_accuracy: 0.6532 - lr: 7.4088e-04\n",
      "\n",
      "Epoch 36: LearningRateScheduler setting learning rate to 0.0007269952498697733.\n",
      "learning rate: 7.27e-04, weight decay: 3.63e-05\n",
      "Epoch 36/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.1954 - accuracy: 0.7524\n",
      "Epoch 36: val_accuracy did not improve from 0.68745\n",
      "590/590 [==============================] - 31s 52ms/step - loss: 2.1955 - accuracy: 0.7524 - val_loss: 2.5461 - val_accuracy: 0.6716 - lr: 7.2700e-04\n",
      "\n",
      "Epoch 37: LearningRateScheduler setting learning rate to 0.0007128896457825364.\n",
      "learning rate: 7.13e-04, weight decay: 3.56e-05\n",
      "Epoch 37/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 2.1919 - accuracy: 0.7573\n",
      "Epoch 37: val_accuracy improved from 0.68745 to 0.69526, saving model to models/weights_stgcn4.h5\n",
      "590/590 [==============================] - 31s 52ms/step - loss: 2.1919 - accuracy: 0.7573 - val_loss: 2.4595 - val_accuracy: 0.6953 - lr: 7.1289e-04\n",
      "\n",
      "Epoch 38: LearningRateScheduler setting learning rate to 0.0006985739453173903.\n",
      "learning rate: 6.99e-04, weight decay: 3.49e-05\n",
      "Epoch 38/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.1903 - accuracy: 0.7604\n",
      "Epoch 38: val_accuracy did not improve from 0.69526\n",
      "590/590 [==============================] - 31s 52ms/step - loss: 2.1900 - accuracy: 0.7604 - val_loss: 2.5002 - val_accuracy: 0.6892 - lr: 6.9857e-04\n",
      "\n",
      "Epoch 39: LearningRateScheduler setting learning rate to 0.0006840622763423391.\n",
      "learning rate: 6.84e-04, weight decay: 3.42e-05\n",
      "Epoch 39/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.1861 - accuracy: 0.7654\n",
      "Epoch 39: val_accuracy did not improve from 0.69526\n",
      "590/590 [==============================] - 31s 52ms/step - loss: 2.1862 - accuracy: 0.7654 - val_loss: 2.4842 - val_accuracy: 0.6899 - lr: 6.8406e-04\n",
      "\n",
      "Epoch 40: LearningRateScheduler setting learning rate to 0.0006693689601226458.\n",
      "learning rate: 6.69e-04, weight decay: 3.35e-05\n",
      "Epoch 40/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.1810 - accuracy: 0.7659\n",
      "Epoch 40: val_accuracy did not improve from 0.69526\n",
      "590/590 [==============================] - 31s 52ms/step - loss: 2.1806 - accuracy: 0.7660 - val_loss: 2.4864 - val_accuracy: 0.6944 - lr: 6.6937e-04\n",
      "\n",
      "Epoch 41: LearningRateScheduler setting learning rate to 0.0006545084971874737.\n",
      "learning rate: 6.55e-04, weight decay: 3.27e-05\n",
      "Epoch 41/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.1759 - accuracy: 0.7689\n",
      "Epoch 41: val_accuracy improved from 0.69526 to 0.70120, saving model to models/weights_stgcn4.h5\n",
      "590/590 [==============================] - 31s 52ms/step - loss: 2.1759 - accuracy: 0.7689 - val_loss: 2.4864 - val_accuracy: 0.7012 - lr: 6.5451e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 42: LearningRateScheduler setting learning rate to 0.0006394955530196147.\n",
      "learning rate: 6.39e-04, weight decay: 3.20e-05\n",
      "Epoch 42/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.1712 - accuracy: 0.7736\n",
      "Epoch 42: val_accuracy did not improve from 0.70120\n",
      "590/590 [==============================] - 31s 52ms/step - loss: 2.1709 - accuracy: 0.7737 - val_loss: 2.5513 - val_accuracy: 0.6850 - lr: 6.3950e-04\n",
      "\n",
      "Epoch 43: LearningRateScheduler setting learning rate to 0.0006243449435824276.\n",
      "learning rate: 6.24e-04, weight decay: 3.12e-05\n",
      "Epoch 43/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.1669 - accuracy: 0.7774\n",
      "Epoch 43: val_accuracy improved from 0.70120 to 0.71046, saving model to models/weights_stgcn4.h5\n",
      "590/590 [==============================] - 31s 52ms/step - loss: 2.1673 - accuracy: 0.7774 - val_loss: 2.4540 - val_accuracy: 0.7105 - lr: 6.2434e-04\n",
      "\n",
      "Epoch 44: LearningRateScheduler setting learning rate to 0.0006090716206982714.\n",
      "learning rate: 6.09e-04, weight decay: 3.05e-05\n",
      "Epoch 44/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 2.1587 - accuracy: 0.7811\n",
      "Epoch 44: val_accuracy did not improve from 0.71046\n",
      "590/590 [==============================] - 31s 52ms/step - loss: 2.1587 - accuracy: 0.7811 - val_loss: 2.5171 - val_accuracy: 0.6947 - lr: 6.0907e-04\n",
      "\n",
      "Epoch 45: LearningRateScheduler setting learning rate to 0.0005936906572928624.\n",
      "learning rate: 5.94e-04, weight decay: 2.97e-05\n",
      "Epoch 45/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 2.1541 - accuracy: 0.7833\n",
      "Epoch 45: val_accuracy did not improve from 0.71046\n",
      "590/590 [==============================] - 31s 52ms/step - loss: 2.1541 - accuracy: 0.7833 - val_loss: 2.6840 - val_accuracy: 0.6568 - lr: 5.9369e-04\n",
      "\n",
      "Epoch 46: LearningRateScheduler setting learning rate to 0.0005782172325201155.\n",
      "learning rate: 5.78e-04, weight decay: 2.89e-05\n",
      "Epoch 46/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.1520 - accuracy: 0.7890\n",
      "Epoch 46: val_accuracy did not improve from 0.71046\n",
      "590/590 [==============================] - 31s 52ms/step - loss: 2.1520 - accuracy: 0.7890 - val_loss: 2.5271 - val_accuracy: 0.6956 - lr: 5.7822e-04\n",
      "\n",
      "Epoch 47: LearningRateScheduler setting learning rate to 0.0005626666167821521.\n",
      "learning rate: 5.63e-04, weight decay: 2.81e-05\n",
      "Epoch 47/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.1466 - accuracy: 0.7933\n",
      "Epoch 47: val_accuracy did not improve from 0.71046\n",
      "590/590 [==============================] - 31s 52ms/step - loss: 2.1467 - accuracy: 0.7932 - val_loss: 2.5333 - val_accuracy: 0.6985 - lr: 5.6267e-04\n",
      "\n",
      "Epoch 48: LearningRateScheduler setting learning rate to 0.0005470541566592572.\n",
      "learning rate: 5.47e-04, weight decay: 2.74e-05\n",
      "Epoch 48/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 2.1388 - accuracy: 0.7946\n",
      "Epoch 48: val_accuracy did not improve from 0.71046\n",
      "590/590 [==============================] - 31s 52ms/step - loss: 2.1388 - accuracy: 0.7946 - val_loss: 2.5353 - val_accuracy: 0.7016 - lr: 5.4705e-04\n",
      "\n",
      "Epoch 49: LearningRateScheduler setting learning rate to 0.0005313952597646568.\n",
      "learning rate: 5.31e-04, weight decay: 2.66e-05\n",
      "Epoch 49/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 2.1303 - accuracy: 0.7993\n",
      "Epoch 49: val_accuracy improved from 0.71046 to 0.71848, saving model to models/weights_stgcn4.h5\n",
      "590/590 [==============================] - 31s 52ms/step - loss: 2.1303 - accuracy: 0.7993 - val_loss: 2.4706 - val_accuracy: 0.7185 - lr: 5.3140e-04\n",
      "\n",
      "Epoch 50: LearningRateScheduler setting learning rate to 0.0005157053795390641.\n",
      "learning rate: 5.16e-04, weight decay: 2.58e-05\n",
      "Epoch 50/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.1244 - accuracy: 0.8041\n",
      "Epoch 50: val_accuracy did not improve from 0.71848\n",
      "590/590 [==============================] - 31s 52ms/step - loss: 2.1246 - accuracy: 0.8041 - val_loss: 2.4892 - val_accuracy: 0.7156 - lr: 5.1571e-04\n",
      "\n",
      "Epoch 51: LearningRateScheduler setting learning rate to 0.0005.\n",
      "learning rate: 5.00e-04, weight decay: 2.50e-05\n",
      "Epoch 51/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.1167 - accuracy: 0.8091\n",
      "Epoch 51: val_accuracy did not improve from 0.71848\n",
      "590/590 [==============================] - 31s 52ms/step - loss: 2.1168 - accuracy: 0.8091 - val_loss: 2.5862 - val_accuracy: 0.6891 - lr: 5.0000e-04\n",
      "\n",
      "Epoch 52: LearningRateScheduler setting learning rate to 0.00048429462046093585.\n",
      "learning rate: 4.84e-04, weight decay: 2.42e-05\n",
      "Epoch 52/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 2.1120 - accuracy: 0.8133\n",
      "Epoch 52: val_accuracy improved from 0.71848 to 0.72560, saving model to models/weights_stgcn4.h5\n",
      "590/590 [==============================] - 31s 52ms/step - loss: 2.1120 - accuracy: 0.8133 - val_loss: 2.4743 - val_accuracy: 0.7256 - lr: 4.8429e-04\n",
      "\n",
      "Epoch 53: LearningRateScheduler setting learning rate to 0.0004686047402353433.\n",
      "learning rate: 4.69e-04, weight decay: 2.34e-05\n",
      "Epoch 53/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.1051 - accuracy: 0.8172\n",
      "Epoch 53: val_accuracy did not improve from 0.72560\n",
      "590/590 [==============================] - 31s 52ms/step - loss: 2.1054 - accuracy: 0.8171 - val_loss: 2.4819 - val_accuracy: 0.7216 - lr: 4.6860e-04\n",
      "\n",
      "Epoch 54: LearningRateScheduler setting learning rate to 0.00045294584334074284.\n",
      "learning rate: 4.53e-04, weight decay: 2.26e-05\n",
      "Epoch 54/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 2.0962 - accuracy: 0.8209\n",
      "Epoch 54: val_accuracy did not improve from 0.72560\n",
      "590/590 [==============================] - 34s 58ms/step - loss: 2.0962 - accuracy: 0.8209 - val_loss: 2.5429 - val_accuracy: 0.7118 - lr: 4.5295e-04\n",
      "\n",
      "Epoch 55: LearningRateScheduler setting learning rate to 0.00043733338321784784.\n",
      "learning rate: 4.37e-04, weight decay: 2.19e-05\n",
      "Epoch 55/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 2.0943 - accuracy: 0.8249\n",
      "Epoch 55: val_accuracy did not improve from 0.72560\n",
      "590/590 [==============================] - 34s 58ms/step - loss: 2.0943 - accuracy: 0.8249 - val_loss: 2.5486 - val_accuracy: 0.7111 - lr: 4.3733e-04\n",
      "\n",
      "Epoch 56: LearningRateScheduler setting learning rate to 0.0004217827674798845.\n",
      "learning rate: 4.22e-04, weight decay: 2.11e-05\n",
      "Epoch 56/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 2.0882 - accuracy: 0.8282\n",
      "Epoch 56: val_accuracy did not improve from 0.72560\n",
      "590/590 [==============================] - 34s 58ms/step - loss: 2.0882 - accuracy: 0.8282 - val_loss: 2.5342 - val_accuracy: 0.7212 - lr: 4.2178e-04\n",
      "\n",
      "Epoch 57: LearningRateScheduler setting learning rate to 0.0004063093427071376.\n",
      "learning rate: 4.06e-04, weight decay: 2.03e-05\n",
      "Epoch 57/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 2.0792 - accuracy: 0.8352\n",
      "Epoch 57: val_accuracy improved from 0.72560 to 0.73721, saving model to models/weights_stgcn4.h5\n",
      "590/590 [==============================] - 34s 58ms/step - loss: 2.0792 - accuracy: 0.8352 - val_loss: 2.4681 - val_accuracy: 0.7372 - lr: 4.0631e-04\n",
      "\n",
      "Epoch 58: LearningRateScheduler setting learning rate to 0.0003909283793017289.\n",
      "learning rate: 3.91e-04, weight decay: 1.95e-05\n",
      "Epoch 58/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 2.0758 - accuracy: 0.8373\n",
      "Epoch 58: val_accuracy did not improve from 0.73721\n",
      "590/590 [==============================] - 34s 58ms/step - loss: 2.0758 - accuracy: 0.8373 - val_loss: 2.5057 - val_accuracy: 0.7319 - lr: 3.9093e-04\n",
      "\n",
      "Epoch 59: LearningRateScheduler setting learning rate to 0.0003756550564175727.\n",
      "learning rate: 3.76e-04, weight decay: 1.88e-05\n",
      "Epoch 59/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 2.0689 - accuracy: 0.8430\n",
      "Epoch 59: val_accuracy did not improve from 0.73721\n",
      "590/590 [==============================] - 35s 59ms/step - loss: 2.0689 - accuracy: 0.8430 - val_loss: 2.5434 - val_accuracy: 0.7211 - lr: 3.7566e-04\n",
      "\n",
      "Epoch 60: LearningRateScheduler setting learning rate to 0.0003605044469803854.\n",
      "learning rate: 3.61e-04, weight decay: 1.80e-05\n",
      "Epoch 60/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 2.0580 - accuracy: 0.8477\n",
      "Epoch 60: val_accuracy did not improve from 0.73721\n",
      "590/590 [==============================] - 34s 57ms/step - loss: 2.0580 - accuracy: 0.8477 - val_loss: 2.5066 - val_accuracy: 0.7353 - lr: 3.6050e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 61: LearningRateScheduler setting learning rate to 0.00034549150281252633.\n",
      "learning rate: 3.45e-04, weight decay: 1.73e-05\n",
      "Epoch 61/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 2.0516 - accuracy: 0.8515\n",
      "Epoch 61: val_accuracy did not improve from 0.73721\n",
      "590/590 [==============================] - 34s 58ms/step - loss: 2.0516 - accuracy: 0.8515 - val_loss: 2.5236 - val_accuracy: 0.7354 - lr: 3.4549e-04\n",
      "\n",
      "Epoch 62: LearningRateScheduler setting learning rate to 0.0003306310398773543.\n",
      "learning rate: 3.31e-04, weight decay: 1.65e-05\n",
      "Epoch 62/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 2.0432 - accuracy: 0.8566\n",
      "Epoch 62: val_accuracy did not improve from 0.73721\n",
      "590/590 [==============================] - 34s 58ms/step - loss: 2.0432 - accuracy: 0.8566 - val_loss: 2.5377 - val_accuracy: 0.7289 - lr: 3.3063e-04\n",
      "\n",
      "Epoch 63: LearningRateScheduler setting learning rate to 0.00031593772365766105.\n",
      "learning rate: 3.16e-04, weight decay: 1.58e-05\n",
      "Epoch 63/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 2.0386 - accuracy: 0.8605\n",
      "Epoch 63: val_accuracy did not improve from 0.73721\n",
      "590/590 [==============================] - 34s 58ms/step - loss: 2.0386 - accuracy: 0.8605 - val_loss: 2.5503 - val_accuracy: 0.7362 - lr: 3.1594e-04\n",
      "\n",
      "Epoch 64: LearningRateScheduler setting learning rate to 0.00030142605468260977.\n",
      "learning rate: 3.01e-04, weight decay: 1.51e-05\n",
      "Epoch 64/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 2.0339 - accuracy: 0.8642\n",
      "Epoch 64: val_accuracy improved from 0.73721 to 0.73775, saving model to models/weights_stgcn4.h5\n",
      "590/590 [==============================] - 34s 57ms/step - loss: 2.0339 - accuracy: 0.8642 - val_loss: 2.5322 - val_accuracy: 0.7377 - lr: 3.0143e-04\n",
      "\n",
      "Epoch 65: LearningRateScheduler setting learning rate to 0.00028711035421746366.\n",
      "learning rate: 2.87e-04, weight decay: 1.44e-05\n",
      "Epoch 65/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 2.0244 - accuracy: 0.8702\n",
      "Epoch 65: val_accuracy improved from 0.73775 to 0.75054, saving model to models/weights_stgcn4.h5\n",
      "590/590 [==============================] - 34s 58ms/step - loss: 2.0244 - accuracy: 0.8702 - val_loss: 2.4929 - val_accuracy: 0.7505 - lr: 2.8711e-04\n",
      "\n",
      "Epoch 66: LearningRateScheduler setting learning rate to 0.00027300475013022663.\n",
      "learning rate: 2.73e-04, weight decay: 1.37e-05\n",
      "Epoch 66/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 2.0188 - accuracy: 0.8749\n",
      "Epoch 66: val_accuracy did not improve from 0.75054\n",
      "590/590 [==============================] - 34s 58ms/step - loss: 2.0188 - accuracy: 0.8749 - val_loss: 2.5378 - val_accuracy: 0.7392 - lr: 2.7300e-04\n",
      "\n",
      "Epoch 67: LearningRateScheduler setting learning rate to 0.0002591231629491423.\n",
      "learning rate: 2.59e-04, weight decay: 1.30e-05\n",
      "Epoch 67/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 2.0098 - accuracy: 0.8802\n",
      "Epoch 67: val_accuracy improved from 0.75054 to 0.76257, saving model to models/weights_stgcn4.h5\n",
      "590/590 [==============================] - 31s 53ms/step - loss: 2.0098 - accuracy: 0.8802 - val_loss: 2.4716 - val_accuracy: 0.7626 - lr: 2.5912e-04\n",
      "\n",
      "Epoch 68: LearningRateScheduler setting learning rate to 0.00024547929212481435.\n",
      "learning rate: 2.45e-04, weight decay: 1.23e-05\n",
      "Epoch 68/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 2.0026 - accuracy: 0.8852\n",
      "Epoch 68: val_accuracy did not improve from 0.76257\n",
      "590/590 [==============================] - 34s 57ms/step - loss: 2.0026 - accuracy: 0.8852 - val_loss: 2.5981 - val_accuracy: 0.7321 - lr: 2.4548e-04\n",
      "\n",
      "Epoch 69: LearningRateScheduler setting learning rate to 0.00023208660251050156.\n",
      "learning rate: 2.32e-04, weight decay: 1.16e-05\n",
      "Epoch 69/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 1.9971 - accuracy: 0.8894\n",
      "Epoch 69: val_accuracy did not improve from 0.76257\n",
      "590/590 [==============================] - 34s 58ms/step - loss: 1.9971 - accuracy: 0.8894 - val_loss: 2.5012 - val_accuracy: 0.7572 - lr: 2.3209e-04\n",
      "\n",
      "Epoch 70: LearningRateScheduler setting learning rate to 0.0002189583110739348.\n",
      "learning rate: 2.19e-04, weight decay: 1.09e-05\n",
      "Epoch 70/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 1.9916 - accuracy: 0.8942\n",
      "Epoch 70: val_accuracy did not improve from 0.76257\n",
      "590/590 [==============================] - 34s 58ms/step - loss: 1.9916 - accuracy: 0.8942 - val_loss: 2.5052 - val_accuracy: 0.7564 - lr: 2.1896e-04\n",
      "\n",
      "Epoch 71: LearningRateScheduler setting learning rate to 0.00020610737385376348.\n",
      "learning rate: 2.06e-04, weight decay: 1.03e-05\n",
      "Epoch 71/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 1.9795 - accuracy: 0.8986\n",
      "Epoch 71: val_accuracy did not improve from 0.76257\n",
      "590/590 [==============================] - 34s 58ms/step - loss: 1.9795 - accuracy: 0.8986 - val_loss: 2.4991 - val_accuracy: 0.7617 - lr: 2.0611e-04\n",
      "\n",
      "Epoch 72: LearningRateScheduler setting learning rate to 0.00019354647317351188.\n",
      "learning rate: 1.94e-04, weight decay: 9.68e-06\n",
      "Epoch 72/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 1.9732 - accuracy: 0.9034\n",
      "Epoch 72: val_accuracy did not improve from 0.76257\n",
      "590/590 [==============================] - 34s 58ms/step - loss: 1.9732 - accuracy: 0.9034 - val_loss: 2.5183 - val_accuracy: 0.7598 - lr: 1.9355e-04\n",
      "\n",
      "Epoch 73: LearningRateScheduler setting learning rate to 0.00018128800512565513.\n",
      "learning rate: 1.81e-04, weight decay: 9.06e-06\n",
      "Epoch 73/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 1.9668 - accuracy: 0.9079\n",
      "Epoch 73: val_accuracy did not improve from 0.76257\n",
      "590/590 [==============================] - 34s 58ms/step - loss: 1.9668 - accuracy: 0.9079 - val_loss: 2.5181 - val_accuracy: 0.7623 - lr: 1.8129e-04\n",
      "\n",
      "Epoch 74: LearningRateScheduler setting learning rate to 0.00016934406733817414.\n",
      "learning rate: 1.69e-04, weight decay: 8.47e-06\n",
      "Epoch 74/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 1.9578 - accuracy: 0.9143\n",
      "Epoch 74: val_accuracy improved from 0.76257 to 0.76579, saving model to models/weights_stgcn4.h5\n",
      "590/590 [==============================] - 34s 58ms/step - loss: 1.9578 - accuracy: 0.9143 - val_loss: 2.5154 - val_accuracy: 0.7658 - lr: 1.6934e-04\n",
      "\n",
      "Epoch 75: LearningRateScheduler setting learning rate to 0.00015772644703565563.\n",
      "learning rate: 1.58e-04, weight decay: 7.89e-06\n",
      "Epoch 75/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 1.9537 - accuracy: 0.9176\n",
      "Epoch 75: val_accuracy improved from 0.76579 to 0.76905, saving model to models/weights_stgcn4.h5\n",
      "590/590 [==============================] - 34s 58ms/step - loss: 1.9537 - accuracy: 0.9176 - val_loss: 2.5103 - val_accuracy: 0.7690 - lr: 1.5773e-04\n",
      "\n",
      "Epoch 76: LearningRateScheduler setting learning rate to 0.00014644660940672628.\n",
      "learning rate: 1.46e-04, weight decay: 7.32e-06\n",
      "Epoch 76/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 1.9454 - accuracy: 0.9209\n",
      "Epoch 76: val_accuracy did not improve from 0.76905\n",
      "590/590 [==============================] - 34s 58ms/step - loss: 1.9454 - accuracy: 0.9209 - val_loss: 2.5100 - val_accuracy: 0.7686 - lr: 1.4645e-04\n",
      "\n",
      "Epoch 77: LearningRateScheduler setting learning rate to 0.00013551568628929433.\n",
      "learning rate: 1.36e-04, weight decay: 6.78e-06\n",
      "Epoch 77/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 1.9397 - accuracy: 0.9263\n",
      "Epoch 77: val_accuracy improved from 0.76905 to 0.76910, saving model to models/weights_stgcn4.h5\n",
      "590/590 [==============================] - 34s 58ms/step - loss: 1.9397 - accuracy: 0.9263 - val_loss: 2.5154 - val_accuracy: 0.7691 - lr: 1.3552e-04\n",
      "\n",
      "Epoch 78: LearningRateScheduler setting learning rate to 0.0001249444651847702.\n",
      "learning rate: 1.25e-04, weight decay: 6.25e-06\n",
      "Epoch 78/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 1.9352 - accuracy: 0.9298\n",
      "Epoch 78: val_accuracy improved from 0.76910 to 0.77371, saving model to models/weights_stgcn4.h5\n",
      "590/590 [==============================] - 34s 58ms/step - loss: 1.9352 - accuracy: 0.9298 - val_loss: 2.5065 - val_accuracy: 0.7737 - lr: 1.2494e-04\n",
      "\n",
      "Epoch 79: LearningRateScheduler setting learning rate to 0.00011474337861210544.\n",
      "learning rate: 1.15e-04, weight decay: 5.74e-06\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "590/590 [==============================] - ETA: 0s - loss: 1.9291 - accuracy: 0.9334\n",
      "Epoch 79: val_accuracy did not improve from 0.77371\n",
      "590/590 [==============================] - 34s 58ms/step - loss: 1.9291 - accuracy: 0.9334 - val_loss: 2.5300 - val_accuracy: 0.7708 - lr: 1.1474e-04\n",
      "\n",
      "Epoch 80: LearningRateScheduler setting learning rate to 0.00010492249381215479.\n",
      "learning rate: 1.05e-04, weight decay: 5.25e-06\n",
      "Epoch 80/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 1.9241 - accuracy: 0.9367\n",
      "Epoch 80: val_accuracy did not improve from 0.77371\n",
      "590/590 [==============================] - 34s 58ms/step - loss: 1.9241 - accuracy: 0.9367 - val_loss: 2.5206 - val_accuracy: 0.7727 - lr: 1.0492e-04\n",
      "\n",
      "Epoch 81: LearningRateScheduler setting learning rate to 9.549150281252633e-05.\n",
      "learning rate: 9.55e-05, weight decay: 4.77e-06\n",
      "Epoch 81/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 1.9180 - accuracy: 0.9415\n",
      "Epoch 81: val_accuracy improved from 0.77371 to 0.77868, saving model to models/weights_stgcn4.h5\n",
      "590/590 [==============================] - 34s 58ms/step - loss: 1.9180 - accuracy: 0.9415 - val_loss: 2.5124 - val_accuracy: 0.7787 - lr: 9.5492e-05\n",
      "\n",
      "Epoch 82: LearningRateScheduler setting learning rate to 8.645971286271903e-05.\n",
      "learning rate: 8.65e-05, weight decay: 4.32e-06\n",
      "Epoch 82/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 1.9158 - accuracy: 0.9436\n",
      "Epoch 82: val_accuracy improved from 0.77868 to 0.77879, saving model to models/weights_stgcn4.h5\n",
      "590/590 [==============================] - 34s 58ms/step - loss: 1.9158 - accuracy: 0.9436 - val_loss: 2.5298 - val_accuracy: 0.7788 - lr: 8.6460e-05\n",
      "\n",
      "Epoch 83: LearningRateScheduler setting learning rate to 7.783603724899258e-05.\n",
      "learning rate: 7.78e-05, weight decay: 3.89e-06\n",
      "Epoch 83/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 1.9090 - accuracy: 0.9488\n",
      "Epoch 83: val_accuracy improved from 0.77879 to 0.78168, saving model to models/weights_stgcn4.h5\n",
      "590/590 [==============================] - 34s 58ms/step - loss: 1.9090 - accuracy: 0.9488 - val_loss: 2.5168 - val_accuracy: 0.7817 - lr: 7.7836e-05\n",
      "\n",
      "Epoch 84: LearningRateScheduler setting learning rate to 6.962898649802824e-05.\n",
      "learning rate: 6.96e-05, weight decay: 3.48e-06\n",
      "Epoch 84/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 1.9043 - accuracy: 0.9501\n",
      "Epoch 84: val_accuracy did not improve from 0.78168\n",
      "590/590 [==============================] - 34s 57ms/step - loss: 1.9043 - accuracy: 0.9501 - val_loss: 2.5196 - val_accuracy: 0.7792 - lr: 6.9629e-05\n",
      "\n",
      "Epoch 85: LearningRateScheduler setting learning rate to 6.184665997806832e-05.\n",
      "learning rate: 6.18e-05, weight decay: 3.09e-06\n",
      "Epoch 85/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 1.8997 - accuracy: 0.9527\n",
      "Epoch 85: val_accuracy did not improve from 0.78168\n",
      "590/590 [==============================] - 34s 58ms/step - loss: 1.8997 - accuracy: 0.9527 - val_loss: 2.5263 - val_accuracy: 0.7807 - lr: 6.1847e-05\n",
      "\n",
      "Epoch 86: LearningRateScheduler setting learning rate to 5.449673790581611e-05.\n",
      "learning rate: 5.45e-05, weight decay: 2.72e-06\n",
      "Epoch 86/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 1.8941 - accuracy: 0.9556\n",
      "Epoch 86: val_accuracy did not improve from 0.78168\n",
      "590/590 [==============================] - 34s 58ms/step - loss: 1.8941 - accuracy: 0.9556 - val_loss: 2.5255 - val_accuracy: 0.7804 - lr: 5.4497e-05\n",
      "\n",
      "Epoch 87: LearningRateScheduler setting learning rate to 4.758647376699032e-05.\n",
      "learning rate: 4.76e-05, weight decay: 2.38e-06\n",
      "Epoch 87/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 1.8892 - accuracy: 0.9590\n",
      "Epoch 87: val_accuracy improved from 0.78168 to 0.78264, saving model to models/weights_stgcn4.h5\n",
      "590/590 [==============================] - 34s 58ms/step - loss: 1.8892 - accuracy: 0.9590 - val_loss: 2.5216 - val_accuracy: 0.7826 - lr: 4.7586e-05\n",
      "\n",
      "Epoch 88: LearningRateScheduler setting learning rate to 4.112268715800943e-05.\n",
      "learning rate: 4.11e-05, weight decay: 2.06e-06\n",
      "Epoch 88/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 1.8849 - accuracy: 0.9617\n",
      "Epoch 88: val_accuracy did not improve from 0.78264\n",
      "590/590 [==============================] - 32s 54ms/step - loss: 1.8849 - accuracy: 0.9617 - val_loss: 2.5246 - val_accuracy: 0.7816 - lr: 4.1123e-05\n",
      "\n",
      "Epoch 89: LearningRateScheduler setting learning rate to 3.5111757055874326e-05.\n",
      "learning rate: 3.51e-05, weight decay: 1.76e-06\n",
      "Epoch 89/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 1.8816 - accuracy: 0.9626\n",
      "Epoch 89: val_accuracy did not improve from 0.78264\n",
      "590/590 [==============================] - 31s 52ms/step - loss: 1.8816 - accuracy: 0.9626 - val_loss: 2.5258 - val_accuracy: 0.7824 - lr: 3.5112e-05\n",
      "\n",
      "Epoch 90: LearningRateScheduler setting learning rate to 2.9559615522887274e-05.\n",
      "learning rate: 2.96e-05, weight decay: 1.48e-06\n",
      "Epoch 90/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 1.8790 - accuracy: 0.9645\n",
      "Epoch 90: val_accuracy improved from 0.78264 to 0.78286, saving model to models/weights_stgcn4.h5\n",
      "590/590 [==============================] - 31s 52ms/step - loss: 1.8790 - accuracy: 0.9645 - val_loss: 2.5257 - val_accuracy: 0.7829 - lr: 2.9560e-05\n",
      "\n",
      "Epoch 91: LearningRateScheduler setting learning rate to 2.4471741852423235e-05.\n",
      "learning rate: 2.45e-05, weight decay: 1.22e-06\n",
      "Epoch 91/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 1.8750 - accuracy: 0.9671\n",
      "Epoch 91: val_accuracy improved from 0.78286 to 0.78360, saving model to models/weights_stgcn4.h5\n",
      "590/590 [==============================] - 31s 52ms/step - loss: 1.8751 - accuracy: 0.9671 - val_loss: 2.5265 - val_accuracy: 0.7836 - lr: 2.4472e-05\n",
      "\n",
      "Epoch 92: LearningRateScheduler setting learning rate to 1.985315716152847e-05.\n",
      "learning rate: 1.99e-05, weight decay: 9.93e-07\n",
      "Epoch 92/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 1.8701 - accuracy: 0.9692\n",
      "Epoch 92: val_accuracy improved from 0.78360 to 0.78366, saving model to models/weights_stgcn4.h5\n",
      "590/590 [==============================] - 31s 52ms/step - loss: 1.8701 - accuracy: 0.9692 - val_loss: 2.5248 - val_accuracy: 0.7837 - lr: 1.9853e-05\n",
      "\n",
      "Epoch 93: LearningRateScheduler setting learning rate to 1.5708419435684463e-05.\n",
      "learning rate: 1.57e-05, weight decay: 7.85e-07\n",
      "Epoch 93/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 1.8701 - accuracy: 0.9690\n",
      "Epoch 93: val_accuracy improved from 0.78366 to 0.78526, saving model to models/weights_stgcn4.h5\n",
      "590/590 [==============================] - 31s 52ms/step - loss: 1.8701 - accuracy: 0.9690 - val_loss: 2.5258 - val_accuracy: 0.7853 - lr: 1.5708e-05\n",
      "\n",
      "Epoch 94: LearningRateScheduler setting learning rate to 1.2041619030626282e-05.\n",
      "learning rate: 1.20e-05, weight decay: 6.02e-07\n",
      "Epoch 94/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 1.8666 - accuracy: 0.9708\n",
      "Epoch 94: val_accuracy did not improve from 0.78526\n",
      "590/590 [==============================] - 31s 52ms/step - loss: 1.8665 - accuracy: 0.9708 - val_loss: 2.5271 - val_accuracy: 0.7842 - lr: 1.2042e-05\n",
      "\n",
      "Epoch 95: LearningRateScheduler setting learning rate to 8.856374635655695e-06.\n",
      "learning rate: 8.86e-06, weight decay: 4.43e-07\n",
      "Epoch 95/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 1.8638 - accuracy: 0.9716\n",
      "Epoch 95: val_accuracy did not improve from 0.78526\n",
      "590/590 [==============================] - 31s 52ms/step - loss: 1.8638 - accuracy: 0.9716 - val_loss: 2.5280 - val_accuracy: 0.7842 - lr: 8.8564e-06\n",
      "\n",
      "Epoch 96: LearningRateScheduler setting learning rate to 6.15582970243117e-06.\n",
      "learning rate: 6.16e-06, weight decay: 3.08e-07\n",
      "Epoch 96/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 1.8610 - accuracy: 0.9724\n",
      "Epoch 96: val_accuracy improved from 0.78526 to 0.78574, saving model to models/weights_stgcn4.h5\n",
      "590/590 [==============================] - 31s 52ms/step - loss: 1.8610 - accuracy: 0.9724 - val_loss: 2.5268 - val_accuracy: 0.7857 - lr: 6.1558e-06\n",
      "\n",
      "Epoch 97: LearningRateScheduler setting learning rate to 3.942649342761117e-06.\n",
      "learning rate: 3.94e-06, weight decay: 1.97e-07\n",
      "Epoch 97/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "590/590 [==============================] - ETA: 0s - loss: 1.8597 - accuracy: 0.9729\n",
      "Epoch 97: val_accuracy did not improve from 0.78574\n",
      "590/590 [==============================] - 32s 53ms/step - loss: 1.8597 - accuracy: 0.9729 - val_loss: 2.5274 - val_accuracy: 0.7853 - lr: 3.9426e-06\n",
      "\n",
      "Epoch 98: LearningRateScheduler setting learning rate to 2.219017698460002e-06.\n",
      "learning rate: 2.22e-06, weight decay: 1.11e-07\n",
      "Epoch 98/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 1.8579 - accuracy: 0.9739\n",
      "Epoch 98: val_accuracy did not improve from 0.78574\n",
      "590/590 [==============================] - 32s 54ms/step - loss: 1.8579 - accuracy: 0.9740 - val_loss: 2.5264 - val_accuracy: 0.7853 - lr: 2.2190e-06\n",
      "\n",
      "Epoch 99: LearningRateScheduler setting learning rate to 9.866357858642206e-07.\n",
      "learning rate: 9.87e-07, weight decay: 4.93e-08\n",
      "Epoch 99/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 1.8564 - accuracy: 0.9744\n",
      "Epoch 99: val_accuracy did not improve from 0.78574\n",
      "590/590 [==============================] - 31s 52ms/step - loss: 1.8564 - accuracy: 0.9744 - val_loss: 2.5265 - val_accuracy: 0.7850 - lr: 9.8664e-07\n",
      "\n",
      "Epoch 100: LearningRateScheduler setting learning rate to 2.467198171342e-07.\n",
      "learning rate: 2.47e-07, weight decay: 1.23e-08\n",
      "Epoch 100/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 1.8560 - accuracy: 0.9744\n",
      "Epoch 100: val_accuracy did not improve from 0.78574\n",
      "590/590 [==============================] - 31s 53ms/step - loss: 1.8560 - accuracy: 0.9744 - val_loss: 2.5263 - val_accuracy: 0.7854 - lr: 2.4672e-07\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Get new fresh model\n",
    "file_name = 'models/weights_stgcn4.h5'\n",
    "#model = tf.keras.models.load_model('models/041423_21_02.h5')\n",
    "\n",
    "# Sanity Check\n",
    "model.summary(expand_nested=True)\n",
    "\n",
    "# Actual Training\n",
    "history = model.fit(\n",
    "        x=x_train,\n",
    "        y=y_train,\n",
    "        epochs=100,\n",
    "        # Only used for validation data since training data is a generator\n",
    "        batch_size=128,\n",
    "        validation_data=(x_test,y_test),\n",
    "        callbacks=[\n",
    "            tf.keras.callbacks.ModelCheckpoint(\n",
    "            file_name,\n",
    "            save_weights_only = True,\n",
    "            save_best_only=True, \n",
    "            monitor=\"val_accuracy\",\n",
    "            mode=\"max\",\n",
    "            verbose = 1),\n",
    "            lr_callback,\n",
    "            WeightDecayCallback(),\n",
    "        ],\n",
    "        verbose = 1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-11T13:19:17.068946Z",
     "iopub.status.busy": "2023-04-11T13:19:17.067991Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_ds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [46], line 12\u001b[0m\n\u001b[1;32m      7\u001b[0m loss_fn\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mSparseCategoricalCrossentropy()\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m30\u001b[39m): \u001b[38;5;66;03m# <----- start for loop, step 1\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \n\u001b[1;32m     10\u001b[0m   \u001b[38;5;66;03m# <-------- start for loop, step 2\u001b[39;00m\n\u001b[1;32m     11\u001b[0m   \u001b[38;5;66;03m# Iterate over the batches of the dataset.\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m step, (x_batch_train, y_batch_train) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mtrain_ds\u001b[49m):\n\u001b[1;32m     13\u001b[0m \n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# <-------- start gradient tape scope, step 3\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# Open a GradientTape to record the operations run\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# during the forward pass, which enables auto-differentiation.\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[1;32m     18\u001b[0m \n\u001b[1;32m     19\u001b[0m        \u001b[38;5;66;03m# Run the forward pass of the layer.\u001b[39;00m\n\u001b[1;32m     20\u001b[0m        \u001b[38;5;66;03m# The operations that the layer applies\u001b[39;00m\n\u001b[1;32m     21\u001b[0m        \u001b[38;5;66;03m# to its inputs are going to be recorded\u001b[39;00m\n\u001b[1;32m     22\u001b[0m        \u001b[38;5;66;03m# on the GradientTape.\u001b[39;00m\n\u001b[1;32m     23\u001b[0m        logits \u001b[38;5;241m=\u001b[39m model(x_batch_train, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_ds' is not defined"
     ]
    }
   ],
   "source": [
    "# https://towardsdatascience.com/model-sub-classing-and-custom-training-loop-from-scratch-in-tensorflow-2-cc1d4f10fb4e\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "train_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
    "val_acc_metric   = tf.keras.metrics.CategoricalAccuracy()\n",
    "# Instantiate a loss function\n",
    "# loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "loss_fn=tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "for epoch in range(30): # <----- start for loop, step 1\n",
    "\n",
    "  # <-------- start for loop, step 2\n",
    "  # Iterate over the batches of the dataset.\n",
    "  for step, (x_batch_train, y_batch_train) in enumerate(train_ds):\n",
    "\n",
    "    # <-------- start gradient tape scope, step 3\n",
    "    # Open a GradientTape to record the operations run\n",
    "    # during the forward pass, which enables auto-differentiation.\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "       # Run the forward pass of the layer.\n",
    "       # The operations that the layer applies\n",
    "       # to its inputs are going to be recorded\n",
    "       # on the GradientTape.\n",
    "       logits = model(x_batch_train, training=True) \n",
    "\n",
    "       # Compute the loss value for this minibatch.\n",
    "       loss_value = loss_fn(y_batch_train, logits)  \n",
    "       print(loss_value )\n",
    "\n",
    "    # compute the gradient of weights w.r.t. loss  <-------- step 5\n",
    "    # Use the gradient tape to automatically retrieve\n",
    "    # the gradients of the trainable variables with respect to the loss.\n",
    "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "\n",
    "    # update the weight based on gradient  <---------- step 6\n",
    "    # Run one step of gradient descent by updating\n",
    "    # the value of the variables to minimize the loss.\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    train_acc_metric.update_state(y_batch_train, logits)\n",
    "    print(train_acc_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 20, 61, 75520, 1)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-11T12:58:58.615002Z",
     "iopub.status.busy": "2023-04-11T12:58:58.614684Z",
     "iopub.status.idle": "2023-04-11T13:00:08.746327Z",
     "shell.execute_reply": "2023-04-11T13:00:08.744036Z",
     "shell.execute_reply.started": "2023-04-11T12:58:58.614967Z"
    }
   },
   "outputs": [],
   "source": [
    "if CFG.is_training:\n",
    "    file_name = \"model.h5\"\n",
    "#     callbacks = [\n",
    "#         tf.keras.callbacks.ModelCheckpoint(\n",
    "#             file_name, \n",
    "#             save_best_only=True, \n",
    "#             restore_best_weights=True, \n",
    "#             monitor=\"val_accuracy\",\n",
    "#             mode=\"max\"\n",
    "#         ),\n",
    "#         tf.keras.callbacks.EarlyStopping(\n",
    "#             patience=5, \n",
    "#             monitor=\"val_accuracy\",\n",
    "#             mode=\"max\"\n",
    "#         )\n",
    "#     ]\n",
    "    model.fit(train_ds, epochs=1, validation_data=valid_ds)\n",
    "    model.save('/kaggle/input/islr-convlstm1d/model.h5',save_format='tf')\n",
    "    model = tf.keras.models.load_model(file_name)\n",
    "# else:\n",
    "#     model = tf.keras.models.load_model(\"/kaggle/input/islr-convlstm1d/model.h5\")\n",
    "model.evaluate(valid_ds)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.069654,
     "end_time": "2023-03-02T08:47:03.916954",
     "exception": false,
     "start_time": "2023-03-02T08:47:03.8473",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Create Model for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-11T13:00:08.747883Z",
     "iopub.status.idle": "2023-04-11T13:00:08.748454Z",
     "shell.execute_reply": "2023-04-11T13:00:08.748205Z",
     "shell.execute_reply.started": "2023-04-11T13:00:08.748177Z"
    },
    "papermill": {
     "duration": 0.086334,
     "end_time": "2023-03-02T08:47:04.072776",
     "exception": false,
     "start_time": "2023-03-02T08:47:03.986442",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def get_inference_model(model):\n",
    "#     inputs = tf.keras.Input((543, 3), dtype=tf.float32, name=\"inputs\")\n",
    "#     vector = tf.image.resize(inputs, (CFG.sequence_length, 543))\n",
    "#     vector = tf.where(tf.math.is_nan(vector), tf.zeros_like(vector), vector)\n",
    "#     vector = tf.expand_dims(vector, axis=0)\n",
    "#     vector = model(vector)\n",
    "#     output = tf.keras.layers.Activation(activation=\"linear\", name=\"outputs\")(vector)\n",
    "#     inference_model = tf.keras.Model(inputs=inputs, outputs=output) \n",
    "#     inference_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics=[\"accuracy\"])\n",
    "#     return inference_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-11T13:00:08.750352Z",
     "iopub.status.idle": "2023-04-11T13:00:08.750858Z",
     "shell.execute_reply": "2023-04-11T13:00:08.750633Z",
     "shell.execute_reply.started": "2023-04-11T13:00:08.750603Z"
    },
    "papermill": {
     "duration": 5.194577,
     "end_time": "2023-03-02T08:47:09.336507",
     "exception": false,
     "start_time": "2023-03-02T08:47:04.14193",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# inference_model = get_inference_model(model)\n",
    "# inference_model.summary()\n",
    "# tf.keras.utils.plot_model(inference_model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.071953,
     "end_time": "2023-03-02T08:47:09.784691",
     "exception": false,
     "start_time": "2023-03-02T08:47:09.712738",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Create submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-11T13:00:08.752458Z",
     "iopub.status.idle": "2023-04-11T13:00:08.758015Z",
     "shell.execute_reply": "2023-04-11T13:00:08.757844Z",
     "shell.execute_reply.started": "2023-04-11T13:00:08.757823Z"
    },
    "papermill": {
     "duration": 153.429402,
     "end_time": "2023-03-02T08:49:43.286145",
     "exception": false,
     "start_time": "2023-03-02T08:47:09.856743",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# converter = tf.lite.TFLiteConverter.from_keras_model(inference_model)\n",
    "# tflite_model = converter.convert()\n",
    "# model_path = \"model.tflite\"\n",
    "# # Save the model.\n",
    "# with open(model_path, 'wb') as f:\n",
    "#     f.write(tflite_model)\n",
    "# !zip submission.zip $model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.1124,
     "end_time": "2023-03-02T08:49:46.762554",
     "exception": false,
     "start_time": "2023-03-02T08:49:46.650154",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-11T13:00:08.759302Z",
     "iopub.status.idle": "2023-04-11T13:00:08.759928Z",
     "shell.execute_reply": "2023-04-11T13:00:08.759646Z",
     "shell.execute_reply.started": "2023-04-11T13:00:08.759618Z"
    },
    "papermill": {
     "duration": 12.663338,
     "end_time": "2023-03-02T08:49:59.538473",
     "exception": false,
     "start_time": "2023-03-02T08:49:46.875135",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install tflite-runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-11T13:00:08.761722Z",
     "iopub.status.idle": "2023-04-11T13:00:08.762695Z",
     "shell.execute_reply": "2023-04-11T13:00:08.762418Z",
     "shell.execute_reply.started": "2023-04-11T13:00:08.762388Z"
    },
    "papermill": {
     "duration": 29.201279,
     "end_time": "2023-03-02T08:50:28.816671",
     "exception": false,
     "start_time": "2023-03-02T08:49:59.615392",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import tflite_runtime.interpreter as tflite\n",
    "# interpreter = tflite.Interpreter(model_path)\n",
    "# found_signatures = list(interpreter.get_signature_list().keys())\n",
    "# prediction_fn = interpreter.get_signature_runner(\"serving_default\")\n",
    "# for i in tqdm(range(10000)):\n",
    "#     frames = load_relevant_data_subset(f'/kaggle/input/asl-signs/{train.iloc[i].path}')\n",
    "#     output = prediction_fn(inputs=frames)\n",
    "#     if i < 100:\n",
    "#         sign = np.argmax(output[\"outputs\"])\n",
    "#         print(f\"Predicted label: {index_label[sign]}, Actual Label: {train.iloc[i].sign}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
