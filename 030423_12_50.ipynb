{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import glob\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import tensorflow_io as tfio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_it_all(seed=7):\n",
    "    \"\"\" Attempt to be Reproducible \"\"\"\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "seed_it_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    left_ROWS_per_frame = 21\n",
    "    sequence_length = 20\n",
    "    batch_size = 32\n",
    "\n",
    "labels  = json.load(open('sign_to_prediction_index_map.json','r'))\n",
    "complete_df = pd.read_csv('train.csv')\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(complete_df, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_loader(with_labels=True):\n",
    "    def load_video(video_path):\n",
    "        #print('herer')\n",
    "        video_df = tfio.IODataset.from_parquet(video_path)\n",
    "        #video_df = pd.read_parquet(video_path, engine='pyarrow')\n",
    "        #video_df.fillna(0,inplace=True)\n",
    "        left_df = video_df[video_df.type=='left_hand']\n",
    "        left_values = left_df[['x','y','z']].values\n",
    "        left_values = left_values.reshape(-1,CFG.left_ROWS_per_frame,3)\n",
    "        left_hand_array =  tf.image.resize(left_values, (CFG.sequence_length, CFG.left_ROWS_per_frame))\n",
    "        right_df = video_df[video_df.type=='right_hand']\n",
    "        right_values = right_df[['x','y','z']].values\n",
    "        right_values = right_values.reshape(-1,CFG.left_ROWS_per_frame,3)\n",
    "        right_hand_array =  tf.image.resize(right_values, (CFG.sequence_length, CFG.left_ROWS_per_frame))\n",
    "        return [left_hand_array, right_hand_array]\n",
    "    \n",
    "    def load_video_with_labels(path, label):\n",
    "        return load_video(path), labels[label]\n",
    "    \n",
    "    return load_video_with_labels if with_labels else load_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomData(tf.keras.utils.Sequence):\n",
    "    def __init__(self,df,num_frames=20,batch_size=8,shuffle=True,\\\n",
    "                 labels_path='sign_to_prediction_index_map.json'):\n",
    "        self.df = df\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.num_frames = num_frames\n",
    "        self.labels  = json.load(open('sign_to_prediction_index_map.json','r'))\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.df))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        batches = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        left_hand_input = np.zeros(shape=(self.batch_size,self.num_frames,CFG.left_ROWS_per_frame,3))\n",
    "        right_hand_input = np.zeros(shape=(self.batch_size,self.num_frames,CFG.left_ROWS_per_frame,3))\n",
    "        labels = []\n",
    "        for i,row_val in enumerate(batches):\n",
    "            row = self.df.iloc[row_val]\n",
    "            left_hand,right_hand = self.load_video(row['path'])\n",
    "            left_hand_input[i,:] = left_hand\n",
    "            right_hand_input[i,:] = right_hand\n",
    "            labels.append(self.labels[row['sign']])\n",
    "        return [left_hand_input,right_hand_input],np.asarray(labels)\n",
    "            \n",
    "    def load_video(self,video_path):\n",
    "        video_df = pd.read_parquet(video_path, engine='pyarrow')\n",
    "        video_df.dropna(inplace=True)\n",
    "        left_df = video_df[video_df.type=='left_hand']\n",
    "        left_values = left_df[['x','y','z']].values\n",
    "        left_values = left_values.reshape(-1,CFG.left_ROWS_per_frame,3)\n",
    "        if len(left_values)!=0:\n",
    "            left_hand_array =  tf.image.resize(left_values, (CFG.sequence_length, CFG.left_ROWS_per_frame))\n",
    "        else:\n",
    "            left_hand_array =  tf.zeros(shape=(CFG.sequence_length, CFG.left_ROWS_per_frame,3),dtype=tf.float32)\n",
    "        \n",
    "        right_df = video_df[video_df.type=='right_hand']\n",
    "        right_values = right_df[['x','y','z']].values\n",
    "        right_values = right_values.reshape(-1,CFG.left_ROWS_per_frame,3)\n",
    "        if len(right_values) != 0:\n",
    "            right_hand_array =  tf.image.resize(right_values, (CFG.sequence_length, CFG.left_ROWS_per_frame))\n",
    "        else:\n",
    "            right_hand_array =  tf.zeros(shape=(CFG.sequence_length, CFG.left_ROWS_per_frame,3),dtype=tf.float32)\n",
    "        return left_hand_array, right_hand_array\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)//self.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = CustomData(train_df,num_frames=CFG.sequence_length,batch_size=CFG.batch_size)\n",
    "test_datagen = CustomData(test_df,num_frames=CFG.sequence_length,batch_size=CFG.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv1d_lstm_block(inputs, filters):\n",
    "    vector = tf.keras.layers.ConvLSTM1D(filters=32, kernel_size=5)(inputs)\n",
    "    vector = tf.keras.layers.Dropout(0.1)(vector)\n",
    "    vector = tf.keras.layers.ConvLSTM1D(filters=64, kernel_size=5)(inputs)\n",
    "    vector = tf.keras.layers.Dropout(0.1)(vector)\n",
    "    return vector\n",
    "\n",
    "def get_model():\n",
    "    input1 = tf.keras.Input((CFG.sequence_length, CFG.left_ROWS_per_frame, 3), dtype=tf.float32)\n",
    "    input2 = tf.keras.Input((CFG.sequence_length, CFG.left_ROWS_per_frame, 3), dtype=tf.float32)\n",
    "    left_hand_vector = conv1d_lstm_block(input1, [64])\n",
    "    right_hand_vector = conv1d_lstm_block(input2, [64])\n",
    "    vector = tf.keras.layers.Concatenate(axis=1)([left_hand_vector, right_hand_vector])\n",
    "    vector = tf.keras.layers.Flatten()(vector)\n",
    "    output = tf.keras.layers.Dense(250, activation=\"softmax\")(vector)\n",
    "    model = tf.keras.Model(inputs=[input1,input2], outputs=output)\n",
    "    model.compile(tf.keras.optimizers.Adam(0.000333), \"sparse_categorical_crossentropy\", metrics=\"accuracy\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 20, 21, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 20, 21, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " conv_lstm1d_1 (ConvLSTM1D)     (None, 17, 64)       86016       ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv_lstm1d_3 (ConvLSTM1D)     (None, 17, 64)       86016       ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 17, 64)       0           ['conv_lstm1d_1[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 17, 64)       0           ['conv_lstm1d_3[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 34, 64)       0           ['dropout_1[0][0]',              \n",
      "                                                                  'dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 2176)         0           ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 250)          544250      ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 716,282\n",
      "Trainable params: 716,282\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 3.7721 - accuracy: 0.1959\n",
      "Epoch 1: val_accuracy improved from -inf to 0.35037, saving model to models/030423_12_50.h5\n",
      "2361/2361 [==============================] - 1129s 476ms/step - loss: 3.7721 - accuracy: 0.1959 - val_loss: 2.8099 - val_accuracy: 0.3504 - lr: 3.3300e-04\n",
      "Epoch 2/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 2.5196 - accuracy: 0.4004\n",
      "Epoch 2: val_accuracy improved from 0.35037 to 0.44354, saving model to models/030423_12_50.h5\n",
      "2361/2361 [==============================] - 1114s 472ms/step - loss: 2.5196 - accuracy: 0.4004 - val_loss: 2.3328 - val_accuracy: 0.4435 - lr: 3.3300e-04\n",
      "Epoch 3/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 2.1190 - accuracy: 0.4825\n",
      "Epoch 3: val_accuracy improved from 0.44354 to 0.49163, saving model to models/030423_12_50.h5\n",
      "2361/2361 [==============================] - 1111s 471ms/step - loss: 2.1190 - accuracy: 0.4825 - val_loss: 2.1059 - val_accuracy: 0.4916 - lr: 3.3300e-04\n",
      "Epoch 4/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 1.8819 - accuracy: 0.5331\n",
      "Epoch 4: val_accuracy improved from 0.49163 to 0.51224, saving model to models/030423_12_50.h5\n",
      "2361/2361 [==============================] - 1115s 472ms/step - loss: 1.8819 - accuracy: 0.5331 - val_loss: 2.0213 - val_accuracy: 0.5122 - lr: 3.3300e-04\n",
      "Epoch 5/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 1.7057 - accuracy: 0.5705\n",
      "Epoch 5: val_accuracy improved from 0.51224 to 0.54094, saving model to models/030423_12_50.h5\n",
      "2361/2361 [==============================] - 1111s 471ms/step - loss: 1.7057 - accuracy: 0.5705 - val_loss: 1.8925 - val_accuracy: 0.5409 - lr: 3.3300e-04\n",
      "Epoch 6/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 1.5600 - accuracy: 0.6013\n",
      "Epoch 6: val_accuracy improved from 0.54094 to 0.54666, saving model to models/030423_12_50.h5\n",
      "2361/2361 [==============================] - 1110s 470ms/step - loss: 1.5600 - accuracy: 0.6013 - val_loss: 1.8728 - val_accuracy: 0.5467 - lr: 3.3300e-04\n",
      "Epoch 7/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 1.4373 - accuracy: 0.6275\n",
      "Epoch 7: val_accuracy improved from 0.54666 to 0.56510, saving model to models/030423_12_50.h5\n",
      "2361/2361 [==============================] - 1113s 472ms/step - loss: 1.4373 - accuracy: 0.6275 - val_loss: 1.8026 - val_accuracy: 0.5651 - lr: 3.3300e-04\n",
      "Epoch 8/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 1.3317 - accuracy: 0.6523\n",
      "Epoch 8: val_accuracy improved from 0.56510 to 0.56891, saving model to models/030423_12_50.h5\n",
      "2361/2361 [==============================] - 1117s 473ms/step - loss: 1.3317 - accuracy: 0.6523 - val_loss: 1.7932 - val_accuracy: 0.5689 - lr: 3.3300e-04\n",
      "Epoch 9/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 1.2366 - accuracy: 0.6734\n",
      "Epoch 9: val_accuracy improved from 0.56891 to 0.57659, saving model to models/030423_12_50.h5\n",
      "2361/2361 [==============================] - 1112s 471ms/step - loss: 1.2366 - accuracy: 0.6734 - val_loss: 1.7609 - val_accuracy: 0.5766 - lr: 3.3300e-04\n",
      "Epoch 10/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 1.1506 - accuracy: 0.6927\n",
      "Epoch 10: val_accuracy improved from 0.57659 to 0.58252, saving model to models/030423_12_50.h5\n",
      "2361/2361 [==============================] - 1115s 472ms/step - loss: 1.1506 - accuracy: 0.6927 - val_loss: 1.7418 - val_accuracy: 0.5825 - lr: 3.3300e-04\n",
      "Epoch 11/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 1.0662 - accuracy: 0.7133\n",
      "Epoch 11: val_accuracy improved from 0.58252 to 0.58755, saving model to models/030423_12_50.h5\n",
      "2361/2361 [==============================] - 1125s 477ms/step - loss: 1.0662 - accuracy: 0.7133 - val_loss: 1.7470 - val_accuracy: 0.5876 - lr: 3.3300e-04\n",
      "Epoch 12/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 0.9956 - accuracy: 0.7293\n",
      "Epoch 12: val_accuracy improved from 0.58755 to 0.59619, saving model to models/030423_12_50.h5\n",
      "2361/2361 [==============================] - 1156s 490ms/step - loss: 0.9956 - accuracy: 0.7293 - val_loss: 1.7367 - val_accuracy: 0.5962 - lr: 3.3300e-04\n",
      "Epoch 13/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 0.9281 - accuracy: 0.7462\n",
      "Epoch 13: val_accuracy did not improve from 0.59619\n",
      "2361/2361 [==============================] - 1191s 504ms/step - loss: 0.9281 - accuracy: 0.7462 - val_loss: 1.7717 - val_accuracy: 0.5879 - lr: 3.3300e-04\n",
      "Epoch 14/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 0.8659 - accuracy: 0.7622\n",
      "Epoch 14: val_accuracy improved from 0.59619 to 0.59725, saving model to models/030423_12_50.h5\n",
      "2361/2361 [==============================] - 1162s 492ms/step - loss: 0.8659 - accuracy: 0.7622 - val_loss: 1.7424 - val_accuracy: 0.5972 - lr: 3.3300e-04\n",
      "Epoch 15/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 0.8054 - accuracy: 0.7768\n",
      "Epoch 15: val_accuracy did not improve from 0.59725\n",
      "2361/2361 [==============================] - 1132s 480ms/step - loss: 0.8054 - accuracy: 0.7768 - val_loss: 1.7688 - val_accuracy: 0.5941 - lr: 3.3300e-04\n",
      "Epoch 16/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 0.7517 - accuracy: 0.7898\n",
      "Epoch 16: val_accuracy improved from 0.59725 to 0.59931, saving model to models/030423_12_50.h5\n",
      "2361/2361 [==============================] - 1118s 473ms/step - loss: 0.7517 - accuracy: 0.7898 - val_loss: 1.7616 - val_accuracy: 0.5993 - lr: 3.3300e-04\n",
      "Epoch 17/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 0.7031 - accuracy: 0.8025\n",
      "Epoch 17: val_accuracy did not improve from 0.59931\n",
      "2361/2361 [==============================] - 1116s 473ms/step - loss: 0.7031 - accuracy: 0.8025 - val_loss: 1.7897 - val_accuracy: 0.5990 - lr: 3.3300e-04\n",
      "Epoch 18/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 0.6537 - accuracy: 0.8162\n",
      "Epoch 18: val_accuracy did not improve from 0.59931\n",
      "2361/2361 [==============================] - 1130s 479ms/step - loss: 0.6537 - accuracy: 0.8162 - val_loss: 1.8063 - val_accuracy: 0.5981 - lr: 3.3300e-04\n",
      "Epoch 19/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 0.6075 - accuracy: 0.8295\n",
      "Epoch 19: val_accuracy improved from 0.59931 to 0.60222, saving model to models/030423_12_50.h5\n",
      "2361/2361 [==============================] - 1160s 491ms/step - loss: 0.6075 - accuracy: 0.8295 - val_loss: 1.8139 - val_accuracy: 0.6022 - lr: 3.3300e-04\n",
      "Epoch 20/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 0.5715 - accuracy: 0.8389\n",
      "Epoch 20: val_accuracy improved from 0.60222 to 0.60270, saving model to models/030423_12_50.h5\n",
      "2361/2361 [==============================] - 1156s 489ms/step - loss: 0.5715 - accuracy: 0.8389 - val_loss: 1.8136 - val_accuracy: 0.6027 - lr: 3.3300e-04\n",
      "Epoch 21/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 0.5379 - accuracy: 0.8472\n",
      "Epoch 21: val_accuracy did not improve from 0.60270\n",
      "2361/2361 [==============================] - 1153s 488ms/step - loss: 0.5379 - accuracy: 0.8472 - val_loss: 1.8421 - val_accuracy: 0.5994 - lr: 3.3300e-04\n",
      "Epoch 22/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 0.5023 - accuracy: 0.8565\n",
      "Epoch 22: val_accuracy did not improve from 0.60270\n",
      "2361/2361 [==============================] - 1129s 478ms/step - loss: 0.5023 - accuracy: 0.8565 - val_loss: 1.8936 - val_accuracy: 0.5943 - lr: 3.3300e-04\n",
      "Epoch 23/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 0.4717 - accuracy: 0.8653\n",
      "Epoch 23: val_accuracy did not improve from 0.60270\n",
      "2361/2361 [==============================] - 1131s 479ms/step - loss: 0.4717 - accuracy: 0.8653 - val_loss: 1.8854 - val_accuracy: 0.5968 - lr: 3.3300e-04\n",
      "Epoch 24/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 0.4452 - accuracy: 0.8727\n",
      "Epoch 24: val_accuracy improved from 0.60270 to 0.60365, saving model to models/030423_12_50.h5\n",
      "2361/2361 [==============================] - 1129s 478ms/step - loss: 0.4452 - accuracy: 0.8727 - val_loss: 1.8761 - val_accuracy: 0.6037 - lr: 3.3300e-04\n",
      "Epoch 25/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 0.4217 - accuracy: 0.8784\n",
      "Epoch 25: val_accuracy did not improve from 0.60365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2361/2361 [==============================] - 1139s 482ms/step - loss: 0.4217 - accuracy: 0.8784 - val_loss: 1.9055 - val_accuracy: 0.6013 - lr: 3.3300e-04\n",
      "Epoch 26/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 0.3921 - accuracy: 0.8871\n",
      "Epoch 26: val_accuracy did not improve from 0.60365\n",
      "2361/2361 [==============================] - 1126s 477ms/step - loss: 0.3921 - accuracy: 0.8871 - val_loss: 1.9700 - val_accuracy: 0.5938 - lr: 3.3300e-04\n",
      "Epoch 27/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 0.3703 - accuracy: 0.8939\n",
      "Epoch 27: val_accuracy did not improve from 0.60365\n",
      "2361/2361 [==============================] - 1128s 478ms/step - loss: 0.3703 - accuracy: 0.8939 - val_loss: 1.9767 - val_accuracy: 0.5942 - lr: 3.3300e-04\n",
      "Epoch 28/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 0.3521 - accuracy: 0.9000\n",
      "Epoch 28: val_accuracy did not improve from 0.60365\n",
      "2361/2361 [==============================] - 1128s 478ms/step - loss: 0.3521 - accuracy: 0.9000 - val_loss: 1.9994 - val_accuracy: 0.5966 - lr: 3.3300e-04\n",
      "Epoch 29/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 0.3325 - accuracy: 0.9049\n",
      "Epoch 29: val_accuracy did not improve from 0.60365\n",
      "\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 3.330000035930425e-05.\n",
      "2361/2361 [==============================] - 1127s 477ms/step - loss: 0.3325 - accuracy: 0.9049 - val_loss: 1.9984 - val_accuracy: 0.5975 - lr: 3.3300e-04\n"
     ]
    }
   ],
   "source": [
    "file_name = \"models/030423_12_50.h5\"\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        file_name, \n",
    "        save_best_only=True, \n",
    "        monitor=\"val_accuracy\",\n",
    "        mode=\"max\",\n",
    "        verbose = 1\n",
    "    ),\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        patience=5, \n",
    "        monitor=\"val_accuracy\",\n",
    "        mode=\"max\"\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', factor=0.1,mode='max',verbose=1,\n",
    "                              patience=5, min_lr=0.000001)\n",
    "]\n",
    "model.fit(train_datagen,validation_data=test_datagen,\\\n",
    "          epochs=30, callbacks=callbacks)\n",
    "model = tf.keras.models.load_model(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
