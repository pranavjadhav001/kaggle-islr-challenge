{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "from focal_loss import SparseCategoricalFocalLoss\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import tensorflow_io as tfio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_it_all(seed=7):\n",
    "    \"\"\" Attempt to be Reproducible \"\"\"\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "seed_it_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROWS_PER_FRAME = 543\n",
    "def load_relevant_data_subset(pq_path):\n",
    "    data_columns = ['x', 'y', 'z']\n",
    "    data = pd.read_parquet(pq_path, columns=data_columns)\n",
    "    n_frames = int(len(data) / ROWS_PER_FRAME)\n",
    "    data = data.values.reshape(n_frames, ROWS_PER_FRAME, len(data_columns))\n",
    "    return data.astype(np.float32)\n",
    "\n",
    "#test_data  = load_relevant_data_subset('train_landmark_files/16069/100015657.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIP = [\n",
    "            61, 185, 40, 39, 37, 0, 267, 269, 270, 409,\n",
    "            291, 146, 91, 181, 84, 17, 314, 405, 321, 375,\n",
    "            78, 191, 80, 81, 82, 13, 312, 311, 310, 415,\n",
    "            95, 88, 178, 87, 14, 317, 402, 318, 324, 308,\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    left_ROWS_per_frame = 21\n",
    "    sequence_length = 20\n",
    "    batch_size = 32\n",
    "    face_ROWS_per_frame = 468\n",
    "    lip_ROWS_per_frame = 40\n",
    "\n",
    "labels  = json.load(open('sign_to_prediction_index_map.json','r'))\n",
    "complete_df = pd.read_csv('train.csv')\n",
    "complete_df = complete_df[complete_df['participant_id']!='37055']\n",
    "from sklearn.model_selection import train_test_split\n",
    "y = complete_df['sign']\n",
    "train_df, test_df = train_test_split(complete_df, test_size=0.2,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_loader(with_labels=True):\n",
    "    def load_video(video_path):\n",
    "        #print('herer')\n",
    "        video_df = tfio.IODataset.from_parquet(video_path)\n",
    "        #video_df = pd.read_parquet(video_path, engine='pyarrow')\n",
    "        #video_df.fillna(0,inplace=True)\n",
    "        left_df = video_df[video_df.type=='left_hand']\n",
    "        left_values = left_df[['x','y','z']].values\n",
    "        left_values = left_values.reshape(-1,CFG.left_ROWS_per_frame,3)\n",
    "        left_hand_array =  tf.image.resize(left_values, (CFG.sequence_length, CFG.left_ROWS_per_frame))\n",
    "        right_df = video_df[video_df.type=='right_hand']\n",
    "        right_values = right_df[['x','y','z']].values\n",
    "        right_values = right_values.reshape(-1,CFG.left_ROWS_per_frame,3)\n",
    "        right_hand_array =  tf.image.resize(right_values, (CFG.sequence_length, CFG.left_ROWS_per_frame))\n",
    "        return [left_hand_array, right_hand_array]\n",
    "    \n",
    "    def load_video_with_labels(path, label):\n",
    "        return load_video(path), labels[label]\n",
    "    \n",
    "    return load_video_with_labels if with_labels else load_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCustomData\u001b[39;00m(\u001b[43mtf\u001b[49m\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mSequence):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,df,num_frames\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\\\n\u001b[1;32m      3\u001b[0m                  labels_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msign_to_prediction_index_map.json\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf \u001b[38;5;241m=\u001b[39m df\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "class CustomData(tf.keras.utils.Sequence):\n",
    "    def __init__(self,df,num_frames=20,batch_size=8,shuffle=True,\\\n",
    "                 labels_path='sign_to_prediction_index_map.json'):\n",
    "        self.df = df\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.num_frames = num_frames\n",
    "        self.labels  = json.load(open('sign_to_prediction_index_map.json','r'))\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.df))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        batches = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        left_hand_input = np.zeros(shape=(self.batch_size,self.num_frames,CFG.left_ROWS_per_frame,2))\n",
    "        lip_input = np.zeros(shape=(self.batch_size,self.num_frames,CFG.lip_ROWS_per_frame,2))\n",
    "        labels = []\n",
    "        for i,row_val in enumerate(batches):\n",
    "            row = self.df.iloc[row_val]\n",
    "            left_hand,lip = self.load_video(row['path'])\n",
    "            left_hand_input[i,:] = left_hand\n",
    "            lip_input[i,:] = lip\n",
    "            labels.append(self.labels[row['sign']])\n",
    "        return [left_hand_input,lip_input],np.asarray(labels)\n",
    "            \n",
    "    def load_video(self,video_path):\n",
    "        video_df = pd.read_parquet(video_path, engine='pyarrow')\n",
    "        #video_df.dropna(inplace=True)\n",
    "        \n",
    "        if video_df[video_df['type']=='left_hand']['x'].isna().mean() <= \\\n",
    "      video_df[video_df['type']=='right_hand']['x'].isna().mean():\n",
    "            left_df = video_df[video_df.type=='left_hand']\n",
    "            left_df.dropna(inplace=True)\n",
    "            left_values = left_df[['x','y']].values\n",
    "            left_values = left_values.reshape(-1,CFG.left_ROWS_per_frame,2)\n",
    "            if len(left_values)!=0:\n",
    "                left_values[:,:,0] = (left_values[:,:,0]- np.min(left_values[:,:,0]))/(left_values[:,:,0].max()- left_values[:,:,0].min())\n",
    "                left_values[:,:,1] = (left_values[:,:,1]- np.min(left_values[:,:,1]))/(left_values[:,:,1].max()- left_values[:,:,1].min())\n",
    "                left_hand_array =  tf.image.resize(left_values, (CFG.sequence_length, CFG.left_ROWS_per_frame))\n",
    "            else:\n",
    "                left_hand_array =  tf.zeros(shape=(CFG.sequence_length, CFG.left_ROWS_per_frame,2),dtype=tf.float32)\n",
    "        else:\n",
    "            left_df = video_df[video_df.type=='right_hand']\n",
    "            left_df.dropna(inplace=True)\n",
    "            left_values = left_df[['x','y']].values\n",
    "            left_values = left_values.reshape(-1,CFG.left_ROWS_per_frame,2)\n",
    "            if len(left_values)!=0:\n",
    "                left_values[:,:,0] = (left_values[:,:,0]- np.min(left_values[:,:,0]))/(left_values[:,:,0].max()- left_values[:,:,0].min())\n",
    "                left_values[:,:,1] = (left_values[:,:,1]- np.min(left_values[:,:,1]))/(left_values[:,:,1].max()- left_values[:,:,1].min())\n",
    "                left_hand_array =  tf.image.resize(left_values, (CFG.sequence_length, CFG.left_ROWS_per_frame))\n",
    "                \n",
    "            else:\n",
    "                left_hand_array =  tf.zeros(shape=(CFG.sequence_length, CFG.left_ROWS_per_frame,2),dtype=tf.float32)\n",
    "        face_df = video_df[video_df.type=='face']\n",
    "        face_df.dropna(inplace=True)\n",
    "        face_df = face_df[['x','y']].values\n",
    "        face_df = face_df.reshape(-1,CFG.face_ROWS_per_frame,2)\n",
    "        lip_values = face_df[:,LIP,:]\n",
    "        if len(lip_values) != 0:\n",
    "            lip_values[:,:,0] = (lip_values[:,:,0]- np.min(lip_values[:,:,0]))/(lip_values[:,:,0].max()- lip_values[:,:,0].min())\n",
    "            lip_values[:,:,1] = (lip_values[:,:,1]- np.min(lip_values[:,:,1]))/(lip_values[:,:,1].max()- lip_values[:,:,1].min())\n",
    "            lip_values_array =  tf.image.resize(lip_values, (CFG.sequence_length, CFG.lip_ROWS_per_frame))\n",
    "        else:\n",
    "            lip_values_array =  tf.zeros(shape=(CFG.sequence_length, CFG.lip_ROWS_per_frame,2),dtype=tf.float32)\n",
    "        return left_hand_array,lip_values_array\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)//self.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = CustomData(train_df,num_frames=CFG.sequence_length,batch_size=CFG.batch_size)\n",
    "test_datagen = CustomData(test_df,num_frames=CFG.sequence_length,batch_size=CFG.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = train_datagen[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv1d_lstm_block(inputs, filters):\n",
    "    vector = tf.keras.layers.ConvLSTM1D(filters=32, kernel_size=3,return_sequences=False)(inputs)\n",
    "    #vector = tf.keras.layers.Dropout(0.2)(vector)\n",
    "    #vector = tf.keras.layers.BatchNormalization(axis=-1)(vector)\n",
    "    #vector = tf.keras.layers.ConvLSTM1D(filters=64, kernel_size=3,return_sequences=True,padding='same',\\\n",
    "    #                                    kernel_regularizer=tf.keras.regularizers.L2(l2=0.01),\\\n",
    "    #                                    recurrent_regularizer=tf.keras.regularizers.L2(l2=0.01))(vector)\n",
    "    #vector = tf.keras.layers.Dropout(0.2)(vector)\n",
    "    #vector = tf.keras.layers.BatchNormalization(axis=-1)(vector)\n",
    "    #vector = tf.keras.layers.ConvLSTM1D(filters=64, kernel_size=3,padding='same',\\\n",
    "    #                                   kernel_regularizer=tf.keras.regularizers.L2(l2=0.01),\\\n",
    "    #                                    recurrent_regularizer=tf.keras.regularizers.L2(l2=0.01))(vector)\n",
    "    #vector = tf.keras.layers.Dropout(0.2)(vector)\n",
    "    return vector\n",
    "\n",
    "def get_model():\n",
    "    input1 = tf.keras.Input((CFG.sequence_length, CFG.left_ROWS_per_frame, 2), dtype=tf.float32)\n",
    "    input3 = tf.keras.Input((CFG.sequence_length, CFG.lip_ROWS_per_frame, 2), dtype=tf.float32)\n",
    "    left_hand_vector = conv1d_lstm_block(input1, [64])\n",
    "    lip_vector = conv1d_lstm_block(input3, [64])\n",
    "    vector = tf.keras.layers.Concatenate(axis=1)([left_hand_vector,lip_vector])\n",
    "    vector = tf.keras.layers.Flatten()(vector)\n",
    "    vector = tf.keras.layers.Dense(512, activation=\"relu\")(vector)\n",
    "    vector = tf.keras.layers.Dropout(0.3)(vector)\n",
    "    output = tf.keras.layers.Dense(250, activation=\"softmax\")(vector)\n",
    "    model = tf.keras.Model(inputs=[input1,input3], outputs=output)\n",
    "    model.compile(tf.keras.optimizers.Adam(0.000333),loss='sparse_categorical_crossentropy', metrics=\"accuracy\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_9 (InputLayer)           [(None, 20, 21, 2)]  0           []                               \n",
      "                                                                                                  \n",
      " input_10 (InputLayer)          [(None, 20, 40, 2)]  0           []                               \n",
      "                                                                                                  \n",
      " conv_lstm1d_16 (ConvLSTM1D)    (None, 19, 32)       13184       ['input_9[0][0]']                \n",
      "                                                                                                  \n",
      " conv_lstm1d_17 (ConvLSTM1D)    (None, 38, 32)       13184       ['input_10[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 57, 32)       0           ['conv_lstm1d_16[0][0]',         \n",
      "                                                                  'conv_lstm1d_17[0][0]']         \n",
      "                                                                                                  \n",
      " flatten_3 (Flatten)            (None, 1824)         0           ['concatenate_3[0][0]']          \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 512)          934400      ['flatten_3[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 512)          0           ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 250)          128250      ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,089,018\n",
      "Trainable params: 1,089,018\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 4.0687 - accuracy: 0.1382\n",
      "Epoch 1: val_accuracy improved from -inf to 0.27897, saving model to models/032823_22_43.h5\n",
      "2361/2361 [==============================] - 1404s 592ms/step - loss: 4.0687 - accuracy: 0.1382 - val_loss: 3.2022 - val_accuracy: 0.2790 - lr: 3.3300e-04\n",
      "Epoch 2/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 3.0317 - accuracy: 0.3056\n",
      "Epoch 2: val_accuracy improved from 0.27897 to 0.38167, saving model to models/032823_22_43.h5\n",
      "2361/2361 [==============================] - 1312s 556ms/step - loss: 3.0317 - accuracy: 0.3056 - val_loss: 2.6729 - val_accuracy: 0.3817 - lr: 3.3300e-04\n",
      "Epoch 3/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 2.6245 - accuracy: 0.3840\n",
      "Epoch 3: val_accuracy improved from 0.38167 to 0.43829, saving model to models/032823_22_43.h5\n",
      "2361/2361 [==============================] - 1310s 555ms/step - loss: 2.6245 - accuracy: 0.3840 - val_loss: 2.4093 - val_accuracy: 0.4383 - lr: 3.3300e-04\n",
      "Epoch 4/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 2.3616 - accuracy: 0.4367\n",
      "Epoch 4: val_accuracy improved from 0.43829 to 0.47701, saving model to models/032823_22_43.h5\n",
      "2361/2361 [==============================] - 1313s 556ms/step - loss: 2.3616 - accuracy: 0.4367 - val_loss: 2.2355 - val_accuracy: 0.4770 - lr: 3.3300e-04\n",
      "Epoch 5/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 2.1598 - accuracy: 0.4786\n",
      "Epoch 5: val_accuracy improved from 0.47701 to 0.50604, saving model to models/032823_22_43.h5\n",
      "2361/2361 [==============================] - 1304s 552ms/step - loss: 2.1598 - accuracy: 0.4786 - val_loss: 2.0910 - val_accuracy: 0.5060 - lr: 3.3300e-04\n",
      "Epoch 6/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 1.9826 - accuracy: 0.5162\n",
      "Epoch 6: val_accuracy improved from 0.50604 to 0.53151, saving model to models/032823_22_43.h5\n",
      "2361/2361 [==============================] - 1313s 556ms/step - loss: 1.9826 - accuracy: 0.5162 - val_loss: 1.9774 - val_accuracy: 0.5315 - lr: 3.3300e-04\n",
      "Epoch 7/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 1.8407 - accuracy: 0.5454\n",
      "Epoch 7: val_accuracy improved from 0.53151 to 0.55879, saving model to models/032823_22_43.h5\n",
      "2361/2361 [==============================] - 1313s 556ms/step - loss: 1.8407 - accuracy: 0.5454 - val_loss: 1.8766 - val_accuracy: 0.5588 - lr: 3.3300e-04\n",
      "Epoch 8/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 1.7213 - accuracy: 0.5737\n",
      "Epoch 8: val_accuracy improved from 0.55879 to 0.57479, saving model to models/032823_22_43.h5\n",
      "2361/2361 [==============================] - 1311s 555ms/step - loss: 1.7213 - accuracy: 0.5737 - val_loss: 1.8054 - val_accuracy: 0.5748 - lr: 3.3300e-04\n",
      "Epoch 9/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 1.6146 - accuracy: 0.5950\n",
      "Epoch 9: val_accuracy improved from 0.57479 to 0.58252, saving model to models/032823_22_43.h5\n",
      "2361/2361 [==============================] - 1315s 557ms/step - loss: 1.6146 - accuracy: 0.5950 - val_loss: 1.7718 - val_accuracy: 0.5825 - lr: 3.3300e-04\n",
      "Epoch 10/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 1.5205 - accuracy: 0.6133\n",
      "Epoch 10: val_accuracy improved from 0.58252 to 0.58787, saving model to models/032823_22_43.h5\n",
      "2361/2361 [==============================] - 1317s 558ms/step - loss: 1.5205 - accuracy: 0.6133 - val_loss: 1.7410 - val_accuracy: 0.5879 - lr: 3.3300e-04\n",
      "Epoch 11/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 1.4389 - accuracy: 0.6317\n",
      "Epoch 11: val_accuracy improved from 0.58787 to 0.60016, saving model to models/032823_22_43.h5\n",
      "2361/2361 [==============================] - 1321s 560ms/step - loss: 1.4389 - accuracy: 0.6317 - val_loss: 1.7110 - val_accuracy: 0.6002 - lr: 3.3300e-04\n",
      "Epoch 12/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 1.3567 - accuracy: 0.6482\n",
      "Epoch 12: val_accuracy improved from 0.60016 to 0.60890, saving model to models/032823_22_43.h5\n",
      "2361/2361 [==============================] - 1317s 558ms/step - loss: 1.3567 - accuracy: 0.6482 - val_loss: 1.6691 - val_accuracy: 0.6089 - lr: 3.3300e-04\n",
      "Epoch 13/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 1.2905 - accuracy: 0.6624\n",
      "Epoch 13: val_accuracy improved from 0.60890 to 0.61494, saving model to models/032823_22_43.h5\n",
      "2361/2361 [==============================] - 1313s 556ms/step - loss: 1.2905 - accuracy: 0.6624 - val_loss: 1.6497 - val_accuracy: 0.6149 - lr: 3.3300e-04\n",
      "Epoch 14/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 1.2274 - accuracy: 0.6756\n",
      "Epoch 14: val_accuracy did not improve from 0.61494\n",
      "2361/2361 [==============================] - 1313s 556ms/step - loss: 1.2274 - accuracy: 0.6756 - val_loss: 1.6631 - val_accuracy: 0.6079 - lr: 3.3300e-04\n",
      "Epoch 15/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 1.1679 - accuracy: 0.6897\n",
      "Epoch 15: val_accuracy did not improve from 0.61494\n",
      "2361/2361 [==============================] - 1312s 556ms/step - loss: 1.1679 - accuracy: 0.6897 - val_loss: 1.7144 - val_accuracy: 0.6141 - lr: 3.3300e-04\n",
      "Epoch 16/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 1.1101 - accuracy: 0.7001\n",
      "Epoch 16: val_accuracy improved from 0.61494 to 0.61986, saving model to models/032823_22_43.h5\n",
      "2361/2361 [==============================] - 1316s 557ms/step - loss: 1.1101 - accuracy: 0.7001 - val_loss: 1.6762 - val_accuracy: 0.6199 - lr: 3.3300e-04\n",
      "Epoch 17/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 1.0527 - accuracy: 0.7142\n",
      "Epoch 17: val_accuracy improved from 0.61986 to 0.62288, saving model to models/032823_22_43.h5\n",
      "2361/2361 [==============================] - 1316s 557ms/step - loss: 1.0527 - accuracy: 0.7142 - val_loss: 1.6939 - val_accuracy: 0.6229 - lr: 3.3300e-04\n",
      "Epoch 18/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 0.9989 - accuracy: 0.7259\n",
      "Epoch 18: val_accuracy improved from 0.62288 to 0.62712, saving model to models/032823_22_43.h5\n",
      "2361/2361 [==============================] - 1312s 556ms/step - loss: 0.9989 - accuracy: 0.7259 - val_loss: 1.6680 - val_accuracy: 0.6271 - lr: 3.3300e-04\n",
      "Epoch 19/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 0.9585 - accuracy: 0.7345\n",
      "Epoch 19: val_accuracy did not improve from 0.62712\n",
      "2361/2361 [==============================] - 1316s 557ms/step - loss: 0.9585 - accuracy: 0.7345 - val_loss: 1.6987 - val_accuracy: 0.6261 - lr: 3.3300e-04\n",
      "Epoch 20/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 0.9099 - accuracy: 0.7457\n",
      "Epoch 20: val_accuracy improved from 0.62712 to 0.63162, saving model to models/032823_22_43.h5\n",
      "2361/2361 [==============================] - 1315s 557ms/step - loss: 0.9099 - accuracy: 0.7457 - val_loss: 1.6789 - val_accuracy: 0.6316 - lr: 3.3300e-04\n",
      "Epoch 21/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 0.8698 - accuracy: 0.7540\n",
      "Epoch 21: val_accuracy did not improve from 0.63162\n",
      "2361/2361 [==============================] - 1315s 557ms/step - loss: 0.8698 - accuracy: 0.7540 - val_loss: 1.7121 - val_accuracy: 0.6305 - lr: 3.3300e-04\n",
      "Epoch 22/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 0.8275 - accuracy: 0.7641\n",
      "Epoch 22: val_accuracy did not improve from 0.63162\n",
      "2361/2361 [==============================] - 1314s 557ms/step - loss: 0.8275 - accuracy: 0.7641 - val_loss: 1.6766 - val_accuracy: 0.6277 - lr: 3.3300e-04\n",
      "Epoch 23/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 0.7850 - accuracy: 0.7743\n",
      "Epoch 23: val_accuracy improved from 0.63162 to 0.63236, saving model to models/032823_22_43.h5\n",
      "2361/2361 [==============================] - 1314s 557ms/step - loss: 0.7850 - accuracy: 0.7743 - val_loss: 1.7504 - val_accuracy: 0.6324 - lr: 3.3300e-04\n",
      "Epoch 24/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 0.7509 - accuracy: 0.7828\n",
      "Epoch 24: val_accuracy improved from 0.63236 to 0.63379, saving model to models/032823_22_43.h5\n",
      "2361/2361 [==============================] - 1315s 557ms/step - loss: 0.7509 - accuracy: 0.7828 - val_loss: 1.7319 - val_accuracy: 0.6338 - lr: 3.3300e-04\n",
      "Epoch 25/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 0.7139 - accuracy: 0.7900\n",
      "Epoch 25: val_accuracy did not improve from 0.63379\n",
      "2361/2361 [==============================] - 1312s 556ms/step - loss: 0.7139 - accuracy: 0.7900 - val_loss: 1.7923 - val_accuracy: 0.6326 - lr: 3.3300e-04\n",
      "Epoch 26/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 0.6786 - accuracy: 0.8002\n",
      "Epoch 26: val_accuracy did not improve from 0.63379\n",
      "2361/2361 [==============================] - 1314s 556ms/step - loss: 0.6786 - accuracy: 0.8002 - val_loss: 1.8117 - val_accuracy: 0.6262 - lr: 3.3300e-04\n",
      "Epoch 27/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 0.6460 - accuracy: 0.8083\n",
      "Epoch 27: val_accuracy did not improve from 0.63379\n",
      "\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 3.330000035930425e-05.\n",
      "2361/2361 [==============================] - 1325s 561ms/step - loss: 0.6460 - accuracy: 0.8083 - val_loss: 1.8133 - val_accuracy: 0.6330 - lr: 3.3300e-04\n",
      "Epoch 28/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 0.4754 - accuracy: 0.8602\n",
      "Epoch 28: val_accuracy improved from 0.63379 to 0.64444, saving model to models/032823_22_43.h5\n",
      "2361/2361 [==============================] - 1313s 556ms/step - loss: 0.4754 - accuracy: 0.8602 - val_loss: 1.7911 - val_accuracy: 0.6444 - lr: 3.3300e-05\n",
      "Epoch 29/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 0.4394 - accuracy: 0.8718\n",
      "Epoch 29: val_accuracy improved from 0.64444 to 0.64544, saving model to models/032823_22_43.h5\n",
      "2361/2361 [==============================] - 1316s 558ms/step - loss: 0.4394 - accuracy: 0.8718 - val_loss: 1.7993 - val_accuracy: 0.6454 - lr: 3.3300e-05\n",
      "Epoch 30/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 0.4213 - accuracy: 0.8762\n",
      "Epoch 30: val_accuracy improved from 0.64544 to 0.64682, saving model to models/032823_22_43.h5\n",
      "2361/2361 [==============================] - 1315s 557ms/step - loss: 0.4213 - accuracy: 0.8762 - val_loss: 1.8167 - val_accuracy: 0.6468 - lr: 3.3300e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f389fb60610>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name = \"models/032823_22_43.h5\"\n",
    "#model = tf.keras.models.load_model(file_name)\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        file_name, \n",
    "        save_best_only=True, \n",
    "        monitor=\"val_accuracy\",\n",
    "        mode=\"max\",\n",
    "        verbose = 1\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', factor=0.1,mode='max',verbose=1,\n",
    "                              patience=3, min_lr=0.000001)\n",
    "]\n",
    "model.fit(train_datagen,validation_data=test_datagen,\\\n",
    "          epochs=30, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epsilon value for layer normalisation\n",
    "LAYER_NORM_EPS = 1e-6\n",
    "\n",
    "# Dense layer units for landmarks\n",
    "LIPS_UNITS = 384\n",
    "HANDS_UNITS = 384\n",
    "POSE_UNITS = 384\n",
    "# final embedding and transformer embedding size\n",
    "UNITS = 384\n",
    "\n",
    "# Transformer\n",
    "NUM_BLOCKS = 2\n",
    "MLP_RATIO = 2\n",
    "\n",
    "# Dropout\n",
    "EMBEDDING_DROPOUT = 0.00\n",
    "MLP_DROPOUT_RATIO = 0.30\n",
    "CLASSIFIER_DROPOUT_RATIO = 0.10\n",
    "\n",
    "# Initiailizers\n",
    "INIT_HE_UNIFORM = tf.keras.initializers.he_uniform\n",
    "INIT_GLOROT_UNIFORM = tf.keras.initializers.glorot_uniform\n",
    "INIT_ZEROS = tf.keras.initializers.constant(0.0)\n",
    "# Activations\n",
    "GELU = tf.keras.activations.gelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on: https://stackoverflow.com/questions/67342988/verifying-the-implementation-of-multihead-attention-in-transformer\n",
    "# replaced softmax with softmax layer to support masked softmax\n",
    "def scaled_dot_product(q,k,v, softmax, attention_mask):\n",
    "    #calculates Q . K(transpose)\n",
    "    qkt = tf.matmul(q,k,transpose_b=True)\n",
    "    #caculates scaling factor\n",
    "    dk = tf.math.sqrt(tf.cast(q.shape[-1],dtype=tf.float32))\n",
    "    scaled_qkt = qkt/dk\n",
    "    softmax = softmax(scaled_qkt, mask=attention_mask)\n",
    "    \n",
    "    z = tf.matmul(softmax,v)\n",
    "    #shape: (m,Tx,depth), same shape as q,k,v\n",
    "    return z\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self,d_model,num_of_heads):\n",
    "        super(MultiHeadAttention,self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_of_heads = num_of_heads\n",
    "        self.depth = d_model//num_of_heads\n",
    "        self.wq = [tf.keras.layers.Dense(self.depth) for i in range(num_of_heads)]\n",
    "        self.wk = [tf.keras.layers.Dense(self.depth) for i in range(num_of_heads)]\n",
    "        self.wv = [tf.keras.layers.Dense(self.depth) for i in range(num_of_heads)]\n",
    "        self.wo = tf.keras.layers.Dense(d_model)\n",
    "        self.softmax = tf.keras.layers.Softmax()\n",
    "        \n",
    "    def call(self,x, attention_mask):\n",
    "        \n",
    "        multi_attn = []\n",
    "        for i in range(self.num_of_heads):\n",
    "            Q = self.wq[i](x)\n",
    "            K = self.wk[i](x)\n",
    "            V = self.wv[i](x)\n",
    "            multi_attn.append(scaled_dot_product(Q,K,V, self.softmax, attention_mask))\n",
    "            \n",
    "        multi_head = tf.concat(multi_attn,axis=-1)\n",
    "        multi_head_attention = self.wo(multi_head)\n",
    "        return multi_head_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full Transformer\n",
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_blocks):\n",
    "        super(Transformer, self).__init__(name='transformer')\n",
    "        self.num_blocks = num_blocks\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.ln_1s = []\n",
    "        self.mhas = []\n",
    "        self.ln_2s = []\n",
    "        self.mlps = []\n",
    "        # Make Transformer Blocks\n",
    "        for i in range(self.num_blocks):\n",
    "            # First Layer Normalisation\n",
    "            self.ln_1s.append(tf.keras.layers.LayerNormalization(epsilon=LAYER_NORM_EPS))\n",
    "            # Multi Head Attention\n",
    "            self.mhas.append(MultiHeadAttention(UNITS, 8))\n",
    "            # Second Layer Normalisation\n",
    "            self.ln_2s.append(tf.keras.layers.LayerNormalization(epsilon=LAYER_NORM_EPS))\n",
    "            # Multi Layer Perception\n",
    "            self.mlps.append(tf.keras.Sequential([\n",
    "                tf.keras.layers.Dense(UNITS * MLP_RATIO, activation=GELU, kernel_initializer=INIT_GLOROT_UNIFORM),\n",
    "                tf.keras.layers.Dropout(MLP_DROPOUT_RATIO),\n",
    "                tf.keras.layers.Dense(UNITS, kernel_initializer=INIT_HE_UNIFORM),\n",
    "            ]))\n",
    "        \n",
    "    def call(self, x, attention_mask):\n",
    "        # Iterate input over transformer blocks\n",
    "        for ln_1, mha, ln_2, mlp in zip(self.ln_1s, self.mhas, self.ln_2s, self.mlps):\n",
    "            x1 = ln_1(x)\n",
    "            attention_output = mha(x1, attention_mask)\n",
    "            x2 = x1 + attention_output\n",
    "            x3 = ln_2(x2)\n",
    "            x3 = mlp(x3)\n",
    "            x = x3 + x2\n",
    "    \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LandmarkEmbedding(tf.keras.Model):\n",
    "    def __init__(self, units, name):\n",
    "        super(LandmarkEmbedding, self).__init__(name=f'{name}_embedding')\n",
    "        self.units = units\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        # Embedding for missing landmark in frame, initizlied with zeros\n",
    "        self.empty_embedding = self.add_weight(\n",
    "            name=f'{self.name}_empty_embedding',\n",
    "            shape=[self.units],\n",
    "            initializer=INIT_ZEROS,\n",
    "        )\n",
    "        # Embedding\n",
    "        self.dense = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(self.units, name=f'{self.name}_dense_1', use_bias=False, kernel_initializer=INIT_GLOROT_UNIFORM, activation=GELU),\n",
    "            tf.keras.layers.Dense(self.units, name=f'{self.name}_dense_2', use_bias=False, kernel_initializer=INIT_HE_UNIFORM),\n",
    "        ], name=f'{self.name}_dense')\n",
    "\n",
    "    def call(self, x):\n",
    "        return tf.where(\n",
    "                # Checks whether landmark is missing in frame\n",
    "                tf.reduce_sum(x, axis=2, keepdims=True) == 0,\n",
    "                # If so, the empty embedding is used\n",
    "                self.empty_embedding,\n",
    "                # Otherwise the landmark data is embedded\n",
    "                self.dense(x),\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Embedding, self).__init__()\n",
    "        \n",
    "    def get_diffs(self, l):\n",
    "        S = l.shape[2]\n",
    "        other = tf.expand_dims(l, 3)\n",
    "        other = tf.repeat(other, S, axis=3)\n",
    "        other = tf.transpose(other, [0,1,3,2])\n",
    "        diffs = tf.expand_dims(l, 3) - other\n",
    "        diffs = tf.reshape(diffs, [-1, INPUT_SIZE, S*S])\n",
    "        return diffs\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Positional Embedding, initialized with zeros\n",
    "        self.positional_embedding = tf.keras.layers.Embedding(INPUT_SIZE+1, UNITS, embeddings_initializer=INIT_ZEROS)\n",
    "        # Embedding layer for Landmarks\n",
    "        self.lips_embedding = LandmarkEmbedding(LIPS_UNITS, 'lips')\n",
    "        self.left_hand_embedding = LandmarkEmbedding(HANDS_UNITS, 'left_hand')\n",
    "        self.right_hand_embedding = LandmarkEmbedding(HANDS_UNITS, 'right_hand')\n",
    "        self.pose_embedding = LandmarkEmbedding(POSE_UNITS, 'pose')\n",
    "        # Landmark Weights\n",
    "        self.landmark_weights = tf.Variable(tf.zeros([4], dtype=tf.float32), name='landmark_weights')\n",
    "        # Fully Connected Layers for combined landmarks\n",
    "        self.fc = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(UNITS, name='fully_connected_1', use_bias=False, kernel_initializer=INIT_GLOROT_UNIFORM, activation=GELU),\n",
    "            tf.keras.layers.Dense(UNITS, name='fully_connected_2', use_bias=False, kernel_initializer=INIT_HE_UNIFORM),\n",
    "        ], name='fc')\n",
    "    def call(self, lips0, left_hand0, right_hand0, pose0, non_empty_frame_idxs, training=False):\n",
    "        # Lips\n",
    "        lips_embedding = self.lips_embedding(lips0)\n",
    "        # Left Hand\n",
    "        left_hand_embedding = self.left_hand_embedding(left_hand0)\n",
    "        # Right Hand\n",
    "        right_hand_embedding = self.right_hand_embedding(right_hand0)\n",
    "        # Pose\n",
    "        pose_embedding = self.pose_embedding(pose0)\n",
    "        # Merge Embeddings of all landmarks with mean pooling\n",
    "        x = tf.stack((lips_embedding, left_hand_embedding, right_hand_embedding, pose_embedding), axis=3)\n",
    "        # Merge Landmarks with trainable attention weights\n",
    "        x = x * tf.nn.softmax(self.landmark_weights)\n",
    "        x = tf.reduce_sum(x, axis=3)\n",
    "        # Fully Connected Layers\n",
    "        x = self.fc(x)\n",
    "        # Add Positional Embedding\n",
    "        normalised_non_empty_frame_idxs = tf.where(\n",
    "            tf.math.equal(non_empty_frame_idxs, -1.0),\n",
    "            INPUT_SIZE,\n",
    "            tf.cast(\n",
    "                non_empty_frame_idxs / tf.reduce_max(non_empty_frame_idxs, axis=1, keepdims=True) * INPUT_SIZE,\n",
    "                tf.int32,\n",
    "            ),\n",
    "        )\n",
    "        x = x + self.positional_embedding(normalised_non_empty_frame_idxs)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    # Inputs\n",
    "    frames = tf.keras.layers.Input([INPUT_SIZE, N_COLS, N_DIMS], dtype=tf.float32, name='frames')\n",
    "    non_empty_frame_idxs = tf.keras.layers.Input([INPUT_SIZE], dtype=tf.float32, name='non_empty_frame_idxs')\n",
    "    # Padding Mask\n",
    "    mask = tf.cast(tf.math.not_equal(non_empty_frame_idxs, -1), tf.float32)\n",
    "    mask = tf.expand_dims(mask, axis=2)\n",
    "    \n",
    "    \"\"\"\n",
    "        left_hand: 468:489\n",
    "        pose: 489:522\n",
    "        right_hand: 522:543\n",
    "    \"\"\"\n",
    "    x = frames\n",
    "    x = tf.slice(x, [0,0,0,0], [-1,INPUT_SIZE, N_COLS, 2])\n",
    "    # LIPS\n",
    "    lips = tf.slice(x, [0,0,LIPS_START,0], [-1,INPUT_SIZE, 40, 2])\n",
    "    lips = tf.where(\n",
    "            tf.math.equal(lips, 0.0),\n",
    "            0.0,\n",
    "            (lips - LIPS_MEAN) / LIPS_STD,\n",
    "        )\n",
    "    lips = tf.reshape(lips, [-1, INPUT_SIZE, 40*2])\n",
    "    # LEFT HAND\n",
    "    left_hand = tf.slice(x, [0,0,40,0], [-1,INPUT_SIZE, 21, 2])\n",
    "    left_hand = tf.where(\n",
    "            tf.math.equal(left_hand, 0.0),\n",
    "            0.0,\n",
    "            (left_hand - LEFT_HANDS_MEAN) / LEFT_HANDS_STD,\n",
    "        )\n",
    "    left_hand = tf.reshape(left_hand, [-1, INPUT_SIZE, 21*2])\n",
    "    # RIGHT HAND\n",
    "    right_hand = tf.slice(x, [0,0,61,0], [-1,INPUT_SIZE, 21, 2])\n",
    "    right_hand = tf.where(\n",
    "            tf.math.equal(right_hand, 0.0),\n",
    "            0.0,\n",
    "            (right_hand - RIGHT_HANDS_MEAN) / RIGHT_HANDS_STD,\n",
    "        )\n",
    "    right_hand = tf.reshape(right_hand, [-1, INPUT_SIZE, 21*2])\n",
    "    pose = tf.slice(x, [0,0,82,0], [-1,INPUT_SIZE, 10, 2])\n",
    "    pose = tf.where(\n",
    "            tf.math.equal(pose, 0.0),\n",
    "            0.0,\n",
    "            (pose - POSE_MEAN) / POSE_STD,\n",
    "        )\n",
    "    pose = tf.reshape(pose, [-1, INPUT_SIZE, 10*2])\n",
    "    \n",
    "    x = lips, left_hand, right_hand, pose\n",
    "        \n",
    "    x = Embedding()(lips, left_hand, right_hand, pose, non_empty_frame_idxs)\n",
    "    \n",
    "    # Encoder Transformer Blocks\n",
    "    x = Transformer(NUM_BLOCKS)(x, mask)\n",
    "    \n",
    "    # Pooling\n",
    "    x = tf.reduce_sum(x * mask, axis=1) / tf.reduce_sum(mask, axis=1)\n",
    "    # Classification Layer\n",
    "    x = tf.keras.layers.Dense(NUM_CLASSES, activation=tf.keras.activations.softmax, kernel_initializer=INIT_GLOROT_UNIFORM)(x)\n",
    "    \n",
    "    outputs = x\n",
    "    \n",
    "    # Create Tensorflow Model\n",
    "    model = tf.keras.models.Model(inputs=[frames, non_empty_frame_idxs], outputs=outputs)\n",
    "    \n",
    "    # Simple Categorical Crossentropy Loss\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "    \n",
    "    # Adam Optimizer with weight decay\n",
    "    optimizer = tfa.optimizers.AdamW(learning_rate=1e-3, weight_decay=1e-5, clipnorm=1.0)\n",
    "    \n",
    "    # TopK Metrics\n",
    "    metrics = [\n",
    "        tf.keras.metrics.SparseCategoricalAccuracy(name='acc'),\n",
    "        tf.keras.metrics.SparseTopKCategoricalAccuracy(k=5, name='top_5_acc'),\n",
    "        tf.keras.metrics.SparseTopKCategoricalAccuracy(k=10, name='top_10_acc'),\n",
    "    ]\n",
    "    \n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model = get_model()\n",
    "model.summary(expand_nested=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
