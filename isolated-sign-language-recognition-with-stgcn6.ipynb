{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.008117,
     "end_time": "2023-03-02T08:44:33.967124",
     "exception": false,
     "start_time": "2023-03-02T08:44:33.959007",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Isolated Sign Language Recognition with STGCN\n",
    "\n",
    "In this notebook, I will create Sign Language Recognition model using STGCN. To build an efficient training pipeline, I will use TFRecord Dataset from https://www.kaggle.com/datasets/lonnieqin/islr-12-time-steps-tfrecords created by notebook https://www.kaggle.com/code/lonnieqin/islr-create-tfrecord for training.\n",
    "The ST-GCN model archetecture was adapated from https://github.com/kdkalvik/ST-GCN\n",
    "It will take about 1 hour to finish runing this notebook using GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.006539,
     "end_time": "2023-03-02T08:44:33.994281",
     "exception": false,
     "start_time": "2023-03-02T08:44:33.987742",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-28T10:00:12.442535Z",
     "iopub.status.busy": "2023-04-28T10:00:12.442080Z",
     "iopub.status.idle": "2023-04-28T10:00:12.470153Z",
     "shell.execute_reply": "2023-04-28T10:00:12.469255Z",
     "shell.execute_reply.started": "2023-04-28T10:00:12.442503Z"
    },
    "papermill": {
     "duration": 0.022144,
     "end_time": "2023-03-02T08:44:34.023146",
     "exception": false,
     "start_time": "2023-03-02T08:44:34.001002",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    data_path = \"\"\n",
    "    tf_record_path = \"/kaggle/input/islr-12-time-steps-tfrecords/\"\n",
    "    sequence_length = 12\n",
    "    rows_per_frame = 543\n",
    "    is_training = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.006464,
     "end_time": "2023-03-02T08:44:34.036262",
     "exception": false,
     "start_time": "2023-03-02T08:44:34.029798",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-28T10:00:12.472531Z",
     "iopub.status.busy": "2023-04-28T10:00:12.472164Z",
     "iopub.status.idle": "2023-04-28T10:00:21.151336Z",
     "shell.execute_reply": "2023-04-28T10:00:21.150218Z",
     "shell.execute_reply.started": "2023-04-28T10:00:12.472493Z"
    },
    "papermill": {
     "duration": 7.496016,
     "end_time": "2023-03-02T08:44:41.539085",
     "exception": false,
     "start_time": "2023-03-02T08:44:34.043069",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import math\n",
    "np.random.seed(16)\n",
    "tf.random.set_seed(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.006496,
     "end_time": "2023-03-02T08:44:41.597734",
     "exception": false,
     "start_time": "2023-03-02T08:44:41.591238",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-28T10:00:28.032382Z",
     "iopub.status.busy": "2023-04-28T10:00:28.031796Z",
     "iopub.status.idle": "2023-04-28T10:00:28.042057Z",
     "shell.execute_reply": "2023-04-28T10:00:28.040895Z",
     "shell.execute_reply.started": "2023-04-28T10:00:28.032346Z"
    },
    "papermill": {
     "duration": 0.017828,
     "end_time": "2023-03-02T08:44:41.622261",
     "exception": false,
     "start_time": "2023-03-02T08:44:41.604433",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ROWS_PER_FRAME = 543  # number of landmarks per frame\n",
    "\n",
    "def load_relevant_data_subset_with_imputation(pq_path):\n",
    "    data_columns = ['x', 'y']\n",
    "    data = pd.read_parquet(pq_path, columns=data_columns)\n",
    "    data.replace(np.nan, 0, inplace=True)\n",
    "    n_frames = int(len(data) / ROWS_PER_FRAME)\n",
    "    data = data.values.reshape(n_frames, ROWS_PER_FRAME, len(data_columns))\n",
    "    return data.astype(np.float16)\n",
    "\n",
    "def load_relevant_data_subset(pq_path):\n",
    "    data_columns = ['x', 'y']\n",
    "    data = pd.read_parquet(pq_path, columns=data_columns)\n",
    "    n_frames = int(len(data) / ROWS_PER_FRAME)\n",
    "    data = data.values.reshape(n_frames, ROWS_PER_FRAME, len(data_columns))\n",
    "    return data.astype(np.float32)\n",
    "\n",
    "def read_dict(file_path):\n",
    "    path = os.path.expanduser(file_path)\n",
    "    with open(path, \"r\") as f:\n",
    "        dic = json.load(f)\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.006542,
     "end_time": "2023-03-02T08:44:41.635478",
     "exception": false,
     "start_time": "2023-03-02T08:44:41.628936",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-28T10:00:28.576086Z",
     "iopub.status.busy": "2023-04-28T10:00:28.575721Z",
     "iopub.status.idle": "2023-04-28T10:00:28.762511Z",
     "shell.execute_reply": "2023-04-28T10:00:28.761424Z",
     "shell.execute_reply.started": "2023-04-28T10:00:28.576053Z"
    },
    "papermill": {
     "duration": 0.206559,
     "end_time": "2023-03-02T08:44:41.848795",
     "exception": false,
     "start_time": "2023-03-02T08:44:41.642236",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>participant_id</th>\n",
       "      <th>sequence_id</th>\n",
       "      <th>sign</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_landmark_files/26734/1000035562.parquet</td>\n",
       "      <td>26734</td>\n",
       "      <td>1000035562</td>\n",
       "      <td>blow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_landmark_files/28656/1000106739.parquet</td>\n",
       "      <td>28656</td>\n",
       "      <td>1000106739</td>\n",
       "      <td>wait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_landmark_files/16069/100015657.parquet</td>\n",
       "      <td>16069</td>\n",
       "      <td>100015657</td>\n",
       "      <td>cloud</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_landmark_files/25571/1000210073.parquet</td>\n",
       "      <td>25571</td>\n",
       "      <td>1000210073</td>\n",
       "      <td>bird</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_landmark_files/62590/1000240708.parquet</td>\n",
       "      <td>62590</td>\n",
       "      <td>1000240708</td>\n",
       "      <td>owie</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            path  participant_id  sequence_id  \\\n",
       "0  train_landmark_files/26734/1000035562.parquet           26734   1000035562   \n",
       "1  train_landmark_files/28656/1000106739.parquet           28656   1000106739   \n",
       "2   train_landmark_files/16069/100015657.parquet           16069    100015657   \n",
       "3  train_landmark_files/25571/1000210073.parquet           25571   1000210073   \n",
       "4  train_landmark_files/62590/1000240708.parquet           62590   1000240708   \n",
       "\n",
       "    sign  \n",
       "0   blow  \n",
       "1   wait  \n",
       "2  cloud  \n",
       "3   bird  \n",
       "4   owie  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(f\"{CFG.data_path}train.csv\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.007434,
     "end_time": "2023-03-02T08:44:41.8635",
     "exception": false,
     "start_time": "2023-03-02T08:44:41.856066",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "There are 21 participants. Each of them created about 3000 to 5000 training records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-28T10:00:21.429554Z",
     "iopub.status.busy": "2023-04-28T10:00:21.428647Z",
     "iopub.status.idle": "2023-04-28T10:00:21.441674Z",
     "shell.execute_reply": "2023-04-28T10:00:21.440628Z",
     "shell.execute_reply.started": "2023-04-28T10:00:21.429504Z"
    },
    "papermill": {
     "duration": 0.024082,
     "end_time": "2023-03-02T08:44:41.894435",
     "exception": false,
     "start_time": "2023-03-02T08:44:41.870353",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.participant_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-28T10:00:21.443808Z",
     "iopub.status.busy": "2023-04-28T10:00:21.443178Z",
     "iopub.status.idle": "2023-04-28T10:00:21.780692Z",
     "shell.execute_reply": "2023-04-28T10:00:21.779677Z",
     "shell.execute_reply.started": "2023-04-28T10:00:21.443772Z"
    },
    "papermill": {
     "duration": 0.339293,
     "end_time": "2023-03-02T08:44:42.240816",
     "exception": false,
     "start_time": "2023-03-02T08:44:41.901523",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot: >"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.participant_id.value_counts().plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.007382,
     "end_time": "2023-03-02T08:44:42.255978",
     "exception": false,
     "start_time": "2023-03-02T08:44:42.248596",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "There are 94477 training samples in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-28T10:00:33.098748Z",
     "iopub.status.busy": "2023-04-28T10:00:33.098364Z",
     "iopub.status.idle": "2023-04-28T10:00:33.105587Z",
     "shell.execute_reply": "2023-04-28T10:00:33.104499Z",
     "shell.execute_reply.started": "2023-04-28T10:00:33.098691Z"
    },
    "papermill": {
     "duration": 0.016757,
     "end_time": "2023-03-02T08:44:42.280157",
     "exception": false,
     "start_time": "2023-03-02T08:44:42.2634",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94477"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.007305,
     "end_time": "2023-03-02T08:44:42.294963",
     "exception": false,
     "start_time": "2023-03-02T08:44:42.287658",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "There are 250 kinds of sign languages that we need to make prediction on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-28T10:00:34.919532Z",
     "iopub.status.busy": "2023-04-28T10:00:34.918776Z",
     "iopub.status.idle": "2023-04-28T10:00:34.974299Z",
     "shell.execute_reply": "2023-04-28T10:00:34.973172Z",
     "shell.execute_reply.started": "2023-04-28T10:00:34.919491Z"
    },
    "papermill": {
     "duration": 0.059155,
     "end_time": "2023-03-02T08:44:42.361615",
     "exception": false,
     "start_time": "2023-03-02T08:44:42.30246",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'TV': 0, 'after': 1, 'airplane': 2, 'all': 3, 'alligator': 4, 'animal': 5, 'another': 6, 'any': 7, 'apple': 8, 'arm': 9, 'aunt': 10, 'awake': 11, 'backyard': 12, 'bad': 13, 'balloon': 14, 'bath': 15, 'because': 16, 'bed': 17, 'bedroom': 18, 'bee': 19, 'before': 20, 'beside': 21, 'better': 22, 'bird': 23, 'black': 24, 'blow': 25, 'blue': 26, 'boat': 27, 'book': 28, 'boy': 29, 'brother': 30, 'brown': 31, 'bug': 32, 'bye': 33, 'callonphone': 34, 'can': 35, 'car': 36, 'carrot': 37, 'cat': 38, 'cereal': 39, 'chair': 40, 'cheek': 41, 'child': 42, 'chin': 43, 'chocolate': 44, 'clean': 45, 'close': 46, 'closet': 47, 'cloud': 48, 'clown': 49, 'cow': 50, 'cowboy': 51, 'cry': 52, 'cut': 53, 'cute': 54, 'dad': 55, 'dance': 56, 'dirty': 57, 'dog': 58, 'doll': 59, 'donkey': 60, 'down': 61, 'drawer': 62, 'drink': 63, 'drop': 64, 'dry': 65, 'dryer': 66, 'duck': 67, 'ear': 68, 'elephant': 69, 'empty': 70, 'every': 71, 'eye': 72, 'face': 73, 'fall': 74, 'farm': 75, 'fast': 76, 'feet': 77, 'find': 78, 'fine': 79, 'finger': 80, 'finish': 81, 'fireman': 82, 'first': 83, 'fish': 84, 'flag': 85, 'flower': 86, 'food': 87, 'for': 88, 'frenchfries': 89, 'frog': 90, 'garbage': 91, 'gift': 92, 'giraffe': 93, 'girl': 94, 'give': 95, 'glasswindow': 96, 'go': 97, 'goose': 98, 'grandma': 99, 'grandpa': 100, 'grass': 101, 'green': 102, 'gum': 103, 'hair': 104, 'happy': 105, 'hat': 106, 'hate': 107, 'have': 108, 'haveto': 109, 'head': 110, 'hear': 111, 'helicopter': 112, 'hello': 113, 'hen': 114, 'hesheit': 115, 'hide': 116, 'high': 117, 'home': 118, 'horse': 119, 'hot': 120, 'hungry': 121, 'icecream': 122, 'if': 123, 'into': 124, 'jacket': 125, 'jeans': 126, 'jump': 127, 'kiss': 128, 'kitty': 129, 'lamp': 130, 'later': 131, 'like': 132, 'lion': 133, 'lips': 134, 'listen': 135, 'look': 136, 'loud': 137, 'mad': 138, 'make': 139, 'man': 140, 'many': 141, 'milk': 142, 'minemy': 143, 'mitten': 144, 'mom': 145, 'moon': 146, 'morning': 147, 'mouse': 148, 'mouth': 149, 'nap': 150, 'napkin': 151, 'night': 152, 'no': 153, 'noisy': 154, 'nose': 155, 'not': 156, 'now': 157, 'nuts': 158, 'old': 159, 'on': 160, 'open': 161, 'orange': 162, 'outside': 163, 'owie': 164, 'owl': 165, 'pajamas': 166, 'pen': 167, 'pencil': 168, 'penny': 169, 'person': 170, 'pig': 171, 'pizza': 172, 'please': 173, 'police': 174, 'pool': 175, 'potty': 176, 'pretend': 177, 'pretty': 178, 'puppy': 179, 'puzzle': 180, 'quiet': 181, 'radio': 182, 'rain': 183, 'read': 184, 'red': 185, 'refrigerator': 186, 'ride': 187, 'room': 188, 'sad': 189, 'same': 190, 'say': 191, 'scissors': 192, 'see': 193, 'shhh': 194, 'shirt': 195, 'shoe': 196, 'shower': 197, 'sick': 198, 'sleep': 199, 'sleepy': 200, 'smile': 201, 'snack': 202, 'snow': 203, 'stairs': 204, 'stay': 205, 'sticky': 206, 'store': 207, 'story': 208, 'stuck': 209, 'sun': 210, 'table': 211, 'talk': 212, 'taste': 213, 'thankyou': 214, 'that': 215, 'there': 216, 'think': 217, 'thirsty': 218, 'tiger': 219, 'time': 220, 'tomorrow': 221, 'tongue': 222, 'tooth': 223, 'toothbrush': 224, 'touch': 225, 'toy': 226, 'tree': 227, 'uncle': 228, 'underwear': 229, 'up': 230, 'vacuum': 231, 'wait': 232, 'wake': 233, 'water': 234, 'wet': 235, 'weus': 236, 'where': 237, 'white': 238, 'who': 239, 'why': 240, 'will': 241, 'wolf': 242, 'yellow': 243, 'yes': 244, 'yesterday': 245, 'yourself': 246, 'yucky': 247, 'zebra': 248, 'zipper': 249}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>participant_id</th>\n",
       "      <th>sequence_id</th>\n",
       "      <th>sign</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_landmark_files/26734/1000035562.parquet</td>\n",
       "      <td>26734</td>\n",
       "      <td>1000035562</td>\n",
       "      <td>blow</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_landmark_files/28656/1000106739.parquet</td>\n",
       "      <td>28656</td>\n",
       "      <td>1000106739</td>\n",
       "      <td>wait</td>\n",
       "      <td>232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_landmark_files/16069/100015657.parquet</td>\n",
       "      <td>16069</td>\n",
       "      <td>100015657</td>\n",
       "      <td>cloud</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_landmark_files/25571/1000210073.parquet</td>\n",
       "      <td>25571</td>\n",
       "      <td>1000210073</td>\n",
       "      <td>bird</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_landmark_files/62590/1000240708.parquet</td>\n",
       "      <td>62590</td>\n",
       "      <td>1000240708</td>\n",
       "      <td>owie</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            path  participant_id  sequence_id  \\\n",
       "0  train_landmark_files/26734/1000035562.parquet           26734   1000035562   \n",
       "1  train_landmark_files/28656/1000106739.parquet           28656   1000106739   \n",
       "2   train_landmark_files/16069/100015657.parquet           16069    100015657   \n",
       "3  train_landmark_files/25571/1000210073.parquet           25571   1000210073   \n",
       "4  train_landmark_files/62590/1000240708.parquet           62590   1000240708   \n",
       "\n",
       "    sign  label  \n",
       "0   blow     25  \n",
       "1   wait    232  \n",
       "2  cloud     48  \n",
       "3   bird     23  \n",
       "4   owie    164  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_index = read_dict(f\"{CFG.data_path}sign_to_prediction_index_map.json\")\n",
    "index_label = dict([(label_index[key], key) for key in label_index])\n",
    "print(label_index)\n",
    "train[\"label\"] = train[\"sign\"].map(lambda sign: label_index[sign])\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.007629,
     "end_time": "2023-03-02T08:44:42.377",
     "exception": false,
     "start_time": "2023-03-02T08:44:42.369371",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Create Tensorflow Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-28T10:00:37.735320Z",
     "iopub.status.busy": "2023-04-28T10:00:37.734393Z",
     "iopub.status.idle": "2023-04-28T10:00:37.745983Z",
     "shell.execute_reply": "2023-04-28T10:00:37.744961Z",
     "shell.execute_reply.started": "2023-04-28T10:00:37.735279Z"
    },
    "papermill": {
     "duration": 0.019748,
     "end_time": "2023-03-02T08:44:42.404541",
     "exception": false,
     "start_time": "2023-03-02T08:44:42.384793",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def decode_function(record_bytes):\n",
    "    return tf.io.parse_single_example(\n",
    "          # Data\n",
    "          record_bytes,\n",
    "          # Schema\n",
    "          {\n",
    "              \"feature\": tf.io.FixedLenFeature([12 * 543 * 3], dtype=tf.float32),\n",
    "              \"label\": tf.io.FixedLenFeature([], dtype=tf.int64)\n",
    "          }\n",
    "      )\n",
    "def preprocess(item):\n",
    "    features = item[\"feature\"]\n",
    "#     features = tf.reshape(features, (1,CFG.sequence_length, 543,3))\n",
    "    features=tf.reshape(features, (1,12, 543, 3))\n",
    "#         \"face\"       : np.arange(0, 468),\n",
    "#     \"left_hand\"  : np.arange(468, 489),\n",
    "#     \"pose\"       : np.arange(489, 522),\n",
    "#     \"right_hand\" : np.arange(522, 543),\n",
    "    features=tf.transpose(features, perm=[3, 1, 2, 0])\n",
    "    features1=features[:,-5:,468:489,:]\n",
    "    features2=features[:,-5:,522:543,:]\n",
    "    features=tf.concat([features1, features2],2)\n",
    "    print(features.shape)\n",
    "    return features, item[\"label\"]         \n",
    "def make_dataset(file_paths, batch_size=128, mode=\"train\"):\n",
    "    ds = tf.data.TFRecordDataset(file_paths)\n",
    "    ds = ds.map(decode_function)\n",
    "    ds = ds.map(preprocess)\n",
    "    options = tf.data.Options()\n",
    "    if mode == \"train\":\n",
    "        ds = ds.shuffle(1024)\n",
    "        options.experimental_deterministic = False\n",
    "    ds = ds.batch(batch_size, drop_remainder=True)\n",
    "    ds = ds.with_options(options) \n",
    "    ds = ds.cache().prefetch(tf.data.AUTOTUNE)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_TYPES = ['left_hand', 'pose', 'right_hand']\n",
    "START_IDX = 468\n",
    "LIPS_IDXS0 = np.array([\n",
    "        61, 185, 40, 39, 37, 0, 267, 269, 270, 409,\n",
    "        291, 146, 91, 181, 84, 17, 314, 405, 321, 375,\n",
    "        78, 191, 80, 81, 82, 13, 312, 311, 310, 415,\n",
    "        95, 88, 178, 87, 14, 317, 402, 318, 324, 308,\n",
    "    ])\n",
    "# Landmark indices in original data\n",
    "LEFT_HAND_IDXS0 = np.arange(468,489)\n",
    "RIGHT_HAND_IDXS0 = np.arange(522,543)\n",
    "LEFT_POSE_IDXS0 = np.array([502, 504, 506, 508, 510])\n",
    "RIGHT_POSE_IDXS0 = np.array([503, 505, 507, 509, 511])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.load('X_train_20x61_left.npy')\n",
    "y_train = np.load('y_train_20x61_left.npy')\n",
    "x_test = np.load('X_test_20x61_left.npy')\n",
    "y_test = np.load('y_test_20x61_left.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIPS_reset = np.arange(len(LIPS_IDXS0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "       34, 35, 36, 37, 38, 39])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LIPS_reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "FACEMESH_LIPS = frozenset([(61, 146), (146, 91), (91, 181), (181, 84), (84, 17),\n",
    "                           (17, 314), (314, 405), (405, 321), (321, 375),\n",
    "                           (375, 291), (61, 185), (185, 40), (40, 39), (39, 37),\n",
    "                           (37, 0), (0, 267),\n",
    "                           (267, 269), (269, 270), (270, 409), (409, 291),\n",
    "                           (78, 95), (95, 88), (88, 178), (178, 87), (87, 14),\n",
    "                           (14, 317), (317, 402), (402, 318), (318, 324),\n",
    "                           (324, 308), (78, 191), (191, 80), (80, 81), (81, 82),\n",
    "                           (82, 13), (13, 312), (312, 311), (311, 310),\n",
    "                           (310, 415), (415, 308)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lips_index = []\n",
    "for link in FACEMESH_LIPS:\n",
    "    i,j = link\n",
    "    lips_index.append((21+LIPS_reset[np.where(LIPS_IDXS0 == i)[0][0]],21+LIPS_reset[np.where(LIPS_IDXS0 == j)[0][0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train[:,:,:,:2]\n",
    "x_test = x_test[:,:,:,:2]\n",
    "x_train = np.transpose(x_train,(0, 3,1, 2))\n",
    "x_test = np.transpose(x_test,(0,3, 1, 2))\n",
    "#x_train = np.expand_dims(x_train,axis=-1)\n",
    "#x_test = np.expand_dims(x_test,axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-28T10:00:39.323274Z",
     "iopub.status.busy": "2023-04-28T10:00:39.322502Z",
     "iopub.status.idle": "2023-04-28T10:00:42.286313Z",
     "shell.execute_reply": "2023-04-28T10:00:42.285220Z",
     "shell.execute_reply.started": "2023-04-28T10:00:39.323222Z"
    },
    "papermill": {
     "duration": 0.017938,
     "end_time": "2023-03-02T08:44:42.43015",
     "exception": false,
     "start_time": "2023-03-02T08:44:42.412212",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "unique_ids = np.array(sorted(train.participant_id.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-28T10:01:29.918426Z",
     "iopub.status.busy": "2023-04-28T10:01:29.917624Z",
     "iopub.status.idle": "2023-04-28T10:01:29.928310Z",
     "shell.execute_reply": "2023-04-28T10:01:29.926731Z",
     "shell.execute_reply.started": "2023-04-28T10:01:29.918383Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def edge2mat(link, num_node):\n",
    "    A = np.zeros((num_node, num_node))\n",
    "    for i, j in link:\n",
    "        A[j, i] = 1\n",
    "    return A\n",
    "\n",
    "\n",
    "def normalize_digraph(A):  # 除以每列的和\n",
    "    Dl = np.sum(A, 0)\n",
    "    h, w = A.shape\n",
    "    Dn = np.zeros((w, w))\n",
    "    for i in range(w):\n",
    "        if Dl[i] > 0:\n",
    "            Dn[i, i] = Dl[i] ** (-1)\n",
    "    AD = np.dot(A, Dn)\n",
    "    return AD\n",
    "\n",
    "\n",
    "def get_spatial_graph(num_node, self_link, inward, outward):\n",
    "    I = edge2mat(self_link, num_node)\n",
    "    In = normalize_digraph(edge2mat(inward, num_node))\n",
    "    Out = normalize_digraph(edge2mat(outward, num_node))\n",
    "    A = np.stack((I, In, Out))\n",
    "    return A\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![handlandmark](https://developers.google.com/static/mediapipe/images/solutions/hand-landmarks.png) create the node graph for hand landmarks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-28T10:01:35.614777Z",
     "iopub.status.busy": "2023-04-28T10:01:35.614391Z",
     "iopub.status.idle": "2023-04-28T10:01:35.626798Z",
     "shell.execute_reply": "2023-04-28T10:01:35.624837Z",
     "shell.execute_reply.started": "2023-04-28T10:01:35.614740Z"
    }
   },
   "outputs": [],
   "source": [
    "num_node = 61\n",
    "self_link = [(i, i) for i in range(num_node)]\n",
    "inward_ori_index = [(1, 2), (2, 3), (3, 4), (4, 5), (1, 6), (6, 7), (7, 8),\n",
    "                    (8, 9), (6, 10), (10, 11), (11, 12), (12, 13), (10, 14),\n",
    "                    (14, 15), (15, 16), (16, 17), (14, 18), (18, 19), (19, 20),\n",
    "                    (20, 21), (18, 1)]\n",
    "inward_ori_index2=[(1+21, 2+21), (2+21, 3+21), (3+21, 4+21), (4+21, 5+21), (1+21, 6), \n",
    "                   (6+21, 7+21), (7+21, 8+21), (8+21, 9+21), (6+21, 10+21), \n",
    "                    (10+21, 11+21), (11+21, 12+21), (12+21, 13+21), (10+21, 14+21),\n",
    "                    (14+21, 15+21), (15+21, 16+21), (16+21, 17+21), (14+21, 18+21), (18+21, 19+21), (19+21, 20+21),\n",
    "                    (20+21, 21+21), (18+21, 1+21)]\n",
    "inward_ori_index.extend(lips_index)\n",
    "inward = [(i - 1, j - 1) for (i, j) in inward_ori_index]\n",
    "outward = [(j, i) for (i, j) in inward]\n",
    "neighbor = inward + outward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(outward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-28T10:02:02.720112Z",
     "iopub.status.busy": "2023-04-28T10:02:02.719314Z",
     "iopub.status.idle": "2023-04-28T10:02:02.726845Z",
     "shell.execute_reply": "2023-04-28T10:02:02.725718Z",
     "shell.execute_reply.started": "2023-04-28T10:02:02.720072Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class Graph:\n",
    "    def __init__(self, labeling_mode='spatial'):\n",
    "        self.A = self.get_adjacency_matrix(labeling_mode)\n",
    "        self.num_node = num_node\n",
    "        self.self_link = self_link\n",
    "        self.inward = inward\n",
    "        self.outward = outward\n",
    "        self.neighbor = neighbor\n",
    "\n",
    "    def get_adjacency_matrix(self, labeling_mode=None):\n",
    "        if labeling_mode is None:\n",
    "            return self.A\n",
    "        if labeling_mode == 'spatial':\n",
    "            A = get_spatial_graph(num_node, self_link, inward, outward)\n",
    "        else:\n",
    "            raise ValueError()\n",
    "        return A\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.007677,
     "end_time": "2023-03-02T08:44:42.445922",
     "exception": false,
     "start_time": "2023-03-02T08:44:42.438245",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-28T10:31:59.243333Z",
     "iopub.status.busy": "2023-04-28T10:31:59.242459Z",
     "iopub.status.idle": "2023-04-28T10:31:59.271826Z",
     "shell.execute_reply": "2023-04-28T10:31:59.270679Z",
     "shell.execute_reply.started": "2023-04-28T10:31:59.243271Z"
    }
   },
   "outputs": [],
   "source": [
    "REGULARIZER = tf.keras.regularizers.l2(l=0.001)\n",
    "INITIALIZER = tf.keras.initializers.VarianceScaling(scale=2.,\n",
    "                                                    mode=\"fan_out\",\n",
    "                                                    distribution=\"truncated_normal\")\n",
    "def SGCN(old_filters,filters,kernel_size,INITIALIZER,REGULARIZER,graph_A):\n",
    "    #(1, 2, 20, 61) (3, 61, 61)\n",
    "    \n",
    "    input_tensor = tf.keras.layers.Input(shape=[old_filters,None,61], dtype=tf.float32)\n",
    "    A = tf.Variable(graph_A, dtype=tf.float32, trainable=False, name='adjacency_matrix')\n",
    "    conv = tf.keras.layers.Conv2D(filters*kernel_size,\n",
    "                                  kernel_size=1,\n",
    "                                  padding='same',\n",
    "                                  kernel_initializer=INITIALIZER,\n",
    "                                  data_format='channels_first',\n",
    "                                  kernel_regularizer=REGULARIZER)(input_tensor)\n",
    "\n",
    "    N = tf.shape(conv)[0]\n",
    "    C = tf.shape(conv)[1]\n",
    "    T = tf.shape(conv)[2]\n",
    "    V = tf.shape(conv)[3]\n",
    "    x = tf.reshape(conv,[N,kernel_size, C//kernel_size, T, V])\n",
    "\n",
    "    x = tf.keras.layers.Lambda(lambda y: tf.einsum('nkctv,kvw->nctw', y[0], y[1]))([x, A])\n",
    "    model = tf.keras.Model(inputs=input_tensor, outputs=x, name='SGCN')\n",
    "    return model\n",
    "\n",
    "\n",
    "#(1, 2, 20, 61) (3, 61, 61)\n",
    "def STGCN(old_filters=2,filters=64, kernel_size=[9, 3], stride=1, activation='relu', residual=True, downsample=False,\\\n",
    "          INITIALIZER=INITIALIZER,REGULARIZER=REGULARIZER,graph_A=graph_A):\n",
    "    input_tensor = tf.keras.layers.Input(shape=[old_filters, None, 61])\n",
    "\n",
    "    sgcn_output = SGCN(old_filters,filters, kernel_size=kernel_size[1],INITIALIZER=INITIALIZER,\\\n",
    "                                     REGULARIZER=REGULARIZER,graph_A=graph_A)(input_tensor)\n",
    "    tgcn_output = tf.keras.layers.BatchNormalization(axis=1)(sgcn_output)\n",
    "    tgcn_output = tf.keras.layers.Activation(activation)(tgcn_output)\n",
    "    tgcn_output = tf.keras.layers.Conv2D(filters, kernel_size=[kernel_size[0], 1], strides=[stride, 1], padding='same',\n",
    "                                          kernel_initializer=INITIALIZER, data_format='channels_first', \\\n",
    "                                         kernel_regularizer=REGULARIZER)(tgcn_output)\n",
    "    tgcn_output = tf.keras.layers.BatchNormalization(axis=1)(tgcn_output)\n",
    "    if not residual:\n",
    "        res = tf.zeros_like(tgcn_output)\n",
    "    elif residual and stride != 1 or downsample:\n",
    "        res = tf.keras.layers.Conv2D(filters, kernel_size=[1, 1], strides=[stride, 1], padding='same', kernel_initializer=INITIALIZER,\n",
    "                                   data_format='channels_first', kernel_regularizer=REGULARIZER)(input_tensor)\n",
    "        res = tf.keras.layers.BatchNormalization(axis=1)(res)\n",
    "    else:\n",
    "        res = input_tensor\n",
    "    x = tf.keras.layers.add([tgcn_output, res])\n",
    "    x = tf.keras.layers.Activation(activation)(x)\n",
    "    model = tf.keras.models.Model(inputs=input_tensor, outputs=x)\n",
    "    return model\n",
    "\n",
    "def MainModel(num_classes=250,graph_A=graph_A):\n",
    "    #(N, in_channels, T_{in}, V_{in}, M_{in})\n",
    "    #graph = Graph()\n",
    "    input_tensor = tf.keras.layers.Input(shape=[2, 20, 61,1],dtype=tf.float32)\n",
    "    \n",
    "    x = tf.keras.layers.BatchNormalization(axis=1, input_shape=(2, 20, 61))(input_tensor)\n",
    "    x = STGCN(old_filters=2,filters=64, residual=False,graph_A=graph_A)(x)\n",
    "    x = STGCN(old_filters=64,filters=128, stride=2, downsample=True,graph_A=graph_A)(x)\n",
    "    x = STGCN(old_filters=128,filters=256, stride=2, downsample=True,graph_A=graph_A)(x)\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D(data_format='channels_first')(x)\n",
    "    x = tf.keras.layers.Reshape((1,256,1,1))(x)\n",
    "    x = tf.keras.layers.Conv2D(num_classes,\n",
    "                           kernel_size=1,\n",
    "                           padding='same',\n",
    "                           kernel_initializer=INITIALIZER,\n",
    "                           data_format='channels_first',\n",
    "                           kernel_regularizer=REGULARIZER)(x)\n",
    "    x = tf.keras.layers.Reshape((-1,))(x)\n",
    "    x = tf.keras.layers.Softmax(axis=-1)(x)\n",
    "    model = tf.keras.models.Model(inputs=input_tensor, outputs=x)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-28T10:31:59.350200Z",
     "iopub.status.busy": "2023-04-28T10:31:59.349922Z",
     "iopub.status.idle": "2023-04-28T10:31:59.496392Z",
     "shell.execute_reply": "2023-04-28T10:31:59.495394Z",
     "shell.execute_reply.started": "2023-04-28T10:31:59.350174Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 2, 20, 61, 1)]    0         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 2, 20, 61, 1)     8         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " model (Functional)          (None, 64, None, 61)      38016     \n",
      "                                                                 \n",
      " model_1 (Functional)        (None, 128, None, 61)     182400    \n",
      "                                                                 \n",
      " model_2 (Functional)        (None, 256, None, 61)     725248    \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 256)              0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 1, 256, 1, 1)      0         \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 1, 250, 1, 1)      64250     \n",
      "                                                                 \n",
      " reshape_1 (Reshape)         (None, 250)               0         \n",
      "                                                                 \n",
      " softmax (Softmax)           (None, 250)               0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,009,922\n",
      "Trainable params: 1,007,358\n",
      "Non-trainable params: 2,564\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "graph_A = graph.A\n",
    "model = MainModel(graph_A=graph_A)\n",
    "model.summary()\n",
    "model.compile(\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics=[\n",
    "            \"accuracy\",\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-11T12:58:57.432016Z",
     "iopub.status.busy": "2023-04-11T12:58:57.431467Z",
     "iopub.status.idle": "2023-04-11T12:58:58.610898Z",
     "shell.execute_reply": "2023-04-11T12:58:58.609576Z",
     "shell.execute_reply.started": "2023-04-11T12:58:57.431981Z"
    },
    "papermill": {
     "duration": 3.974692,
     "end_time": "2023-03-02T08:44:46.689625",
     "exception": false,
     "start_time": "2023-03-02T08:44:42.714933",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# If True, processing data from scratch\n",
    "# If False, loads preprocessed data\n",
    "PREPROCESS_DATA = False\n",
    "TRAIN_MODEL = True\n",
    "# True: use 10% of participants as validation set\n",
    "# False: use all data for training -> gives better LB result\n",
    "USE_VAL = False\n",
    "N_ROWS = 543\n",
    "N_DIMS = 3\n",
    "DIM_NAMES = ['x', 'y', 'z']\n",
    "SEED = 42\n",
    "NUM_CLASSES = 250\n",
    "INPUT_SIZE = 64\n",
    "BATCH_ALL_SIGNS_N = 4\n",
    "BATCH_SIZE = 256\n",
    "N_EPOCHS = 100\n",
    "LR_MAX = 1e-3\n",
    "N_WARMUP_EPOCHS = 0\n",
    "WD_RATIO = 0.05\n",
    "MASK_VAL = 4237\n",
    "N_COLS = 61\n",
    "# Custom callback to update weight decay with learning rate\n",
    "class WeightDecayCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, wd_ratio=WD_RATIO):\n",
    "        self.step_counter = 0\n",
    "        self.wd_ratio = wd_ratio\n",
    "    \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        model.optimizer.weight_decay = model.optimizer.learning_rate * self.wd_ratio\n",
    "        print(f'learning rate: {model.optimizer.learning_rate.numpy():.2e}, weight decay: {model.optimizer.weight_decay.numpy():.2e}')\n",
    "\n",
    "def lrfn(current_step, num_warmup_steps, lr_max, num_cycles=0.50, num_training_steps=N_EPOCHS):\n",
    "    \n",
    "    if current_step < num_warmup_steps:\n",
    "        if WARMUP_METHOD == 'log':\n",
    "            return lr_max * 0.10 ** (num_warmup_steps - current_step)\n",
    "        else:\n",
    "            return lr_max * 2 ** -(num_warmup_steps - current_step)\n",
    "    else:\n",
    "        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n",
    "\n",
    "        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress))) * lr_max\n",
    "# Learning rate for encoder\n",
    "LR_SCHEDULE = [lrfn(step, num_warmup_steps=N_WARMUP_EPOCHS, lr_max=LR_MAX, num_cycles=0.50) for step in range(N_EPOCHS)]\n",
    "lr_callback = tf.keras.callbacks.LearningRateScheduler(lambda step: LR_SCHEDULE[step], verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 2, 20, 61, 1)]    0         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 2, 20, 61, 1)     8         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " model (Functional)          (None, 64, None, 61)      38016     \n",
      "                                                                 \n",
      " model_1 (Functional)        (None, 128, None, 61)     182400    \n",
      "                                                                 \n",
      " model_2 (Functional)        (None, 256, None, 61)     725248    \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 256)              0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 1, 256, 1, 1)      0         \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 1, 250, 1, 1)      64250     \n",
      "                                                                 \n",
      " reshape_1 (Reshape)         (None, 250)               0         \n",
      "                                                                 \n",
      " softmax (Softmax)           (None, 250)               0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,009,922\n",
      "Trainable params: 1,007,358\n",
      "Non-trainable params: 2,564\n",
      "_________________________________________________________________\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.001.\n",
      "learning rate: 1.00e-03, weight decay: 5.00e-05\n",
      "Epoch 1/100\n",
      "  5/590 [..............................] - ETA: 27s - loss: 7.9487 - accuracy: 0.0047     WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0047s vs `on_train_batch_end` time: 0.0351s). Check your callbacks.\n",
      "589/590 [============================>.] - ETA: 0s - loss: 5.3707 - accuracy: 0.1149\n",
      "Epoch 1: val_accuracy improved from -inf to 0.17241, saving model to models/weights_stgcn6.h5\n",
      "590/590 [==============================] - 32s 51ms/step - loss: 5.3688 - accuracy: 0.1151 - val_loss: 4.6401 - val_accuracy: 0.1724 - lr: 0.0010\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.0009997532801828658.\n",
      "learning rate: 1.00e-03, weight decay: 5.00e-05\n",
      "Epoch 2/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 3.7939 - accuracy: 0.3136\n",
      "Epoch 2: val_accuracy improved from 0.17241 to 0.29377, saving model to models/weights_stgcn6.h5\n",
      "590/590 [==============================] - 31s 52ms/step - loss: 3.7939 - accuracy: 0.3136 - val_loss: 3.8719 - val_accuracy: 0.2938 - lr: 9.9975e-04\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.0009990133642141358.\n",
      "learning rate: 9.99e-04, weight decay: 5.00e-05\n",
      "Epoch 3/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 3.2802 - accuracy: 0.4266\n",
      "Epoch 3: val_accuracy improved from 0.29377 to 0.31560, saving model to models/weights_stgcn6.h5\n",
      "590/590 [==============================] - 31s 52ms/step - loss: 3.2802 - accuracy: 0.4266 - val_loss: 3.7164 - val_accuracy: 0.3156 - lr: 9.9901e-04\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.00099778098230154.\n",
      "learning rate: 9.98e-04, weight decay: 4.99e-05\n",
      "Epoch 4/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 3.0097 - accuracy: 0.4897\n",
      "Epoch 4: val_accuracy improved from 0.31560 to 0.45024, saving model to models/weights_stgcn6.h5\n",
      "590/590 [==============================] - 31s 52ms/step - loss: 3.0095 - accuracy: 0.4897 - val_loss: 3.2055 - val_accuracy: 0.4502 - lr: 9.9778e-04\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.000996057350657239.\n",
      "learning rate: 9.96e-04, weight decay: 4.98e-05\n",
      "Epoch 5/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 2.8335 - accuracy: 0.5327\n",
      "Epoch 5: val_accuracy improved from 0.45024 to 0.48090, saving model to models/weights_stgcn6.h5\n",
      "590/590 [==============================] - 32s 54ms/step - loss: 2.8335 - accuracy: 0.5327 - val_loss: 3.0407 - val_accuracy: 0.4809 - lr: 9.9606e-04\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.0009938441702975688.\n",
      "learning rate: 9.94e-04, weight decay: 4.97e-05\n",
      "Epoch 6/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.7151 - accuracy: 0.5608\n",
      "Epoch 6: val_accuracy did not improve from 0.48090\n",
      "590/590 [==============================] - 32s 55ms/step - loss: 2.7158 - accuracy: 0.5606 - val_loss: 3.0485 - val_accuracy: 0.4761 - lr: 9.9384e-04\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.0009911436253643444.\n",
      "learning rate: 9.91e-04, weight decay: 4.96e-05\n",
      "Epoch 7/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 2.6317 - accuracy: 0.5822\n",
      "Epoch 7: val_accuracy did not improve from 0.48090\n",
      "590/590 [==============================] - 30s 51ms/step - loss: 2.6317 - accuracy: 0.5822 - val_loss: 3.2041 - val_accuracy: 0.4496 - lr: 9.9114e-04\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 0.0009879583809693738.\n",
      "learning rate: 9.88e-04, weight decay: 4.94e-05\n",
      "Epoch 8/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 2.5697 - accuracy: 0.5969\n",
      "Epoch 8: val_accuracy improved from 0.48090 to 0.50359, saving model to models/weights_stgcn6.h5\n",
      "590/590 [==============================] - 32s 54ms/step - loss: 2.5697 - accuracy: 0.5969 - val_loss: 2.9407 - val_accuracy: 0.5036 - lr: 9.8796e-04\n",
      "\n",
      "Epoch 9: LearningRateScheduler setting learning rate to 0.0009842915805643156.\n",
      "learning rate: 9.84e-04, weight decay: 4.92e-05\n",
      "Epoch 9/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.5196 - accuracy: 0.6130\n",
      "Epoch 9: val_accuracy improved from 0.50359 to 0.55319, saving model to models/weights_stgcn6.h5\n",
      "590/590 [==============================] - 32s 54ms/step - loss: 2.5193 - accuracy: 0.6131 - val_loss: 2.7627 - val_accuracy: 0.5532 - lr: 9.8429e-04\n",
      "\n",
      "Epoch 10: LearningRateScheduler setting learning rate to 0.0009801468428384716.\n",
      "learning rate: 9.80e-04, weight decay: 4.90e-05\n",
      "Epoch 10/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.4792 - accuracy: 0.6239\n",
      "Epoch 10: val_accuracy did not improve from 0.55319\n",
      "590/590 [==============================] - 30s 51ms/step - loss: 2.4794 - accuracy: 0.6239 - val_loss: 2.8312 - val_accuracy: 0.5428 - lr: 9.8015e-04\n",
      "\n",
      "Epoch 11: LearningRateScheduler setting learning rate to 0.0009755282581475768.\n",
      "learning rate: 9.76e-04, weight decay: 4.88e-05\n",
      "Epoch 11/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 2.4472 - accuracy: 0.6343\n",
      "Epoch 11: val_accuracy did not improve from 0.55319\n",
      "590/590 [==============================] - 30s 51ms/step - loss: 2.4472 - accuracy: 0.6343 - val_loss: 2.8712 - val_accuracy: 0.5295 - lr: 9.7553e-04\n",
      "\n",
      "Epoch 12: LearningRateScheduler setting learning rate to 0.0009704403844771128.\n",
      "learning rate: 9.70e-04, weight decay: 4.85e-05\n",
      "Epoch 12/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.4179 - accuracy: 0.6444\n",
      "Epoch 12: val_accuracy improved from 0.55319 to 0.56817, saving model to models/weights_stgcn6.h5\n",
      "590/590 [==============================] - 31s 53ms/step - loss: 2.4181 - accuracy: 0.6444 - val_loss: 2.7175 - val_accuracy: 0.5682 - lr: 9.7044e-04\n",
      "\n",
      "Epoch 13: LearningRateScheduler setting learning rate to 0.0009648882429441257.\n",
      "learning rate: 9.65e-04, weight decay: 4.82e-05\n",
      "Epoch 13/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 2.3945 - accuracy: 0.6495\n",
      "Epoch 13: val_accuracy did not improve from 0.56817\n",
      "590/590 [==============================] - 33s 56ms/step - loss: 2.3945 - accuracy: 0.6495 - val_loss: 3.0089 - val_accuracy: 0.4992 - lr: 9.6489e-04\n",
      "\n",
      "Epoch 14: LearningRateScheduler setting learning rate to 0.0009588773128419905.\n",
      "learning rate: 9.59e-04, weight decay: 4.79e-05\n",
      "Epoch 14/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 2.3754 - accuracy: 0.6573\n",
      "Epoch 14: val_accuracy did not improve from 0.56817\n",
      "590/590 [==============================] - 34s 58ms/step - loss: 2.3754 - accuracy: 0.6573 - val_loss: 2.7439 - val_accuracy: 0.5650 - lr: 9.5888e-04\n",
      "\n",
      "Epoch 15: LearningRateScheduler setting learning rate to 0.0009524135262330098.\n",
      "learning rate: 9.52e-04, weight decay: 4.76e-05\n",
      "Epoch 15/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 2.3546 - accuracy: 0.6628\n",
      "Epoch 15: val_accuracy improved from 0.56817 to 0.59065, saving model to models/weights_stgcn6.h5\n",
      "590/590 [==============================] - 33s 55ms/step - loss: 2.3546 - accuracy: 0.6628 - val_loss: 2.6475 - val_accuracy: 0.5906 - lr: 9.5241e-04\n",
      "\n",
      "Epoch 16: LearningRateScheduler setting learning rate to 0.0009455032620941839.\n",
      "learning rate: 9.46e-04, weight decay: 4.73e-05\n",
      "Epoch 16/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 2.3399 - accuracy: 0.6690\n",
      "Epoch 16: val_accuracy improved from 0.59065 to 0.60017, saving model to models/weights_stgcn6.h5\n",
      "590/590 [==============================] - 33s 56ms/step - loss: 2.3399 - accuracy: 0.6690 - val_loss: 2.6255 - val_accuracy: 0.6002 - lr: 9.4550e-04\n",
      "\n",
      "Epoch 17: LearningRateScheduler setting learning rate to 0.0009381533400219318.\n",
      "learning rate: 9.38e-04, weight decay: 4.69e-05\n",
      "Epoch 17/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.3237 - accuracy: 0.6735\n",
      "Epoch 17: val_accuracy did not improve from 0.60017\n",
      "590/590 [==============================] - 30s 51ms/step - loss: 2.3237 - accuracy: 0.6734 - val_loss: 2.7073 - val_accuracy: 0.5768 - lr: 9.3815e-04\n",
      "\n",
      "Epoch 18: LearningRateScheduler setting learning rate to 0.0009303710135019718.\n",
      "learning rate: 9.30e-04, weight decay: 4.65e-05\n",
      "Epoch 18/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.3084 - accuracy: 0.6804\n",
      "Epoch 18: val_accuracy did not improve from 0.60017\n",
      "590/590 [==============================] - 30s 51ms/step - loss: 2.3089 - accuracy: 0.6802 - val_loss: 2.9562 - val_accuracy: 0.5349 - lr: 9.3037e-04\n",
      "\n",
      "Epoch 19: LearningRateScheduler setting learning rate to 0.0009221639627510075.\n",
      "learning rate: 9.22e-04, weight decay: 4.61e-05\n",
      "Epoch 19/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 2.2988 - accuracy: 0.6849\n",
      "Epoch 19: val_accuracy did not improve from 0.60017\n",
      "590/590 [==============================] - 30s 51ms/step - loss: 2.2988 - accuracy: 0.6849 - val_loss: 2.7835 - val_accuracy: 0.5642 - lr: 9.2216e-04\n",
      "\n",
      "Epoch 20: LearningRateScheduler setting learning rate to 0.0009135402871372809.\n",
      "learning rate: 9.14e-04, weight decay: 4.57e-05\n",
      "Epoch 20/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.2853 - accuracy: 0.6898\n",
      "Epoch 20: val_accuracy did not improve from 0.60017\n",
      "590/590 [==============================] - 30s 51ms/step - loss: 2.2853 - accuracy: 0.6898 - val_loss: 2.7331 - val_accuracy: 0.5796 - lr: 9.1354e-04\n",
      "\n",
      "Epoch 21: LearningRateScheduler setting learning rate to 0.0009045084971874737.\n",
      "learning rate: 9.05e-04, weight decay: 4.52e-05\n",
      "Epoch 21/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.2719 - accuracy: 0.6944\n",
      "Epoch 21: val_accuracy did not improve from 0.60017\n",
      "590/590 [==============================] - 30s 51ms/step - loss: 2.2720 - accuracy: 0.6943 - val_loss: 2.8530 - val_accuracy: 0.5433 - lr: 9.0451e-04\n",
      "\n",
      "Epoch 22: LearningRateScheduler setting learning rate to 0.0008950775061878452.\n",
      "learning rate: 8.95e-04, weight decay: 4.48e-05\n",
      "Epoch 22/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.2612 - accuracy: 0.6984\n",
      "Epoch 22: val_accuracy improved from 0.60017 to 0.60568, saving model to models/weights_stgcn6.h5\n",
      "590/590 [==============================] - 30s 51ms/step - loss: 2.2611 - accuracy: 0.6985 - val_loss: 2.6464 - val_accuracy: 0.6057 - lr: 8.9508e-04\n",
      "\n",
      "Epoch 23: LearningRateScheduler setting learning rate to 0.0008852566213878947.\n",
      "learning rate: 8.85e-04, weight decay: 4.43e-05\n",
      "Epoch 23/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 2.2544 - accuracy: 0.7017\n",
      "Epoch 23: val_accuracy improved from 0.60568 to 0.61184, saving model to models/weights_stgcn6.h5\n",
      "590/590 [==============================] - 30s 51ms/step - loss: 2.2544 - accuracy: 0.7017 - val_loss: 2.6581 - val_accuracy: 0.6118 - lr: 8.8526e-04\n",
      "\n",
      "Epoch 24: LearningRateScheduler setting learning rate to 0.0008750555348152298.\n",
      "learning rate: 8.75e-04, weight decay: 4.38e-05\n",
      "Epoch 24/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 2.2450 - accuracy: 0.7057\n",
      "Epoch 24: val_accuracy did not improve from 0.61184\n",
      "590/590 [==============================] - 30s 51ms/step - loss: 2.2450 - accuracy: 0.7057 - val_loss: 2.7633 - val_accuracy: 0.5777 - lr: 8.7506e-04\n",
      "\n",
      "Epoch 25: LearningRateScheduler setting learning rate to 0.0008644843137107057.\n",
      "learning rate: 8.64e-04, weight decay: 4.32e-05\n",
      "Epoch 25/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.2381 - accuracy: 0.7114\n",
      "Epoch 25: val_accuracy did not improve from 0.61184\n",
      "590/590 [==============================] - 30s 51ms/step - loss: 2.2378 - accuracy: 0.7114 - val_loss: 2.8082 - val_accuracy: 0.5759 - lr: 8.6448e-04\n",
      "\n",
      "Epoch 26: LearningRateScheduler setting learning rate to 0.0008535533905932737.\n",
      "learning rate: 8.54e-04, weight decay: 4.27e-05\n",
      "Epoch 26/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.2282 - accuracy: 0.7152\n",
      "Epoch 26: val_accuracy improved from 0.61184 to 0.64705, saving model to models/weights_stgcn6.h5\n",
      "590/590 [==============================] - 30s 51ms/step - loss: 2.2279 - accuracy: 0.7153 - val_loss: 2.5050 - val_accuracy: 0.6470 - lr: 8.5355e-04\n",
      "\n",
      "Epoch 27: LearningRateScheduler setting learning rate to 0.0008422735529643444.\n",
      "learning rate: 8.42e-04, weight decay: 4.21e-05\n",
      "Epoch 27/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.2218 - accuracy: 0.7187\n",
      "Epoch 27: val_accuracy did not improve from 0.64705\n",
      "590/590 [==============================] - 30s 51ms/step - loss: 2.2218 - accuracy: 0.7187 - val_loss: 2.6379 - val_accuracy: 0.6169 - lr: 8.4227e-04\n",
      "\n",
      "Epoch 28: LearningRateScheduler setting learning rate to 0.0008306559326618259.\n",
      "learning rate: 8.31e-04, weight decay: 4.15e-05\n",
      "Epoch 28/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.2134 - accuracy: 0.7203\n",
      "Epoch 28: val_accuracy improved from 0.64705 to 0.64876, saving model to models/weights_stgcn6.h5\n",
      "590/590 [==============================] - 30s 51ms/step - loss: 2.2136 - accuracy: 0.7202 - val_loss: 2.5306 - val_accuracy: 0.6488 - lr: 8.3066e-04\n",
      "\n",
      "Epoch 29: LearningRateScheduler setting learning rate to 0.0008187119948743449.\n",
      "learning rate: 8.19e-04, weight decay: 4.09e-05\n",
      "Epoch 29/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.2081 - accuracy: 0.7251\n",
      "Epoch 29: val_accuracy did not improve from 0.64876\n",
      "590/590 [==============================] - 30s 51ms/step - loss: 2.2082 - accuracy: 0.7251 - val_loss: 2.6090 - val_accuracy: 0.6295 - lr: 8.1871e-04\n",
      "\n",
      "Epoch 30: LearningRateScheduler setting learning rate to 0.0008064535268264883.\n",
      "learning rate: 8.06e-04, weight decay: 4.03e-05\n",
      "Epoch 30/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.1996 - accuracy: 0.7291\n",
      "Epoch 30: val_accuracy did not improve from 0.64876\n",
      "590/590 [==============================] - 30s 51ms/step - loss: 2.1995 - accuracy: 0.7290 - val_loss: 2.6077 - val_accuracy: 0.6271 - lr: 8.0645e-04\n",
      "\n",
      "Epoch 31: LearningRateScheduler setting learning rate to 0.0007938926261462366.\n",
      "learning rate: 7.94e-04, weight decay: 3.97e-05\n",
      "Epoch 31/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 2.1895 - accuracy: 0.7353\n",
      "Epoch 31: val_accuracy did not improve from 0.64876\n",
      "590/590 [==============================] - 30s 51ms/step - loss: 2.1895 - accuracy: 0.7353 - val_loss: 2.5561 - val_accuracy: 0.6457 - lr: 7.9389e-04\n",
      "\n",
      "Epoch 32: LearningRateScheduler setting learning rate to 0.0007810416889260653.\n",
      "learning rate: 7.81e-04, weight decay: 3.91e-05\n",
      "Epoch 32/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.1883 - accuracy: 0.7377\n",
      "Epoch 32: val_accuracy did not improve from 0.64876\n",
      "590/590 [==============================] - 30s 51ms/step - loss: 2.1887 - accuracy: 0.7375 - val_loss: 2.8635 - val_accuracy: 0.5839 - lr: 7.8104e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 33: LearningRateScheduler setting learning rate to 0.0007679133974894983.\n",
      "learning rate: 7.68e-04, weight decay: 3.84e-05\n",
      "Epoch 33/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.1807 - accuracy: 0.7402\n",
      "Epoch 33: val_accuracy improved from 0.64876 to 0.66160, saving model to models/weights_stgcn6.h5\n",
      "590/590 [==============================] - 30s 51ms/step - loss: 2.1811 - accuracy: 0.7401 - val_loss: 2.5102 - val_accuracy: 0.6616 - lr: 7.6791e-04\n",
      "\n",
      "Epoch 34: LearningRateScheduler setting learning rate to 0.0007545207078751857.\n",
      "learning rate: 7.55e-04, weight decay: 3.77e-05\n",
      "Epoch 34/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 2.1737 - accuracy: 0.7450\n",
      "Epoch 34: val_accuracy did not improve from 0.66160\n",
      "590/590 [==============================] - 30s 51ms/step - loss: 2.1737 - accuracy: 0.7450 - val_loss: 2.7104 - val_accuracy: 0.6205 - lr: 7.5452e-04\n",
      "\n",
      "Epoch 35: LearningRateScheduler setting learning rate to 0.0007408768370508576.\n",
      "learning rate: 7.41e-04, weight decay: 3.70e-05\n",
      "Epoch 35/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 2.1682 - accuracy: 0.7477\n",
      "Epoch 35: val_accuracy improved from 0.66160 to 0.66165, saving model to models/weights_stgcn6.h5\n",
      "590/590 [==============================] - 30s 51ms/step - loss: 2.1682 - accuracy: 0.7477 - val_loss: 2.5222 - val_accuracy: 0.6617 - lr: 7.4088e-04\n",
      "\n",
      "Epoch 36: LearningRateScheduler setting learning rate to 0.0007269952498697733.\n",
      "learning rate: 7.27e-04, weight decay: 3.63e-05\n",
      "Epoch 36/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.1602 - accuracy: 0.7520\n",
      "Epoch 36: val_accuracy did not improve from 0.66165\n",
      "590/590 [==============================] - 30s 51ms/step - loss: 2.1602 - accuracy: 0.7520 - val_loss: 2.5920 - val_accuracy: 0.6490 - lr: 7.2700e-04\n",
      "\n",
      "Epoch 37: LearningRateScheduler setting learning rate to 0.0007128896457825364.\n",
      "learning rate: 7.13e-04, weight decay: 3.56e-05\n",
      "Epoch 37/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.1546 - accuracy: 0.7558\n",
      "Epoch 37: val_accuracy improved from 0.66165 to 0.67963, saving model to models/weights_stgcn6.h5\n",
      "590/590 [==============================] - 30s 51ms/step - loss: 2.1547 - accuracy: 0.7557 - val_loss: 2.4646 - val_accuracy: 0.6796 - lr: 7.1289e-04\n",
      "\n",
      "Epoch 38: LearningRateScheduler setting learning rate to 0.0006985739453173903.\n",
      "learning rate: 6.99e-04, weight decay: 3.49e-05\n",
      "Epoch 38/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.1505 - accuracy: 0.7581\n",
      "Epoch 38: val_accuracy did not improve from 0.67963\n",
      "590/590 [==============================] - 30s 51ms/step - loss: 2.1502 - accuracy: 0.7582 - val_loss: 2.5184 - val_accuracy: 0.6729 - lr: 6.9857e-04\n",
      "\n",
      "Epoch 39: LearningRateScheduler setting learning rate to 0.0006840622763423391.\n",
      "learning rate: 6.84e-04, weight decay: 3.42e-05\n",
      "Epoch 39/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 2.1470 - accuracy: 0.7616\n",
      "Epoch 39: val_accuracy did not improve from 0.67963\n",
      "590/590 [==============================] - 30s 51ms/step - loss: 2.1470 - accuracy: 0.7616 - val_loss: 2.5440 - val_accuracy: 0.6607 - lr: 6.8406e-04\n",
      "\n",
      "Epoch 40: LearningRateScheduler setting learning rate to 0.0006693689601226458.\n",
      "learning rate: 6.69e-04, weight decay: 3.35e-05\n",
      "Epoch 40/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.1382 - accuracy: 0.7654\n",
      "Epoch 40: val_accuracy did not improve from 0.67963\n",
      "590/590 [==============================] - 30s 51ms/step - loss: 2.1378 - accuracy: 0.7655 - val_loss: 2.4860 - val_accuracy: 0.6791 - lr: 6.6937e-04\n",
      "\n",
      "Epoch 41: LearningRateScheduler setting learning rate to 0.0006545084971874737.\n",
      "learning rate: 6.55e-04, weight decay: 3.27e-05\n",
      "Epoch 41/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.1268 - accuracy: 0.7709\n",
      "Epoch 41: val_accuracy did not improve from 0.67963\n",
      "590/590 [==============================] - 30s 51ms/step - loss: 2.1269 - accuracy: 0.7709 - val_loss: 2.5858 - val_accuracy: 0.6618 - lr: 6.5451e-04\n",
      "\n",
      "Epoch 42: LearningRateScheduler setting learning rate to 0.0006394955530196147.\n",
      "learning rate: 6.39e-04, weight decay: 3.20e-05\n",
      "Epoch 42/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.1261 - accuracy: 0.7735\n",
      "Epoch 42: val_accuracy did not improve from 0.67963\n",
      "590/590 [==============================] - 30s 51ms/step - loss: 2.1258 - accuracy: 0.7735 - val_loss: 2.5871 - val_accuracy: 0.6600 - lr: 6.3950e-04\n",
      "\n",
      "Epoch 43: LearningRateScheduler setting learning rate to 0.0006243449435824276.\n",
      "learning rate: 6.24e-04, weight decay: 3.12e-05\n",
      "Epoch 43/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.1181 - accuracy: 0.7787\n",
      "Epoch 43: val_accuracy improved from 0.67963 to 0.68145, saving model to models/weights_stgcn6.h5\n",
      "590/590 [==============================] - 30s 51ms/step - loss: 2.1185 - accuracy: 0.7786 - val_loss: 2.5187 - val_accuracy: 0.6815 - lr: 6.2434e-04\n",
      "\n",
      "Epoch 44: LearningRateScheduler setting learning rate to 0.0006090716206982714.\n",
      "learning rate: 6.09e-04, weight decay: 3.05e-05\n",
      "Epoch 44/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.1074 - accuracy: 0.7831\n",
      "Epoch 44: val_accuracy did not improve from 0.68145\n",
      "590/590 [==============================] - 30s 51ms/step - loss: 2.1072 - accuracy: 0.7831 - val_loss: 2.5642 - val_accuracy: 0.6682 - lr: 6.0907e-04\n",
      "\n",
      "Epoch 45: LearningRateScheduler setting learning rate to 0.0005936906572928624.\n",
      "learning rate: 5.94e-04, weight decay: 2.97e-05\n",
      "Epoch 45/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.0985 - accuracy: 0.7846\n",
      "Epoch 45: val_accuracy improved from 0.68145 to 0.69628, saving model to models/weights_stgcn6.h5\n",
      "590/590 [==============================] - 30s 51ms/step - loss: 2.0987 - accuracy: 0.7845 - val_loss: 2.4741 - val_accuracy: 0.6963 - lr: 5.9369e-04\n",
      "\n",
      "Epoch 46: LearningRateScheduler setting learning rate to 0.0005782172325201155.\n",
      "learning rate: 5.78e-04, weight decay: 2.89e-05\n",
      "Epoch 46/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 2.0962 - accuracy: 0.7881\n",
      "Epoch 46: val_accuracy did not improve from 0.69628\n",
      "590/590 [==============================] - 30s 51ms/step - loss: 2.0962 - accuracy: 0.7881 - val_loss: 2.5592 - val_accuracy: 0.6778 - lr: 5.7822e-04\n",
      "\n",
      "Epoch 47: LearningRateScheduler setting learning rate to 0.0005626666167821521.\n",
      "learning rate: 5.63e-04, weight decay: 2.81e-05\n",
      "Epoch 47/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 2.0895 - accuracy: 0.7927\n",
      "Epoch 47: val_accuracy did not improve from 0.69628\n",
      "590/590 [==============================] - 30s 51ms/step - loss: 2.0895 - accuracy: 0.7927 - val_loss: 2.5907 - val_accuracy: 0.6676 - lr: 5.6267e-04\n",
      "\n",
      "Epoch 48: LearningRateScheduler setting learning rate to 0.0005470541566592572.\n",
      "learning rate: 5.47e-04, weight decay: 2.74e-05\n",
      "Epoch 48/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.0779 - accuracy: 0.7977\n",
      "Epoch 48: val_accuracy did not improve from 0.69628\n",
      "590/590 [==============================] - 30s 51ms/step - loss: 2.0778 - accuracy: 0.7977 - val_loss: 2.5432 - val_accuracy: 0.6847 - lr: 5.4705e-04\n",
      "\n",
      "Epoch 49: LearningRateScheduler setting learning rate to 0.0005313952597646568.\n",
      "learning rate: 5.31e-04, weight decay: 2.66e-05\n",
      "Epoch 49/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.0699 - accuracy: 0.8018\n",
      "Epoch 49: val_accuracy did not improve from 0.69628\n",
      "590/590 [==============================] - 30s 51ms/step - loss: 2.0696 - accuracy: 0.8018 - val_loss: 2.5054 - val_accuracy: 0.6962 - lr: 5.3140e-04\n",
      "\n",
      "Epoch 50: LearningRateScheduler setting learning rate to 0.0005157053795390641.\n",
      "learning rate: 5.16e-04, weight decay: 2.58e-05\n",
      "Epoch 50/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.0637 - accuracy: 0.8049\n",
      "Epoch 50: val_accuracy did not improve from 0.69628\n",
      "590/590 [==============================] - 30s 51ms/step - loss: 2.0639 - accuracy: 0.8048 - val_loss: 2.5053 - val_accuracy: 0.6946 - lr: 5.1571e-04\n",
      "\n",
      "Epoch 51: LearningRateScheduler setting learning rate to 0.0005.\n",
      "learning rate: 5.00e-04, weight decay: 2.50e-05\n",
      "Epoch 51/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.0523 - accuracy: 0.8125\n",
      "Epoch 51: val_accuracy did not improve from 0.69628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "590/590 [==============================] - 30s 51ms/step - loss: 2.0525 - accuracy: 0.8125 - val_loss: 2.6432 - val_accuracy: 0.6703 - lr: 5.0000e-04\n",
      "\n",
      "Epoch 52: LearningRateScheduler setting learning rate to 0.00048429462046093585.\n",
      "learning rate: 4.84e-04, weight decay: 2.42e-05\n",
      "Epoch 52/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.0454 - accuracy: 0.8152\n",
      "Epoch 52: val_accuracy improved from 0.69628 to 0.71110, saving model to models/weights_stgcn6.h5\n",
      "590/590 [==============================] - 30s 51ms/step - loss: 2.0456 - accuracy: 0.8151 - val_loss: 2.4728 - val_accuracy: 0.7111 - lr: 4.8429e-04\n",
      "\n",
      "Epoch 53: LearningRateScheduler setting learning rate to 0.0004686047402353433.\n",
      "learning rate: 4.69e-04, weight decay: 2.34e-05\n",
      "Epoch 53/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.0370 - accuracy: 0.8187\n",
      "Epoch 53: val_accuracy improved from 0.71110 to 0.71597, saving model to models/weights_stgcn6.h5\n",
      "590/590 [==============================] - 30s 51ms/step - loss: 2.0373 - accuracy: 0.8187 - val_loss: 2.4553 - val_accuracy: 0.7160 - lr: 4.6860e-04\n",
      "\n",
      "Epoch 54: LearningRateScheduler setting learning rate to 0.00045294584334074284.\n",
      "learning rate: 4.53e-04, weight decay: 2.26e-05\n",
      "Epoch 54/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.0289 - accuracy: 0.8230\n",
      "Epoch 54: val_accuracy did not improve from 0.71597\n",
      "590/590 [==============================] - 30s 51ms/step - loss: 2.0291 - accuracy: 0.8230 - val_loss: 2.4772 - val_accuracy: 0.7136 - lr: 4.5295e-04\n",
      "\n",
      "Epoch 55: LearningRateScheduler setting learning rate to 0.00043733338321784784.\n",
      "learning rate: 4.37e-04, weight decay: 2.19e-05\n",
      "Epoch 55/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.0217 - accuracy: 0.8290\n",
      "Epoch 55: val_accuracy did not improve from 0.71597\n",
      "590/590 [==============================] - 30s 51ms/step - loss: 2.0217 - accuracy: 0.8290 - val_loss: 2.4853 - val_accuracy: 0.7152 - lr: 4.3733e-04\n",
      "\n",
      "Epoch 56: LearningRateScheduler setting learning rate to 0.0004217827674798845.\n",
      "learning rate: 4.22e-04, weight decay: 2.11e-05\n",
      "Epoch 56/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.0143 - accuracy: 0.8317\n",
      "Epoch 56: val_accuracy improved from 0.71597 to 0.72432, saving model to models/weights_stgcn6.h5\n",
      "590/590 [==============================] - 30s 51ms/step - loss: 2.0142 - accuracy: 0.8317 - val_loss: 2.4509 - val_accuracy: 0.7243 - lr: 4.2178e-04\n",
      "\n",
      "Epoch 57: LearningRateScheduler setting learning rate to 0.0004063093427071376.\n",
      "learning rate: 4.06e-04, weight decay: 2.03e-05\n",
      "Epoch 57/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.0024 - accuracy: 0.8374\n",
      "Epoch 57: val_accuracy did not improve from 0.72432\n",
      "590/590 [==============================] - 30s 51ms/step - loss: 2.0022 - accuracy: 0.8375 - val_loss: 2.5691 - val_accuracy: 0.7010 - lr: 4.0631e-04\n",
      "\n",
      "Epoch 58: LearningRateScheduler setting learning rate to 0.0003909283793017289.\n",
      "learning rate: 3.91e-04, weight decay: 1.95e-05\n",
      "Epoch 58/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 1.9918 - accuracy: 0.8429\n",
      "Epoch 58: val_accuracy did not improve from 0.72432\n",
      "590/590 [==============================] - 30s 51ms/step - loss: 1.9917 - accuracy: 0.8429 - val_loss: 2.4989 - val_accuracy: 0.7193 - lr: 3.9093e-04\n",
      "\n",
      "Epoch 59: LearningRateScheduler setting learning rate to 0.0003756550564175727.\n",
      "learning rate: 3.76e-04, weight decay: 1.88e-05\n",
      "Epoch 59/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 1.9823 - accuracy: 0.8456\n",
      "Epoch 59: val_accuracy did not improve from 0.72432\n",
      "590/590 [==============================] - 30s 51ms/step - loss: 1.9823 - accuracy: 0.8456 - val_loss: 2.5113 - val_accuracy: 0.7199 - lr: 3.7566e-04\n",
      "\n",
      "Epoch 60: LearningRateScheduler setting learning rate to 0.0003605044469803854.\n",
      "learning rate: 3.61e-04, weight decay: 1.80e-05\n",
      "Epoch 60/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 1.9725 - accuracy: 0.8505\n",
      "Epoch 60: val_accuracy improved from 0.72432 to 0.72790, saving model to models/weights_stgcn6.h5\n",
      "590/590 [==============================] - 30s 51ms/step - loss: 1.9725 - accuracy: 0.8505 - val_loss: 2.4766 - val_accuracy: 0.7279 - lr: 3.6050e-04\n",
      "\n",
      "Epoch 61: LearningRateScheduler setting learning rate to 0.00034549150281252633.\n",
      "learning rate: 3.45e-04, weight decay: 1.73e-05\n",
      "Epoch 61/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 1.9615 - accuracy: 0.8561\n",
      "Epoch 61: val_accuracy did not improve from 0.72790\n",
      "590/590 [==============================] - 30s 51ms/step - loss: 1.9615 - accuracy: 0.8561 - val_loss: 2.4944 - val_accuracy: 0.7260 - lr: 3.4549e-04\n",
      "\n",
      "Epoch 62: LearningRateScheduler setting learning rate to 0.0003306310398773543.\n",
      "learning rate: 3.31e-04, weight decay: 1.65e-05\n",
      "Epoch 62/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 1.9530 - accuracy: 0.8612\n",
      "Epoch 62: val_accuracy improved from 0.72790 to 0.73052, saving model to models/weights_stgcn6.h5\n",
      "590/590 [==============================] - 30s 51ms/step - loss: 1.9531 - accuracy: 0.8612 - val_loss: 2.4868 - val_accuracy: 0.7305 - lr: 3.3063e-04\n",
      "\n",
      "Epoch 63: LearningRateScheduler setting learning rate to 0.00031593772365766105.\n",
      "learning rate: 3.16e-04, weight decay: 1.58e-05\n",
      "Epoch 63/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 1.9434 - accuracy: 0.8656\n",
      "Epoch 63: val_accuracy improved from 0.73052 to 0.73839, saving model to models/weights_stgcn6.h5\n",
      "590/590 [==============================] - 30s 51ms/step - loss: 1.9435 - accuracy: 0.8657 - val_loss: 2.4550 - val_accuracy: 0.7384 - lr: 3.1594e-04\n",
      "\n",
      "Epoch 64: LearningRateScheduler setting learning rate to 0.00030142605468260977.\n",
      "learning rate: 3.01e-04, weight decay: 1.51e-05\n",
      "Epoch 64/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 1.9316 - accuracy: 0.8714\n",
      "Epoch 64: val_accuracy did not improve from 0.73839\n",
      "590/590 [==============================] - 30s 51ms/step - loss: 1.9318 - accuracy: 0.8714 - val_loss: 2.4613 - val_accuracy: 0.7372 - lr: 3.0143e-04\n",
      "\n",
      "Epoch 65: LearningRateScheduler setting learning rate to 0.00028711035421746366.\n",
      "learning rate: 2.87e-04, weight decay: 1.44e-05\n",
      "Epoch 65/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 1.9200 - accuracy: 0.8771\n",
      "Epoch 65: val_accuracy did not improve from 0.73839\n",
      "590/590 [==============================] - 30s 51ms/step - loss: 1.9202 - accuracy: 0.8771 - val_loss: 2.4900 - val_accuracy: 0.7349 - lr: 2.8711e-04\n",
      "\n",
      "Epoch 66: LearningRateScheduler setting learning rate to 0.00027300475013022663.\n",
      "learning rate: 2.73e-04, weight decay: 1.37e-05\n",
      "Epoch 66/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 1.9117 - accuracy: 0.8818\n",
      "Epoch 66: val_accuracy did not improve from 0.73839\n",
      "590/590 [==============================] - 30s 51ms/step - loss: 1.9117 - accuracy: 0.8818 - val_loss: 2.4930 - val_accuracy: 0.7324 - lr: 2.7300e-04\n",
      "\n",
      "Epoch 67: LearningRateScheduler setting learning rate to 0.0002591231629491423.\n",
      "learning rate: 2.59e-04, weight decay: 1.30e-05\n",
      "Epoch 67/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 1.9001 - accuracy: 0.8862\n",
      "Epoch 67: val_accuracy improved from 0.73839 to 0.75594, saving model to models/weights_stgcn6.h5\n",
      "590/590 [==============================] - 30s 51ms/step - loss: 1.9001 - accuracy: 0.8862 - val_loss: 2.4202 - val_accuracy: 0.7559 - lr: 2.5912e-04\n",
      "\n",
      "Epoch 68: LearningRateScheduler setting learning rate to 0.00024547929212481435.\n",
      "learning rate: 2.45e-04, weight decay: 1.23e-05\n",
      "Epoch 68/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 1.8918 - accuracy: 0.8915\n",
      "Epoch 68: val_accuracy did not improve from 0.75594\n",
      "590/590 [==============================] - 30s 51ms/step - loss: 1.8919 - accuracy: 0.8915 - val_loss: 2.5141 - val_accuracy: 0.7314 - lr: 2.4548e-04\n",
      "\n",
      "Epoch 69: LearningRateScheduler setting learning rate to 0.00023208660251050156.\n",
      "learning rate: 2.32e-04, weight decay: 1.16e-05\n",
      "Epoch 69/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 1.8804 - accuracy: 0.8964\n",
      "Epoch 69: val_accuracy did not improve from 0.75594\n",
      "590/590 [==============================] - 30s 51ms/step - loss: 1.8804 - accuracy: 0.8964 - val_loss: 2.4457 - val_accuracy: 0.7484 - lr: 2.3209e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 70: LearningRateScheduler setting learning rate to 0.0002189583110739348.\n",
      "learning rate: 2.19e-04, weight decay: 1.09e-05\n",
      "Epoch 70/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 1.8701 - accuracy: 0.9009\n",
      "Epoch 70: val_accuracy improved from 0.75594 to 0.76156, saving model to models/weights_stgcn6.h5\n",
      "590/590 [==============================] - 30s 51ms/step - loss: 1.8700 - accuracy: 0.9009 - val_loss: 2.4146 - val_accuracy: 0.7616 - lr: 2.1896e-04\n",
      "\n",
      "Epoch 71: LearningRateScheduler setting learning rate to 0.00020610737385376348.\n",
      "learning rate: 2.06e-04, weight decay: 1.03e-05\n",
      "Epoch 71/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 1.8579 - accuracy: 0.9063\n",
      "Epoch 71: val_accuracy did not improve from 0.76156\n",
      "590/590 [==============================] - 30s 51ms/step - loss: 1.8581 - accuracy: 0.9062 - val_loss: 2.4543 - val_accuracy: 0.7528 - lr: 2.0611e-04\n",
      "\n",
      "Epoch 72: LearningRateScheduler setting learning rate to 0.00019354647317351188.\n",
      "learning rate: 1.94e-04, weight decay: 9.68e-06\n",
      "Epoch 72/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 1.8510 - accuracy: 0.9111\n",
      "Epoch 72: val_accuracy improved from 0.76156 to 0.76215, saving model to models/weights_stgcn6.h5\n",
      "590/590 [==============================] - 30s 51ms/step - loss: 1.8512 - accuracy: 0.9111 - val_loss: 2.4279 - val_accuracy: 0.7621 - lr: 1.9355e-04\n",
      "\n",
      "Epoch 73: LearningRateScheduler setting learning rate to 0.00018128800512565513.\n",
      "learning rate: 1.81e-04, weight decay: 9.06e-06\n",
      "Epoch 73/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 1.8418 - accuracy: 0.9162\n",
      "Epoch 73: val_accuracy did not improve from 0.76215\n",
      "590/590 [==============================] - 30s 51ms/step - loss: 1.8421 - accuracy: 0.9161 - val_loss: 2.4529 - val_accuracy: 0.7577 - lr: 1.8129e-04\n",
      "\n",
      "Epoch 74: LearningRateScheduler setting learning rate to 0.00016934406733817414.\n",
      "learning rate: 1.69e-04, weight decay: 8.47e-06\n",
      "Epoch 74/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 1.8291 - accuracy: 0.9209\n",
      "Epoch 74: val_accuracy did not improve from 0.76215\n",
      "590/590 [==============================] - 30s 51ms/step - loss: 1.8291 - accuracy: 0.9209 - val_loss: 2.4723 - val_accuracy: 0.7521 - lr: 1.6934e-04\n",
      "\n",
      "Epoch 75: LearningRateScheduler setting learning rate to 0.00015772644703565563.\n",
      "learning rate: 1.58e-04, weight decay: 7.89e-06\n",
      "Epoch 75/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 1.8202 - accuracy: 0.9256\n",
      "Epoch 75: val_accuracy did not improve from 0.76215\n",
      "590/590 [==============================] - 30s 51ms/step - loss: 1.8202 - accuracy: 0.9256 - val_loss: 2.4463 - val_accuracy: 0.7602 - lr: 1.5773e-04\n",
      "\n",
      "Epoch 76: LearningRateScheduler setting learning rate to 0.00014644660940672628.\n",
      "learning rate: 1.46e-04, weight decay: 7.32e-06\n",
      "Epoch 76/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 1.8124 - accuracy: 0.9295\n",
      "Epoch 76: val_accuracy improved from 0.76215 to 0.76332, saving model to models/weights_stgcn6.h5\n",
      "590/590 [==============================] - 30s 51ms/step - loss: 1.8122 - accuracy: 0.9295 - val_loss: 2.4414 - val_accuracy: 0.7633 - lr: 1.4645e-04\n",
      "\n",
      "Epoch 77: LearningRateScheduler setting learning rate to 0.00013551568628929433.\n",
      "learning rate: 1.36e-04, weight decay: 6.78e-06\n",
      "Epoch 77/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 1.8033 - accuracy: 0.9341\n",
      "Epoch 77: val_accuracy improved from 0.76332 to 0.76348, saving model to models/weights_stgcn6.h5\n",
      "590/590 [==============================] - 30s 51ms/step - loss: 1.8035 - accuracy: 0.9341 - val_loss: 2.4468 - val_accuracy: 0.7635 - lr: 1.3552e-04\n",
      "\n",
      "Epoch 78: LearningRateScheduler setting learning rate to 0.0001249444651847702.\n",
      "learning rate: 1.25e-04, weight decay: 6.25e-06\n",
      "Epoch 78/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 1.7946 - accuracy: 0.9379\n",
      "Epoch 78: val_accuracy did not improve from 0.76348\n",
      "590/590 [==============================] - 30s 51ms/step - loss: 1.7946 - accuracy: 0.9378 - val_loss: 2.4607 - val_accuracy: 0.7629 - lr: 1.2494e-04\n",
      "\n",
      "Epoch 79: LearningRateScheduler setting learning rate to 0.00011474337861210544.\n",
      "learning rate: 1.15e-04, weight decay: 5.74e-06\n",
      "Epoch 79/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 1.7863 - accuracy: 0.9421\n",
      "Epoch 79: val_accuracy improved from 0.76348 to 0.76889, saving model to models/weights_stgcn6.h5\n",
      "590/590 [==============================] - 30s 51ms/step - loss: 1.7863 - accuracy: 0.9421 - val_loss: 2.4385 - val_accuracy: 0.7689 - lr: 1.1474e-04\n",
      "\n",
      "Epoch 80: LearningRateScheduler setting learning rate to 0.00010492249381215479.\n",
      "learning rate: 1.05e-04, weight decay: 5.25e-06\n",
      "Epoch 80/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 1.7800 - accuracy: 0.9455\n",
      "Epoch 80: val_accuracy did not improve from 0.76889\n",
      "590/590 [==============================] - 30s 51ms/step - loss: 1.7803 - accuracy: 0.9454 - val_loss: 2.4606 - val_accuracy: 0.7642 - lr: 1.0492e-04\n",
      "\n",
      "Epoch 81: LearningRateScheduler setting learning rate to 9.549150281252633e-05.\n",
      "learning rate: 9.55e-05, weight decay: 4.77e-06\n",
      "Epoch 81/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 1.7726 - accuracy: 0.9494\n",
      "Epoch 81: val_accuracy improved from 0.76889 to 0.77060, saving model to models/weights_stgcn6.h5\n",
      "590/590 [==============================] - 30s 51ms/step - loss: 1.7725 - accuracy: 0.9494 - val_loss: 2.4387 - val_accuracy: 0.7706 - lr: 9.5492e-05\n",
      "\n",
      "Epoch 82: LearningRateScheduler setting learning rate to 8.645971286271903e-05.\n",
      "learning rate: 8.65e-05, weight decay: 4.32e-06\n",
      "Epoch 82/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 1.7658 - accuracy: 0.9522\n",
      "Epoch 82: val_accuracy improved from 0.77060 to 0.77194, saving model to models/weights_stgcn6.h5\n",
      "590/590 [==============================] - 30s 51ms/step - loss: 1.7659 - accuracy: 0.9522 - val_loss: 2.4354 - val_accuracy: 0.7719 - lr: 8.6460e-05\n",
      "\n",
      "Epoch 83: LearningRateScheduler setting learning rate to 7.783603724899258e-05.\n",
      "learning rate: 7.78e-05, weight decay: 3.89e-06\n",
      "Epoch 83/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 1.7588 - accuracy: 0.9551\n",
      "Epoch 83: val_accuracy improved from 0.77194 to 0.77258, saving model to models/weights_stgcn6.h5\n",
      "590/590 [==============================] - 30s 51ms/step - loss: 1.7588 - accuracy: 0.9551 - val_loss: 2.4451 - val_accuracy: 0.7726 - lr: 7.7836e-05\n",
      "\n",
      "Epoch 84: LearningRateScheduler setting learning rate to 6.962898649802824e-05.\n",
      "learning rate: 6.96e-05, weight decay: 3.48e-06\n",
      "Epoch 84/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 1.7519 - accuracy: 0.9584\n",
      "Epoch 84: val_accuracy improved from 0.77258 to 0.77354, saving model to models/weights_stgcn6.h5\n",
      "590/590 [==============================] - 30s 51ms/step - loss: 1.7518 - accuracy: 0.9585 - val_loss: 2.4414 - val_accuracy: 0.7735 - lr: 6.9629e-05\n",
      "\n",
      "Epoch 85: LearningRateScheduler setting learning rate to 6.184665997806832e-05.\n",
      "learning rate: 6.18e-05, weight decay: 3.09e-06\n",
      "Epoch 85/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 1.7461 - accuracy: 0.9610\n",
      "Epoch 85: val_accuracy improved from 0.77354 to 0.77585, saving model to models/weights_stgcn6.h5\n",
      "590/590 [==============================] - 30s 51ms/step - loss: 1.7463 - accuracy: 0.9610 - val_loss: 2.4381 - val_accuracy: 0.7758 - lr: 6.1847e-05\n",
      "\n",
      "Epoch 86: LearningRateScheduler setting learning rate to 5.449673790581611e-05.\n",
      "learning rate: 5.45e-05, weight decay: 2.72e-06\n",
      "Epoch 86/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 1.7402 - accuracy: 0.9640\n",
      "Epoch 86: val_accuracy improved from 0.77585 to 0.77654, saving model to models/weights_stgcn6.h5\n",
      "590/590 [==============================] - 30s 51ms/step - loss: 1.7403 - accuracy: 0.9640 - val_loss: 2.4361 - val_accuracy: 0.7765 - lr: 5.4497e-05\n",
      "\n",
      "Epoch 87: LearningRateScheduler setting learning rate to 4.758647376699032e-05.\n",
      "learning rate: 4.76e-05, weight decay: 2.38e-06\n",
      "Epoch 87/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 1.7347 - accuracy: 0.9662\n",
      "Epoch 87: val_accuracy did not improve from 0.77654\n",
      "590/590 [==============================] - 30s 51ms/step - loss: 1.7347 - accuracy: 0.9662 - val_loss: 2.4431 - val_accuracy: 0.7754 - lr: 4.7586e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 88: LearningRateScheduler setting learning rate to 4.112268715800943e-05.\n",
      "learning rate: 4.11e-05, weight decay: 2.06e-06\n",
      "Epoch 88/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 1.7296 - accuracy: 0.9686\n",
      "Epoch 88: val_accuracy improved from 0.77654 to 0.77724, saving model to models/weights_stgcn6.h5\n",
      "590/590 [==============================] - 30s 51ms/step - loss: 1.7296 - accuracy: 0.9686 - val_loss: 2.4409 - val_accuracy: 0.7772 - lr: 4.1123e-05\n",
      "\n",
      "Epoch 89: LearningRateScheduler setting learning rate to 3.5111757055874326e-05.\n",
      "learning rate: 3.51e-05, weight decay: 1.76e-06\n",
      "Epoch 89/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 1.7258 - accuracy: 0.9691\n",
      "Epoch 89: val_accuracy improved from 0.77724 to 0.77740, saving model to models/weights_stgcn6.h5\n",
      "590/590 [==============================] - 30s 51ms/step - loss: 1.7257 - accuracy: 0.9691 - val_loss: 2.4374 - val_accuracy: 0.7774 - lr: 3.5112e-05\n",
      "\n",
      "Epoch 90: LearningRateScheduler setting learning rate to 2.9559615522887274e-05.\n",
      "learning rate: 2.96e-05, weight decay: 1.48e-06\n",
      "Epoch 90/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 1.7217 - accuracy: 0.9718\n",
      "Epoch 90: val_accuracy improved from 0.77740 to 0.77959, saving model to models/weights_stgcn6.h5\n",
      "590/590 [==============================] - 30s 51ms/step - loss: 1.7217 - accuracy: 0.9718 - val_loss: 2.4355 - val_accuracy: 0.7796 - lr: 2.9560e-05\n",
      "\n",
      "Epoch 91: LearningRateScheduler setting learning rate to 2.4471741852423235e-05.\n",
      "learning rate: 2.45e-05, weight decay: 1.22e-06\n",
      "Epoch 91/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 1.7166 - accuracy: 0.9736\n",
      "Epoch 91: val_accuracy did not improve from 0.77959\n",
      "590/590 [==============================] - 30s 51ms/step - loss: 1.7167 - accuracy: 0.9736 - val_loss: 2.4363 - val_accuracy: 0.7789 - lr: 2.4472e-05\n",
      "\n",
      "Epoch 92: LearningRateScheduler setting learning rate to 1.985315716152847e-05.\n",
      "learning rate: 1.99e-05, weight decay: 9.93e-07\n",
      "Epoch 92/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 1.7137 - accuracy: 0.9756\n",
      "Epoch 92: val_accuracy did not improve from 0.77959\n",
      "590/590 [==============================] - 30s 51ms/step - loss: 1.7137 - accuracy: 0.9756 - val_loss: 2.4355 - val_accuracy: 0.7796 - lr: 1.9853e-05\n",
      "\n",
      "Epoch 93: LearningRateScheduler setting learning rate to 1.5708419435684463e-05.\n",
      "learning rate: 1.57e-05, weight decay: 7.85e-07\n",
      "Epoch 93/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 1.7107 - accuracy: 0.9765\n",
      "Epoch 93: val_accuracy improved from 0.77959 to 0.78013, saving model to models/weights_stgcn6.h5\n",
      "590/590 [==============================] - 30s 51ms/step - loss: 1.7106 - accuracy: 0.9765 - val_loss: 2.4349 - val_accuracy: 0.7801 - lr: 1.5708e-05\n",
      "\n",
      "Epoch 94: LearningRateScheduler setting learning rate to 1.2041619030626282e-05.\n",
      "learning rate: 1.20e-05, weight decay: 6.02e-07\n",
      "Epoch 94/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 1.7071 - accuracy: 0.9779\n",
      "Epoch 94: val_accuracy improved from 0.78013 to 0.78130, saving model to models/weights_stgcn6.h5\n",
      "590/590 [==============================] - 30s 51ms/step - loss: 1.7072 - accuracy: 0.9778 - val_loss: 2.4347 - val_accuracy: 0.7813 - lr: 1.2042e-05\n",
      "\n",
      "Epoch 95: LearningRateScheduler setting learning rate to 8.856374635655695e-06.\n",
      "learning rate: 8.86e-06, weight decay: 4.43e-07\n",
      "Epoch 95/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 1.7049 - accuracy: 0.9784\n",
      "Epoch 95: val_accuracy did not improve from 0.78130\n",
      "590/590 [==============================] - 30s 51ms/step - loss: 1.7049 - accuracy: 0.9784 - val_loss: 2.4358 - val_accuracy: 0.7786 - lr: 8.8564e-06\n",
      "\n",
      "Epoch 96: LearningRateScheduler setting learning rate to 6.15582970243117e-06.\n",
      "learning rate: 6.16e-06, weight decay: 3.08e-07\n",
      "Epoch 96/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 1.7029 - accuracy: 0.9791\n",
      "Epoch 96: val_accuracy did not improve from 0.78130\n",
      "590/590 [==============================] - 30s 51ms/step - loss: 1.7028 - accuracy: 0.9791 - val_loss: 2.4353 - val_accuracy: 0.7810 - lr: 6.1558e-06\n",
      "\n",
      "Epoch 97: LearningRateScheduler setting learning rate to 3.942649342761117e-06.\n",
      "learning rate: 3.94e-06, weight decay: 1.97e-07\n",
      "Epoch 97/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 1.7005 - accuracy: 0.9800\n",
      "Epoch 97: val_accuracy did not improve from 0.78130\n",
      "590/590 [==============================] - 30s 51ms/step - loss: 1.7006 - accuracy: 0.9800 - val_loss: 2.4350 - val_accuracy: 0.7807 - lr: 3.9426e-06\n",
      "\n",
      "Epoch 98: LearningRateScheduler setting learning rate to 2.219017698460002e-06.\n",
      "learning rate: 2.22e-06, weight decay: 1.11e-07\n",
      "Epoch 98/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 1.6986 - accuracy: 0.9803\n",
      "Epoch 98: val_accuracy improved from 0.78130 to 0.78136, saving model to models/weights_stgcn6.h5\n",
      "590/590 [==============================] - 30s 51ms/step - loss: 1.6985 - accuracy: 0.9803 - val_loss: 2.4341 - val_accuracy: 0.7814 - lr: 2.2190e-06\n",
      "\n",
      "Epoch 99: LearningRateScheduler setting learning rate to 9.866357858642206e-07.\n",
      "learning rate: 9.87e-07, weight decay: 4.93e-08\n",
      "Epoch 99/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 1.6983 - accuracy: 0.9806\n",
      "Epoch 99: val_accuracy did not improve from 0.78136\n",
      "590/590 [==============================] - 30s 51ms/step - loss: 1.6984 - accuracy: 0.9806 - val_loss: 2.4345 - val_accuracy: 0.7809 - lr: 9.8664e-07\n",
      "\n",
      "Epoch 100: LearningRateScheduler setting learning rate to 2.467198171342e-07.\n",
      "learning rate: 2.47e-07, weight decay: 1.23e-08\n",
      "Epoch 100/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 1.6965 - accuracy: 0.9808\n",
      "Epoch 100: val_accuracy did not improve from 0.78136\n",
      "590/590 [==============================] - 30s 51ms/step - loss: 1.6965 - accuracy: 0.9808 - val_loss: 2.4344 - val_accuracy: 0.7806 - lr: 2.4672e-07\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Get new fresh model\n",
    "file_name = 'models/weights_stgcn6.h5'\n",
    "#model = tf.keras.models.load_model('models/041423_21_02.h5')\n",
    "model.summary()\n",
    "\n",
    "# Actual Training\n",
    "history = model.fit(\n",
    "        x=x_train,\n",
    "        y=y_train,\n",
    "        epochs=100,\n",
    "        # Only used for validation data since training data is a generator\n",
    "        batch_size=128,\n",
    "        validation_data=(x_test,y_test),\n",
    "        callbacks=[\n",
    "            tf.keras.callbacks.ModelCheckpoint(\n",
    "            file_name,\n",
    "            save_weights_only = True,\n",
    "            save_best_only=True, \n",
    "            monitor=\"val_accuracy\",\n",
    "            mode=\"max\",\n",
    "            verbose = 1),\n",
    "            lr_callback,\n",
    "            WeightDecayCallback(),\n",
    "        ],\n",
    "        verbose = 1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75520, 2, 20, 61)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('models/weights_stgcn5.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 250)\n",
      "584/584 [==============================] - 3s 5ms/step\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_file(file_path):\n",
    "    \"\"\"Read a JSON file and parse it into a Python object.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the JSON file to read.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary object representing the JSON data.\n",
    "        \n",
    "    Raises:\n",
    "        FileNotFoundError: If the specified file path does not exist.\n",
    "        ValueError: If the specified file path does not contain valid JSON data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Open the file and load the JSON data into a Python object\n",
    "        with open(file_path, 'r') as file:\n",
    "            json_data = json.load(file)\n",
    "        return json_data\n",
    "    except FileNotFoundError:\n",
    "        # Raise an error if the file path does not exist\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    except ValueError:\n",
    "        # Raise an error if the file does not contain valid JSON data\n",
    "        raise ValueError(f\"Invalid JSON data in file: {file_path}\")\n",
    "p2s_map = {v:k for k,v in read_json_file(os.path.join('', \"sign_to_prediction_index_map.json\")).items()}\n",
    "encoder = lambda x: s2p_map.get(x.lower())\n",
    "decoder = lambda x: p2s_map.get(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7854773116438356\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "for i , j in zip(preds,y_test):\n",
    "    i = np.argmax(i, axis=-1)\n",
    "    if i == j:\n",
    "        cnt+=1\n",
    "print(cnt/len(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_df = pd.read_csv('/kaggle/input/asl-signs/train.csv')\n",
    "complete_df = complete_df[complete_df['participant_id']!='37055']\n",
    "from sklearn.model_selection import train_test_split\n",
    "y = complete_df['sign']\n",
    "train_df, test_df = train_test_split(complete_df, test_size=0.2,stratify=y)\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "for index,row in test_df.iterrows():\n",
    "    demo_output = tflite_keras_model(load_relevant_data_subset('/kaggle/input/asl-signs/'+row['path']))[\"outputs\"]\n",
    "    all_preds.append(decoder(np.argmax(demo_output.numpy(), axis=-1)))\n",
    "    all_labels.append(row['sign'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-11T13:19:17.068946Z",
     "iopub.status.busy": "2023-04-11T13:19:17.067991Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_ds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [46], line 12\u001b[0m\n\u001b[1;32m      7\u001b[0m loss_fn\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mSparseCategoricalCrossentropy()\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m30\u001b[39m): \u001b[38;5;66;03m# <----- start for loop, step 1\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \n\u001b[1;32m     10\u001b[0m   \u001b[38;5;66;03m# <-------- start for loop, step 2\u001b[39;00m\n\u001b[1;32m     11\u001b[0m   \u001b[38;5;66;03m# Iterate over the batches of the dataset.\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m step, (x_batch_train, y_batch_train) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mtrain_ds\u001b[49m):\n\u001b[1;32m     13\u001b[0m \n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# <-------- start gradient tape scope, step 3\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# Open a GradientTape to record the operations run\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# during the forward pass, which enables auto-differentiation.\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[1;32m     18\u001b[0m \n\u001b[1;32m     19\u001b[0m        \u001b[38;5;66;03m# Run the forward pass of the layer.\u001b[39;00m\n\u001b[1;32m     20\u001b[0m        \u001b[38;5;66;03m# The operations that the layer applies\u001b[39;00m\n\u001b[1;32m     21\u001b[0m        \u001b[38;5;66;03m# to its inputs are going to be recorded\u001b[39;00m\n\u001b[1;32m     22\u001b[0m        \u001b[38;5;66;03m# on the GradientTape.\u001b[39;00m\n\u001b[1;32m     23\u001b[0m        logits \u001b[38;5;241m=\u001b[39m model(x_batch_train, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_ds' is not defined"
     ]
    }
   ],
   "source": [
    "# https://towardsdatascience.com/model-sub-classing-and-custom-training-loop-from-scratch-in-tensorflow-2-cc1d4f10fb4e\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "train_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
    "val_acc_metric   = tf.keras.metrics.CategoricalAccuracy()\n",
    "# Instantiate a loss function\n",
    "# loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "loss_fn=tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "for epoch in range(30): # <----- start for loop, step 1\n",
    "\n",
    "  # <-------- start for loop, step 2\n",
    "  # Iterate over the batches of the dataset.\n",
    "  for step, (x_batch_train, y_batch_train) in enumerate(train_ds):\n",
    "\n",
    "    # <-------- start gradient tape scope, step 3\n",
    "    # Open a GradientTape to record the operations run\n",
    "    # during the forward pass, which enables auto-differentiation.\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "       # Run the forward pass of the layer.\n",
    "       # The operations that the layer applies\n",
    "       # to its inputs are going to be recorded\n",
    "       # on the GradientTape.\n",
    "       logits = model(x_batch_train, training=True) \n",
    "\n",
    "       # Compute the loss value for this minibatch.\n",
    "       loss_value = loss_fn(y_batch_train, logits)  \n",
    "       print(loss_value )\n",
    "\n",
    "    # compute the gradient of weights w.r.t. loss  <-------- step 5\n",
    "    # Use the gradient tape to automatically retrieve\n",
    "    # the gradients of the trainable variables with respect to the loss.\n",
    "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "\n",
    "    # update the weight based on gradient  <---------- step 6\n",
    "    # Run one step of gradient descent by updating\n",
    "    # the value of the variables to minimize the loss.\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    train_acc_metric.update_state(y_batch_train, logits)\n",
    "    print(train_acc_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 20, 61, 75520, 1)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-11T12:58:58.615002Z",
     "iopub.status.busy": "2023-04-11T12:58:58.614684Z",
     "iopub.status.idle": "2023-04-11T13:00:08.746327Z",
     "shell.execute_reply": "2023-04-11T13:00:08.744036Z",
     "shell.execute_reply.started": "2023-04-11T12:58:58.614967Z"
    }
   },
   "outputs": [],
   "source": [
    "if CFG.is_training:\n",
    "    file_name = \"model.h5\"\n",
    "#     callbacks = [\n",
    "#         tf.keras.callbacks.ModelCheckpoint(\n",
    "#             file_name, \n",
    "#             save_best_only=True, \n",
    "#             restore_best_weights=True, \n",
    "#             monitor=\"val_accuracy\",\n",
    "#             mode=\"max\"\n",
    "#         ),\n",
    "#         tf.keras.callbacks.EarlyStopping(\n",
    "#             patience=5, \n",
    "#             monitor=\"val_accuracy\",\n",
    "#             mode=\"max\"\n",
    "#         )\n",
    "#     ]\n",
    "    model.fit(train_ds, epochs=1, validation_data=valid_ds)\n",
    "    model.save('/kaggle/input/islr-convlstm1d/model.h5',save_format='tf')\n",
    "    model = tf.keras.models.load_model(file_name)\n",
    "# else:\n",
    "#     model = tf.keras.models.load_model(\"/kaggle/input/islr-convlstm1d/model.h5\")\n",
    "model.evaluate(valid_ds)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.069654,
     "end_time": "2023-03-02T08:47:03.916954",
     "exception": false,
     "start_time": "2023-03-02T08:47:03.8473",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Create Model for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-11T13:00:08.747883Z",
     "iopub.status.idle": "2023-04-11T13:00:08.748454Z",
     "shell.execute_reply": "2023-04-11T13:00:08.748205Z",
     "shell.execute_reply.started": "2023-04-11T13:00:08.748177Z"
    },
    "papermill": {
     "duration": 0.086334,
     "end_time": "2023-03-02T08:47:04.072776",
     "exception": false,
     "start_time": "2023-03-02T08:47:03.986442",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def get_inference_model(model):\n",
    "#     inputs = tf.keras.Input((543, 3), dtype=tf.float32, name=\"inputs\")\n",
    "#     vector = tf.image.resize(inputs, (CFG.sequence_length, 543))\n",
    "#     vector = tf.where(tf.math.is_nan(vector), tf.zeros_like(vector), vector)\n",
    "#     vector = tf.expand_dims(vector, axis=0)\n",
    "#     vector = model(vector)\n",
    "#     output = tf.keras.layers.Activation(activation=\"linear\", name=\"outputs\")(vector)\n",
    "#     inference_model = tf.keras.Model(inputs=inputs, outputs=output) \n",
    "#     inference_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics=[\"accuracy\"])\n",
    "#     return inference_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-11T13:00:08.750352Z",
     "iopub.status.idle": "2023-04-11T13:00:08.750858Z",
     "shell.execute_reply": "2023-04-11T13:00:08.750633Z",
     "shell.execute_reply.started": "2023-04-11T13:00:08.750603Z"
    },
    "papermill": {
     "duration": 5.194577,
     "end_time": "2023-03-02T08:47:09.336507",
     "exception": false,
     "start_time": "2023-03-02T08:47:04.14193",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# inference_model = get_inference_model(model)\n",
    "# inference_model.summary()\n",
    "# tf.keras.utils.plot_model(inference_model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.071953,
     "end_time": "2023-03-02T08:47:09.784691",
     "exception": false,
     "start_time": "2023-03-02T08:47:09.712738",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Create submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-11T13:00:08.752458Z",
     "iopub.status.idle": "2023-04-11T13:00:08.758015Z",
     "shell.execute_reply": "2023-04-11T13:00:08.757844Z",
     "shell.execute_reply.started": "2023-04-11T13:00:08.757823Z"
    },
    "papermill": {
     "duration": 153.429402,
     "end_time": "2023-03-02T08:49:43.286145",
     "exception": false,
     "start_time": "2023-03-02T08:47:09.856743",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# converter = tf.lite.TFLiteConverter.from_keras_model(inference_model)\n",
    "# tflite_model = converter.convert()\n",
    "# model_path = \"model.tflite\"\n",
    "# # Save the model.\n",
    "# with open(model_path, 'wb') as f:\n",
    "#     f.write(tflite_model)\n",
    "# !zip submission.zip $model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.1124,
     "end_time": "2023-03-02T08:49:46.762554",
     "exception": false,
     "start_time": "2023-03-02T08:49:46.650154",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-11T13:00:08.759302Z",
     "iopub.status.idle": "2023-04-11T13:00:08.759928Z",
     "shell.execute_reply": "2023-04-11T13:00:08.759646Z",
     "shell.execute_reply.started": "2023-04-11T13:00:08.759618Z"
    },
    "papermill": {
     "duration": 12.663338,
     "end_time": "2023-03-02T08:49:59.538473",
     "exception": false,
     "start_time": "2023-03-02T08:49:46.875135",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install tflite-runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-11T13:00:08.761722Z",
     "iopub.status.idle": "2023-04-11T13:00:08.762695Z",
     "shell.execute_reply": "2023-04-11T13:00:08.762418Z",
     "shell.execute_reply.started": "2023-04-11T13:00:08.762388Z"
    },
    "papermill": {
     "duration": 29.201279,
     "end_time": "2023-03-02T08:50:28.816671",
     "exception": false,
     "start_time": "2023-03-02T08:49:59.615392",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import tflite_runtime.interpreter as tflite\n",
    "# interpreter = tflite.Interpreter(model_path)\n",
    "# found_signatures = list(interpreter.get_signature_list().keys())\n",
    "# prediction_fn = interpreter.get_signature_runner(\"serving_default\")\n",
    "# for i in tqdm(range(10000)):\n",
    "#     frames = load_relevant_data_subset(f'/kaggle/input/asl-signs/{train.iloc[i].path}')\n",
    "#     output = prediction_fn(inputs=frames)\n",
    "#     if i < 100:\n",
    "#         sign = np.argmax(output[\"outputs\"])\n",
    "#         print(f\"Predicted label: {index_label[sign]}, Actual Label: {train.iloc[i].sign}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
