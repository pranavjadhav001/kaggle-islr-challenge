{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "from focal_loss import SparseCategoricalFocalLoss\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import tensorflow_addons as tfa\n",
    "import os\n",
    "import tensorflow_io as tfio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_it_all(seed=7):\n",
    "    \"\"\" Attempt to be Reproducible \"\"\"\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "seed_it_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROWS_PER_FRAME = 543\n",
    "def load_relevant_data_subset(pq_path):\n",
    "    data_columns = ['x', 'y', 'z']\n",
    "    data = pd.read_parquet(pq_path, columns=data_columns)\n",
    "    n_frames = int(len(data) / ROWS_PER_FRAME)\n",
    "    data = data.values.reshape(n_frames, ROWS_PER_FRAME, len(data_columns))\n",
    "    return data.astype(np.float32)\n",
    "\n",
    "#test_data  = load_relevant_data_subset('train_landmark_files/16069/100015657.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIP = [\n",
    "            61, 185, 40, 39, 37, 0, 267, 269, 270, 409,\n",
    "            291, 146, 91, 181, 84, 17, 314, 405, 321, 375,\n",
    "            78, 191, 80, 81, 82, 13, 312, 311, 310, 415,\n",
    "            95, 88, 178, 87, 14, 317, 402, 318, 324, 308,\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    left_ROWS_per_frame = 21\n",
    "    sequence_length = 20\n",
    "    batch_size = 32\n",
    "    face_ROWS_per_frame = 468\n",
    "    lip_ROWS_per_frame = 40\n",
    "\n",
    "labels  = json.load(open('sign_to_prediction_index_map.json','r'))\n",
    "complete_df = pd.read_csv('extended_train.csv')\n",
    "complete_df = complete_df[complete_df['participant_id']!='37055']\n",
    "from sklearn.model_selection import train_test_split\n",
    "y = complete_df['sign']\n",
    "train_df, test_df = train_test_split(complete_df, test_size=0.2,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_loader(with_labels=True):\n",
    "    def load_video(video_path):\n",
    "        #print('herer')\n",
    "        video_df = tfio.IODataset.from_parquet(video_path)\n",
    "        #video_df = pd.read_parquet(video_path, engine='pyarrow')\n",
    "        #video_df.fillna(0,inplace=True)\n",
    "        left_df = video_df[video_df.type=='left_hand']\n",
    "        left_values = left_df[['x','y','z']].values\n",
    "        left_values = left_values.reshape(-1,CFG.left_ROWS_per_frame,3)\n",
    "        left_hand_array =  tf.image.resize(left_values, (CFG.sequence_length, CFG.left_ROWS_per_frame))\n",
    "        right_df = video_df[video_df.type=='right_hand']\n",
    "        right_values = right_df[['x','y','z']].values\n",
    "        right_values = right_values.reshape(-1,CFG.left_ROWS_per_frame,3)\n",
    "        right_hand_array =  tf.image.resize(right_values, (CFG.sequence_length, CFG.left_ROWS_per_frame))\n",
    "        return [left_hand_array, right_hand_array]\n",
    "    \n",
    "    def load_video_with_labels(path, label):\n",
    "        return load_video(path), labels[label]\n",
    "    \n",
    "    return load_video_with_labels if with_labels else load_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomData(tf.keras.utils.Sequence):\n",
    "    def __init__(self,df,num_frames=20,batch_size=8,shuffle=True,\\\n",
    "                 labels_path='sign_to_prediction_index_map.json'):\n",
    "        self.df = df\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.num_frames = num_frames\n",
    "        self.labels  = json.load(open('sign_to_prediction_index_map.json','r'))\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.df))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        batches = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        combined = np.zeros(shape=(self.batch_size,self.num_frames,\\\n",
    "                                        CFG.left_ROWS_per_frame+CFG.lip_ROWS_per_frame,3))\n",
    "        labels = []\n",
    "        for i,row_val in enumerate(batches):\n",
    "            row = self.df.iloc[row_val]\n",
    "            left_hand,lip = self.load_video(row['path'])\n",
    "            combined[i,:,:21,:] = left_hand\n",
    "            combined[i,:,21:,:] = lip\n",
    "            labels.append(self.labels[row['sign']])\n",
    "        return combined,np.asarray(labels)\n",
    "            \n",
    "    def load_video(self,video_path):\n",
    "        video_df = pd.read_parquet(video_path, engine='pyarrow')\n",
    "        #video_df.dropna(inplace=True)\n",
    "        \n",
    "        if video_df[video_df['type']=='left_hand']['x'].isna().mean() <= \\\n",
    "      video_df[video_df['type']=='right_hand']['x'].isna().mean():\n",
    "            left_df = video_df[video_df.type=='left_hand']\n",
    "            left_df.dropna(inplace=True)\n",
    "            if len(left_df) != 0:\n",
    "                left_values = left_df[['x','y','z']].values\n",
    "                left_values = left_values.reshape(-1,CFG.left_ROWS_per_frame,3)\n",
    "                left_hand_array =  tf.image.resize(left_values, (CFG.sequence_length, CFG.left_ROWS_per_frame))\n",
    "            else:\n",
    "                left_hand_array = tf.zeros(shape=(CFG.sequence_length, CFG.left_ROWS_per_frame,2),dtype=tf.float32)\n",
    "        else:\n",
    "            left_df = video_df[video_df.type=='right_hand']\n",
    "            left_df.dropna(inplace=True)\n",
    "            if len(left_df) != 0:\n",
    "                left_values = left_df[['x','y','z']].values\n",
    "                left_values = left_values.reshape(-1,CFG.left_ROWS_per_frame,3)\n",
    "                left_values[:,:,:1] = np.max(left_values[:,:,:1]) - left_values[:,:,:1] \n",
    "                left_hand_array =  tf.image.resize(left_values, (CFG.sequence_length, CFG.left_ROWS_per_frame))\n",
    "            else:\n",
    "                left_hand_array = tf.zeros(shape=(CFG.sequence_length, CFG.left_ROWS_per_frame,3),dtype=tf.float32)\n",
    "        \n",
    "        face_df = video_df[video_df.type=='face']\n",
    "        face_df.dropna(inplace=True)\n",
    "        face_df = face_df[['x','y','z']].values\n",
    "        face_df = face_df.reshape(-1,CFG.face_ROWS_per_frame,3)\n",
    "        lip_values = face_df[:,LIP,:]\n",
    "        if len(lip_values) != 0:\n",
    "            lip_values_array =  tf.image.resize(lip_values, (CFG.sequence_length, CFG.lip_ROWS_per_frame))\n",
    "        else:\n",
    "            lip_values_array = tf.zeros(shape=(CFG.sequence_length, CFG.lip_ROWS_per_frame,3),dtype=tf.float32)\n",
    "        \n",
    "        return left_hand_array,lip_values_array\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)//self.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = CustomData(train_df,num_frames=CFG.sequence_length,batch_size=256)\n",
    "test_datagen = CustomData(test_df,num_frames=CFG.sequence_length,batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# define the graph convolution layer\n",
    "class GraphConv(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, activation=None, use_bias=True):\n",
    "        super(GraphConv, self).__init__()\n",
    "        self.units = units\n",
    "        self.activation = activation\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(name='kernel', shape=(input_shape[-1], self.units),\n",
    "                                      initializer='glorot_uniform', trainable=True)\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(name='bias', shape=(self.units,),\n",
    "                                        initializer='zeros', trainable=True)\n",
    "\n",
    "    def call(self, inputs, adj):\n",
    "        x = tf.matmul(adj, inputs)\n",
    "        x = tf.matmul(x, self.kernel)\n",
    "        if self.use_bias:\n",
    "            x = x + self.bias\n",
    "        if self.activation is not None:\n",
    "            x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "# define the graph pooling layer\n",
    "class GraphPool(tf.keras.layers.Layer):\n",
    "    def __init__(self, activation=None):\n",
    "        super(GraphPool, self).__init__()\n",
    "        self.activation = activation\n",
    "\n",
    "    def call(self, inputs, adj):\n",
    "        x = tf.matmul(adj, inputs)\n",
    "        if self.activation is not None:\n",
    "            x = self.activation(x)\n",
    "        return x\n",
    "def build_gcn(input_shape, num_classes):\n",
    "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
    "    x = tf.slice(inputs, [0,0,0,0], [-1,20, 61, 2])\n",
    "    # adjacency matrix\n",
    "    adj = tf.linalg.diag(tf.ones(shape=(input_shape[1],)))\n",
    "    adj = tf.expand_dims(adj, axis=0)\n",
    "    adj = tf.tile(adj, [input_shape[0], 1, 1])\n",
    "    \n",
    "    # first GCN layer\n",
    "    x = GraphConv(units=64, activation=tf.nn.relu)(x, adj)\n",
    "    \n",
    "    # second GCN layer\n",
    "    x = GraphConv(units=128, activation=tf.nn.relu)(x, adj)\n",
    "    \n",
    "    # graph pooling layer\n",
    "    x = GraphPool(activation=tf.nn.relu)(x, adj)\n",
    "    # Output layer\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = tf.keras.layers.Dense(NUM_CLASSES, activation=tf.keras.activations.softmax)(x)\n",
    "    outputs = x\n",
    "    # compile the model\n",
    "    model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_gcn((20,61,2),250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 20, 61, 2)]       0         \n",
      "                                                                 \n",
      " tf.slice_3 (TFOpLambda)     (None, 20, 61, 2)         0         \n",
      "                                                                 \n",
      " graph_conv_6 (GraphConv)    (None, 20, 61, 64)        192       \n",
      "                                                                 \n",
      " graph_conv_7 (GraphConv)    (None, 20, 61, 128)       8320      \n",
      "                                                                 \n",
      " graph_pool_3 (GraphPool)    (None, 20, 61, 128)       0         \n",
      "                                                                 \n",
      " global_average_pooling2d_1   (None, 128)              0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 250)               32250     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 40,762\n",
      "Trainable params: 40,762\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If True, processing data from scratch\n",
    "# If False, loads preprocessed data\n",
    "PREPROCESS_DATA = False\n",
    "TRAIN_MODEL = True\n",
    "# True: use 10% of participants as validation set\n",
    "# False: use all data for training -> gives better LB result\n",
    "USE_VAL = False\n",
    "N_ROWS = 543\n",
    "N_DIMS = 3\n",
    "DIM_NAMES = ['x', 'y', 'z']\n",
    "SEED = 42\n",
    "NUM_CLASSES = 250\n",
    "INPUT_SIZE = 64\n",
    "BATCH_ALL_SIGNS_N = 4\n",
    "BATCH_SIZE = 256\n",
    "N_EPOCHS = 100\n",
    "LR_MAX = 1e-3\n",
    "N_WARMUP_EPOCHS = 0\n",
    "WD_RATIO = 0.05\n",
    "MASK_VAL = 4237\n",
    "N_COLS = 61"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom callback to update weight decay with learning rate\n",
    "class WeightDecayCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, wd_ratio=WD_RATIO):\n",
    "        self.step_counter = 0\n",
    "        self.wd_ratio = wd_ratio\n",
    "    \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        model.optimizer.weight_decay = model.optimizer.learning_rate * self.wd_ratio\n",
    "        print(f'learning rate: {model.optimizer.learning_rate.numpy():.2e}, weight decay: {model.optimizer.weight_decay.numpy():.2e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lrfn(current_step, num_warmup_steps, lr_max, num_cycles=0.50, num_training_steps=N_EPOCHS):\n",
    "    \n",
    "    if current_step < num_warmup_steps:\n",
    "        if WARMUP_METHOD == 'log':\n",
    "            return lr_max * 0.10 ** (num_warmup_steps - current_step)\n",
    "        else:\n",
    "            return lr_max * 2 ** -(num_warmup_steps - current_step)\n",
    "    else:\n",
    "        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n",
    "\n",
    "        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress))) * lr_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate for encoder\n",
    "LR_SCHEDULE = [lrfn(step, num_warmup_steps=N_WARMUP_EPOCHS, lr_max=LR_MAX, num_cycles=0.50) for step in range(N_EPOCHS)]\n",
    "lr_callback = tf.keras.callbacks.LearningRateScheduler(lambda step: LR_SCHEDULE[step], verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.load('X_train_20x61_left.npy')\n",
    "y_train = np.load('y_train_20x61_left.npy')\n",
    "x_test = np.load('X_test_20x61_left.npy')\n",
    "y_test = np.load('y_test_20x61_left.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 20, 61, 3)]       0         \n",
      "                                                                 \n",
      " tf.slice (TFOpLambda)       (None, 20, 61, 2)         0         \n",
      "                                                                 \n",
      " graph_conv (GraphConv)      (None, 20, 61, 64)        192       \n",
      "                                                                 \n",
      " graph_conv_1 (GraphConv)    (None, 20, 61, 128)       8320      \n",
      "                                                                 \n",
      " graph_pool (GraphPool)      (None, 20, 61, 128)       0         \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 128)              0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 250)               32250     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 40,762\n",
      "Trainable params: 40,762\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.001.\n",
      "learning rate: 1.00e-03, weight decay: 5.00e-05\n",
      "Epoch 1/100\n",
      "  6/295 [..............................] - ETA: 3s - loss: 5.5224 - accuracy: 0.0013      WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0029s vs `on_train_batch_end` time: 0.0072s). Check your callbacks.\n",
      "291/295 [============================>.] - ETA: 0s - loss: 5.5144 - accuracy: 0.0047\n",
      "Epoch 1: val_accuracy improved from -inf to 0.00680, saving model to models/weights_042323_18_47.h5\n",
      "295/295 [==============================] - 5s 15ms/step - loss: 5.5142 - accuracy: 0.0047 - val_loss: 5.4899 - val_accuracy: 0.0068 - lr: 0.0010\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.0009997532801828658.\n",
      "learning rate: 1.00e-03, weight decay: 5.00e-05\n",
      "Epoch 2/100\n",
      "293/295 [============================>.] - ETA: 0s - loss: 5.4520 - accuracy: 0.0076\n",
      "Epoch 2: val_accuracy improved from 0.00680 to 0.00824, saving model to models/weights_042323_18_47.h5\n",
      "295/295 [==============================] - 4s 14ms/step - loss: 5.4517 - accuracy: 0.0076 - val_loss: 5.4122 - val_accuracy: 0.0082 - lr: 9.9975e-04\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.0009990133642141358.\n",
      "learning rate: 9.99e-04, weight decay: 5.00e-05\n",
      "Epoch 3/100\n",
      "291/295 [============================>.] - ETA: 0s - loss: 5.3830 - accuracy: 0.0096\n",
      "Epoch 3: val_accuracy improved from 0.00824 to 0.01043, saving model to models/weights_042323_18_47.h5\n",
      "295/295 [==============================] - 4s 14ms/step - loss: 5.3827 - accuracy: 0.0096 - val_loss: 5.3571 - val_accuracy: 0.0104 - lr: 9.9901e-04\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.00099778098230154.\n",
      "learning rate: 9.98e-04, weight decay: 4.99e-05\n",
      "Epoch 4/100\n",
      "291/295 [============================>.] - ETA: 0s - loss: 5.3332 - accuracy: 0.0124\n",
      "Epoch 4: val_accuracy improved from 0.01043 to 0.01514, saving model to models/weights_042323_18_47.h5\n",
      "295/295 [==============================] - 4s 14ms/step - loss: 5.3330 - accuracy: 0.0124 - val_loss: 5.3129 - val_accuracy: 0.0151 - lr: 9.9778e-04\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.000996057350657239.\n",
      "learning rate: 9.96e-04, weight decay: 4.98e-05\n",
      "Epoch 5/100\n",
      "292/295 [============================>.] - ETA: 0s - loss: 5.2922 - accuracy: 0.0144\n",
      "Epoch 5: val_accuracy improved from 0.01514 to 0.01562, saving model to models/weights_042323_18_47.h5\n",
      "295/295 [==============================] - 4s 13ms/step - loss: 5.2921 - accuracy: 0.0144 - val_loss: 5.2786 - val_accuracy: 0.0156 - lr: 9.9606e-04\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.0009938441702975688.\n",
      "learning rate: 9.94e-04, weight decay: 4.97e-05\n",
      "Epoch 6/100\n",
      "293/295 [============================>.] - ETA: 0s - loss: 5.2576 - accuracy: 0.0164\n",
      "Epoch 6: val_accuracy improved from 0.01562 to 0.01777, saving model to models/weights_042323_18_47.h5\n",
      "295/295 [==============================] - 4s 14ms/step - loss: 5.2577 - accuracy: 0.0164 - val_loss: 5.2470 - val_accuracy: 0.0178 - lr: 9.9384e-04\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.0009911436253643444.\n",
      "learning rate: 9.91e-04, weight decay: 4.96e-05\n",
      "Epoch 7/100\n",
      "292/295 [============================>.] - ETA: 0s - loss: 5.2264 - accuracy: 0.0176\n",
      "Epoch 7: val_accuracy improved from 0.01777 to 0.01798, saving model to models/weights_042323_18_47.h5\n",
      "295/295 [==============================] - 4s 13ms/step - loss: 5.2262 - accuracy: 0.0177 - val_loss: 5.2155 - val_accuracy: 0.0180 - lr: 9.9114e-04\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 0.0009879583809693738.\n",
      "learning rate: 9.88e-04, weight decay: 4.94e-05\n",
      "Epoch 8/100\n",
      "294/295 [============================>.] - ETA: 0s - loss: 5.1955 - accuracy: 0.0201\n",
      "Epoch 8: val_accuracy did not improve from 0.01798\n",
      "295/295 [==============================] - 4s 13ms/step - loss: 5.1956 - accuracy: 0.0201 - val_loss: 5.1919 - val_accuracy: 0.0165 - lr: 9.8796e-04\n",
      "\n",
      "Epoch 9: LearningRateScheduler setting learning rate to 0.0009842915805643156.\n",
      "learning rate: 9.84e-04, weight decay: 4.92e-05\n",
      "Epoch 9/100\n",
      "292/295 [============================>.] - ETA: 0s - loss: 5.1702 - accuracy: 0.0214\n",
      "Epoch 9: val_accuracy improved from 0.01798 to 0.02167, saving model to models/weights_042323_18_47.h5\n",
      "295/295 [==============================] - 4s 14ms/step - loss: 5.1698 - accuracy: 0.0214 - val_loss: 5.1660 - val_accuracy: 0.0217 - lr: 9.8429e-04\n",
      "\n",
      "Epoch 10: LearningRateScheduler setting learning rate to 0.0009801468428384716.\n",
      "learning rate: 9.80e-04, weight decay: 4.90e-05\n",
      "Epoch 10/100\n",
      "294/295 [============================>.] - ETA: 0s - loss: 5.1483 - accuracy: 0.0226\n",
      "Epoch 10: val_accuracy improved from 0.02167 to 0.02173, saving model to models/weights_042323_18_47.h5\n",
      "295/295 [==============================] - 4s 14ms/step - loss: 5.1483 - accuracy: 0.0226 - val_loss: 5.1515 - val_accuracy: 0.0217 - lr: 9.8015e-04\n",
      "\n",
      "Epoch 11: LearningRateScheduler setting learning rate to 0.0009755282581475768.\n",
      "learning rate: 9.76e-04, weight decay: 4.88e-05\n",
      "Epoch 11/100\n",
      "293/295 [============================>.] - ETA: 0s - loss: 5.1320 - accuracy: 0.0232\n",
      "Epoch 11: val_accuracy improved from 0.02173 to 0.02317, saving model to models/weights_042323_18_47.h5\n",
      "295/295 [==============================] - 4s 14ms/step - loss: 5.1320 - accuracy: 0.0232 - val_loss: 5.1339 - val_accuracy: 0.0232 - lr: 9.7553e-04\n",
      "\n",
      "Epoch 12: LearningRateScheduler setting learning rate to 0.0009704403844771128.\n",
      "learning rate: 9.70e-04, weight decay: 4.85e-05\n",
      "Epoch 12/100\n",
      "294/295 [============================>.] - ETA: 0s - loss: 5.1183 - accuracy: 0.0234\n",
      "Epoch 12: val_accuracy did not improve from 0.02317\n",
      "295/295 [==============================] - 4s 13ms/step - loss: 5.1182 - accuracy: 0.0234 - val_loss: 5.1237 - val_accuracy: 0.0231 - lr: 9.7044e-04\n",
      "\n",
      "Epoch 13: LearningRateScheduler setting learning rate to 0.0009648882429441257.\n",
      "learning rate: 9.65e-04, weight decay: 4.82e-05\n",
      "Epoch 13/100\n",
      "293/295 [============================>.] - ETA: 0s - loss: 5.1072 - accuracy: 0.0237\n",
      "Epoch 13: val_accuracy improved from 0.02317 to 0.02408, saving model to models/weights_042323_18_47.h5\n",
      "295/295 [==============================] - 4s 14ms/step - loss: 5.1067 - accuracy: 0.0238 - val_loss: 5.1151 - val_accuracy: 0.0241 - lr: 9.6489e-04\n",
      "\n",
      "Epoch 14: LearningRateScheduler setting learning rate to 0.0009588773128419905.\n",
      "learning rate: 9.59e-04, weight decay: 4.79e-05\n",
      "Epoch 14/100\n",
      "294/295 [============================>.] - ETA: 0s - loss: 5.0967 - accuracy: 0.0256\n",
      "Epoch 14: val_accuracy improved from 0.02408 to 0.02429, saving model to models/weights_042323_18_47.h5\n",
      "295/295 [==============================] - 4s 14ms/step - loss: 5.0968 - accuracy: 0.0257 - val_loss: 5.1063 - val_accuracy: 0.0243 - lr: 9.5888e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15: LearningRateScheduler setting learning rate to 0.0009524135262330098.\n",
      "learning rate: 9.52e-04, weight decay: 4.76e-05\n",
      "Epoch 15/100\n",
      "293/295 [============================>.] - ETA: 0s - loss: 5.0894 - accuracy: 0.0251\n",
      "Epoch 15: val_accuracy improved from 0.02429 to 0.02461, saving model to models/weights_042323_18_47.h5\n",
      "295/295 [==============================] - 4s 14ms/step - loss: 5.0889 - accuracy: 0.0251 - val_loss: 5.0966 - val_accuracy: 0.0246 - lr: 9.5241e-04\n",
      "\n",
      "Epoch 16: LearningRateScheduler setting learning rate to 0.0009455032620941839.\n",
      "learning rate: 9.46e-04, weight decay: 4.73e-05\n",
      "Epoch 16/100\n",
      "292/295 [============================>.] - ETA: 0s - loss: 5.0812 - accuracy: 0.0258\n",
      "Epoch 16: val_accuracy did not improve from 0.02461\n",
      "295/295 [==============================] - 4s 13ms/step - loss: 5.0812 - accuracy: 0.0258 - val_loss: 5.0920 - val_accuracy: 0.0244 - lr: 9.4550e-04\n",
      "\n",
      "Epoch 17: LearningRateScheduler setting learning rate to 0.0009381533400219318.\n",
      "learning rate: 9.38e-04, weight decay: 4.69e-05\n",
      "Epoch 17/100\n",
      "293/295 [============================>.] - ETA: 0s - loss: 5.0752 - accuracy: 0.0263\n",
      "Epoch 17: val_accuracy improved from 0.02461 to 0.02697, saving model to models/weights_042323_18_47.h5\n",
      "295/295 [==============================] - 4s 14ms/step - loss: 5.0752 - accuracy: 0.0263 - val_loss: 5.0849 - val_accuracy: 0.0270 - lr: 9.3815e-04\n",
      "\n",
      "Epoch 18: LearningRateScheduler setting learning rate to 0.0009303710135019718.\n",
      "learning rate: 9.30e-04, weight decay: 4.65e-05\n",
      "Epoch 18/100\n",
      "294/295 [============================>.] - ETA: 0s - loss: 5.0685 - accuracy: 0.0266\n",
      "Epoch 18: val_accuracy did not improve from 0.02697\n",
      "295/295 [==============================] - 4s 14ms/step - loss: 5.0686 - accuracy: 0.0266 - val_loss: 5.0799 - val_accuracy: 0.0251 - lr: 9.3037e-04\n",
      "\n",
      "Epoch 19: LearningRateScheduler setting learning rate to 0.0009221639627510075.\n",
      "learning rate: 9.22e-04, weight decay: 4.61e-05\n",
      "Epoch 19/100\n",
      "292/295 [============================>.] - ETA: 0s - loss: 5.0627 - accuracy: 0.0272\n",
      "Epoch 19: val_accuracy did not improve from 0.02697\n",
      "295/295 [==============================] - 4s 15ms/step - loss: 5.0625 - accuracy: 0.0271 - val_loss: 5.0741 - val_accuracy: 0.0269 - lr: 9.2216e-04\n",
      "\n",
      "Epoch 20: LearningRateScheduler setting learning rate to 0.0009135402871372809.\n",
      "learning rate: 9.14e-04, weight decay: 4.57e-05\n",
      "Epoch 20/100\n",
      "292/295 [============================>.] - ETA: 0s - loss: 5.0570 - accuracy: 0.0272\n",
      "Epoch 20: val_accuracy did not improve from 0.02697\n",
      "295/295 [==============================] - 4s 15ms/step - loss: 5.0569 - accuracy: 0.0272 - val_loss: 5.0701 - val_accuracy: 0.0269 - lr: 9.1354e-04\n",
      "\n",
      "Epoch 21: LearningRateScheduler setting learning rate to 0.0009045084971874737.\n",
      "learning rate: 9.05e-04, weight decay: 4.52e-05\n",
      "Epoch 21/100\n",
      "291/295 [============================>.] - ETA: 0s - loss: 5.0519 - accuracy: 0.0294\n",
      "Epoch 21: val_accuracy did not improve from 0.02697\n",
      "295/295 [==============================] - 4s 14ms/step - loss: 5.0513 - accuracy: 0.0293 - val_loss: 5.0655 - val_accuracy: 0.0258 - lr: 9.0451e-04\n",
      "\n",
      "Epoch 22: LearningRateScheduler setting learning rate to 0.0008950775061878452.\n",
      "learning rate: 8.95e-04, weight decay: 4.48e-05\n",
      "Epoch 22/100\n",
      "294/295 [============================>.] - ETA: 0s - loss: 5.0464 - accuracy: 0.0279\n",
      "Epoch 22: val_accuracy did not improve from 0.02697\n",
      "295/295 [==============================] - 4s 13ms/step - loss: 5.0461 - accuracy: 0.0280 - val_loss: 5.0610 - val_accuracy: 0.0267 - lr: 8.9508e-04\n",
      "\n",
      "Epoch 23: LearningRateScheduler setting learning rate to 0.0008852566213878947.\n",
      "learning rate: 8.85e-04, weight decay: 4.43e-05\n",
      "Epoch 23/100\n",
      "294/295 [============================>.] - ETA: 0s - loss: 5.0414 - accuracy: 0.0283\n",
      "Epoch 23: val_accuracy improved from 0.02697 to 0.02820, saving model to models/weights_042323_18_47.h5\n",
      "295/295 [==============================] - 4s 13ms/step - loss: 5.0412 - accuracy: 0.0283 - val_loss: 5.0557 - val_accuracy: 0.0282 - lr: 8.8526e-04\n",
      "\n",
      "Epoch 24: LearningRateScheduler setting learning rate to 0.0008750555348152298.\n",
      "learning rate: 8.75e-04, weight decay: 4.38e-05\n",
      "Epoch 24/100\n",
      "295/295 [==============================] - ETA: 0s - loss: 5.0363 - accuracy: 0.0293\n",
      "Epoch 24: val_accuracy did not improve from 0.02820\n",
      "295/295 [==============================] - 4s 13ms/step - loss: 5.0363 - accuracy: 0.0293 - val_loss: 5.0584 - val_accuracy: 0.0273 - lr: 8.7506e-04\n",
      "\n",
      "Epoch 25: LearningRateScheduler setting learning rate to 0.0008644843137107057.\n",
      "learning rate: 8.64e-04, weight decay: 4.32e-05\n",
      "Epoch 25/100\n",
      "293/295 [============================>.] - ETA: 0s - loss: 5.0317 - accuracy: 0.0289\n",
      "Epoch 25: val_accuracy improved from 0.02820 to 0.02938, saving model to models/weights_042323_18_47.h5\n",
      "295/295 [==============================] - 4s 14ms/step - loss: 5.0318 - accuracy: 0.0290 - val_loss: 5.0468 - val_accuracy: 0.0294 - lr: 8.6448e-04\n",
      "\n",
      "Epoch 26: LearningRateScheduler setting learning rate to 0.0008535533905932737.\n",
      "learning rate: 8.54e-04, weight decay: 4.27e-05\n",
      "Epoch 26/100\n",
      "291/295 [============================>.] - ETA: 0s - loss: 5.0268 - accuracy: 0.0295\n",
      "Epoch 26: val_accuracy did not improve from 0.02938\n",
      "295/295 [==============================] - 4s 14ms/step - loss: 5.0267 - accuracy: 0.0296 - val_loss: 5.0439 - val_accuracy: 0.0285 - lr: 8.5355e-04\n",
      "\n",
      "Epoch 27: LearningRateScheduler setting learning rate to 0.0008422735529643444.\n",
      "learning rate: 8.42e-04, weight decay: 4.21e-05\n",
      "Epoch 27/100\n",
      "291/295 [============================>.] - ETA: 0s - loss: 5.0217 - accuracy: 0.0299\n",
      "Epoch 27: val_accuracy did not improve from 0.02938\n",
      "295/295 [==============================] - 4s 14ms/step - loss: 5.0217 - accuracy: 0.0299 - val_loss: 5.0403 - val_accuracy: 0.0284 - lr: 8.4227e-04\n",
      "\n",
      "Epoch 28: LearningRateScheduler setting learning rate to 0.0008306559326618259.\n",
      "learning rate: 8.31e-04, weight decay: 4.15e-05\n",
      "Epoch 28/100\n",
      "294/295 [============================>.] - ETA: 0s - loss: 5.0178 - accuracy: 0.0299\n",
      "Epoch 28: val_accuracy did not improve from 0.02938\n",
      "295/295 [==============================] - 4s 14ms/step - loss: 5.0178 - accuracy: 0.0298 - val_loss: 5.0380 - val_accuracy: 0.0287 - lr: 8.3066e-04\n",
      "\n",
      "Epoch 29: LearningRateScheduler setting learning rate to 0.0008187119948743449.\n",
      "learning rate: 8.19e-04, weight decay: 4.09e-05\n",
      "Epoch 29/100\n",
      "294/295 [============================>.] - ETA: 0s - loss: 5.0134 - accuracy: 0.0308\n",
      "Epoch 29: val_accuracy improved from 0.02938 to 0.03114, saving model to models/weights_042323_18_47.h5\n",
      "295/295 [==============================] - 4s 14ms/step - loss: 5.0134 - accuracy: 0.0307 - val_loss: 5.0311 - val_accuracy: 0.0311 - lr: 8.1871e-04\n",
      "\n",
      "Epoch 30: LearningRateScheduler setting learning rate to 0.0008064535268264883.\n",
      "learning rate: 8.06e-04, weight decay: 4.03e-05\n",
      "Epoch 30/100\n",
      "293/295 [============================>.] - ETA: 0s - loss: 5.0095 - accuracy: 0.0309\n",
      "Epoch 30: val_accuracy did not improve from 0.03114\n",
      "295/295 [==============================] - 4s 14ms/step - loss: 5.0100 - accuracy: 0.0309 - val_loss: 5.0301 - val_accuracy: 0.0294 - lr: 8.0645e-04\n",
      "\n",
      "Epoch 31: LearningRateScheduler setting learning rate to 0.0007938926261462366.\n",
      "learning rate: 7.94e-04, weight decay: 3.97e-05\n",
      "Epoch 31/100\n",
      "291/295 [============================>.] - ETA: 0s - loss: 5.0051 - accuracy: 0.0311\n",
      "Epoch 31: val_accuracy did not improve from 0.03114\n",
      "295/295 [==============================] - 4s 13ms/step - loss: 5.0060 - accuracy: 0.0312 - val_loss: 5.0250 - val_accuracy: 0.0308 - lr: 7.9389e-04\n",
      "\n",
      "Epoch 32: LearningRateScheduler setting learning rate to 0.0007810416889260653.\n",
      "learning rate: 7.81e-04, weight decay: 3.91e-05\n",
      "Epoch 32/100\n",
      "293/295 [============================>.] - ETA: 0s - loss: 5.0020 - accuracy: 0.0319\n",
      "Epoch 32: val_accuracy did not improve from 0.03114\n",
      "295/295 [==============================] - 4s 14ms/step - loss: 5.0022 - accuracy: 0.0319 - val_loss: 5.0289 - val_accuracy: 0.0288 - lr: 7.8104e-04\n",
      "\n",
      "Epoch 33: LearningRateScheduler setting learning rate to 0.0007679133974894983.\n",
      "learning rate: 7.68e-04, weight decay: 3.84e-05\n",
      "Epoch 33/100\n",
      "291/295 [============================>.] - ETA: 0s - loss: 4.9998 - accuracy: 0.0322\n",
      "Epoch 33: val_accuracy did not improve from 0.03114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "295/295 [==============================] - 4s 14ms/step - loss: 4.9990 - accuracy: 0.0322 - val_loss: 5.0204 - val_accuracy: 0.0306 - lr: 7.6791e-04\n",
      "\n",
      "Epoch 34: LearningRateScheduler setting learning rate to 0.0007545207078751857.\n",
      "learning rate: 7.55e-04, weight decay: 3.77e-05\n",
      "Epoch 34/100\n",
      "292/295 [============================>.] - ETA: 0s - loss: 4.9959 - accuracy: 0.0326\n",
      "Epoch 34: val_accuracy improved from 0.03114 to 0.03146, saving model to models/weights_042323_18_47.h5\n",
      "295/295 [==============================] - 4s 14ms/step - loss: 4.9958 - accuracy: 0.0326 - val_loss: 5.0162 - val_accuracy: 0.0315 - lr: 7.5452e-04\n",
      "\n",
      "Epoch 35: LearningRateScheduler setting learning rate to 0.0007408768370508576.\n",
      "learning rate: 7.41e-04, weight decay: 3.70e-05\n",
      "Epoch 35/100\n",
      "292/295 [============================>.] - ETA: 0s - loss: 4.9929 - accuracy: 0.0321\n",
      "Epoch 35: val_accuracy improved from 0.03146 to 0.03216, saving model to models/weights_042323_18_47.h5\n",
      "295/295 [==============================] - 4s 14ms/step - loss: 4.9923 - accuracy: 0.0322 - val_loss: 5.0145 - val_accuracy: 0.0322 - lr: 7.4088e-04\n",
      "\n",
      "Epoch 36: LearningRateScheduler setting learning rate to 0.0007269952498697733.\n",
      "learning rate: 7.27e-04, weight decay: 3.63e-05\n",
      "Epoch 36/100\n",
      "291/295 [============================>.] - ETA: 0s - loss: 4.9894 - accuracy: 0.0315\n",
      "Epoch 36: val_accuracy did not improve from 0.03216\n",
      "295/295 [==============================] - 4s 13ms/step - loss: 4.9891 - accuracy: 0.0315 - val_loss: 5.0124 - val_accuracy: 0.0315 - lr: 7.2700e-04\n",
      "\n",
      "Epoch 37: LearningRateScheduler setting learning rate to 0.0007128896457825364.\n",
      "learning rate: 7.13e-04, weight decay: 3.56e-05\n",
      "Epoch 37/100\n",
      "295/295 [==============================] - ETA: 0s - loss: 4.9865 - accuracy: 0.0329\n",
      "Epoch 37: val_accuracy did not improve from 0.03216\n",
      "295/295 [==============================] - 4s 14ms/step - loss: 4.9865 - accuracy: 0.0329 - val_loss: 5.0106 - val_accuracy: 0.0295 - lr: 7.1289e-04\n",
      "\n",
      "Epoch 38: LearningRateScheduler setting learning rate to 0.0006985739453173903.\n",
      "learning rate: 6.99e-04, weight decay: 3.49e-05\n",
      "Epoch 38/100\n",
      "292/295 [============================>.] - ETA: 0s - loss: 4.9841 - accuracy: 0.0331\n",
      "Epoch 38: val_accuracy did not improve from 0.03216\n",
      "295/295 [==============================] - 4s 14ms/step - loss: 4.9837 - accuracy: 0.0332 - val_loss: 5.0108 - val_accuracy: 0.0307 - lr: 6.9857e-04\n",
      "\n",
      "Epoch 39: LearningRateScheduler setting learning rate to 0.0006840622763423391.\n",
      "learning rate: 6.84e-04, weight decay: 3.42e-05\n",
      "Epoch 39/100\n",
      "291/295 [============================>.] - ETA: 0s - loss: 4.9809 - accuracy: 0.0335\n",
      "Epoch 39: val_accuracy did not improve from 0.03216\n",
      "295/295 [==============================] - 4s 14ms/step - loss: 4.9808 - accuracy: 0.0336 - val_loss: 5.0057 - val_accuracy: 0.0316 - lr: 6.8406e-04\n",
      "\n",
      "Epoch 40: LearningRateScheduler setting learning rate to 0.0006693689601226458.\n",
      "learning rate: 6.69e-04, weight decay: 3.35e-05\n",
      "Epoch 40/100\n",
      "295/295 [==============================] - ETA: 0s - loss: 4.9790 - accuracy: 0.0333\n",
      "Epoch 40: val_accuracy did not improve from 0.03216\n",
      "295/295 [==============================] - 4s 14ms/step - loss: 4.9790 - accuracy: 0.0333 - val_loss: 5.0028 - val_accuracy: 0.0315 - lr: 6.6937e-04\n",
      "\n",
      "Epoch 41: LearningRateScheduler setting learning rate to 0.0006545084971874737.\n",
      "learning rate: 6.55e-04, weight decay: 3.27e-05\n",
      "Epoch 41/100\n",
      "293/295 [============================>.] - ETA: 0s - loss: 4.9763 - accuracy: 0.0346\n",
      "Epoch 41: val_accuracy did not improve from 0.03216\n",
      "295/295 [==============================] - 4s 14ms/step - loss: 4.9764 - accuracy: 0.0346 - val_loss: 5.0001 - val_accuracy: 0.0308 - lr: 6.5451e-04\n",
      "\n",
      "Epoch 42: LearningRateScheduler setting learning rate to 0.0006394955530196147.\n",
      "learning rate: 6.39e-04, weight decay: 3.20e-05\n",
      "Epoch 42/100\n",
      "294/295 [============================>.] - ETA: 0s - loss: 4.9737 - accuracy: 0.0344\n",
      "Epoch 42: val_accuracy improved from 0.03216 to 0.03269, saving model to models/weights_042323_18_47.h5\n",
      "295/295 [==============================] - 4s 14ms/step - loss: 4.9740 - accuracy: 0.0344 - val_loss: 5.0002 - val_accuracy: 0.0327 - lr: 6.3950e-04\n",
      "\n",
      "Epoch 43: LearningRateScheduler setting learning rate to 0.0006243449435824276.\n",
      "learning rate: 6.24e-04, weight decay: 3.12e-05\n",
      "Epoch 43/100\n",
      "291/295 [============================>.] - ETA: 0s - loss: 4.9722 - accuracy: 0.0340\n",
      "Epoch 43: val_accuracy did not improve from 0.03269\n",
      "295/295 [==============================] - 4s 14ms/step - loss: 4.9724 - accuracy: 0.0340 - val_loss: 4.9986 - val_accuracy: 0.0317 - lr: 6.2434e-04\n",
      "\n",
      "Epoch 44: LearningRateScheduler setting learning rate to 0.0006090716206982714.\n",
      "learning rate: 6.09e-04, weight decay: 3.05e-05\n",
      "Epoch 44/100\n",
      "295/295 [==============================] - ETA: 0s - loss: 4.9699 - accuracy: 0.0338\n",
      "Epoch 44: val_accuracy did not improve from 0.03269\n",
      "295/295 [==============================] - 4s 14ms/step - loss: 4.9699 - accuracy: 0.0338 - val_loss: 4.9960 - val_accuracy: 0.0319 - lr: 6.0907e-04\n",
      "\n",
      "Epoch 45: LearningRateScheduler setting learning rate to 0.0005936906572928624.\n",
      "learning rate: 5.94e-04, weight decay: 2.97e-05\n",
      "Epoch 45/100\n",
      "294/295 [============================>.] - ETA: 0s - loss: 4.9680 - accuracy: 0.0352\n",
      "Epoch 45: val_accuracy improved from 0.03269 to 0.03344, saving model to models/weights_042323_18_47.h5\n",
      "295/295 [==============================] - 4s 14ms/step - loss: 4.9683 - accuracy: 0.0351 - val_loss: 4.9942 - val_accuracy: 0.0334 - lr: 5.9369e-04\n",
      "\n",
      "Epoch 46: LearningRateScheduler setting learning rate to 0.0005782172325201155.\n",
      "learning rate: 5.78e-04, weight decay: 2.89e-05\n",
      "Epoch 46/100\n",
      "295/295 [==============================] - ETA: 0s - loss: 4.9657 - accuracy: 0.0347\n",
      "Epoch 46: val_accuracy did not improve from 0.03344\n",
      "295/295 [==============================] - 4s 14ms/step - loss: 4.9657 - accuracy: 0.0347 - val_loss: 4.9958 - val_accuracy: 0.0325 - lr: 5.7822e-04\n",
      "\n",
      "Epoch 47: LearningRateScheduler setting learning rate to 0.0005626666167821521.\n",
      "learning rate: 5.63e-04, weight decay: 2.81e-05\n",
      "Epoch 47/100\n",
      "294/295 [============================>.] - ETA: 0s - loss: 4.9641 - accuracy: 0.0358\n",
      "Epoch 47: val_accuracy did not improve from 0.03344\n",
      "295/295 [==============================] - 4s 14ms/step - loss: 4.9643 - accuracy: 0.0357 - val_loss: 4.9909 - val_accuracy: 0.0334 - lr: 5.6267e-04\n",
      "\n",
      "Epoch 48: LearningRateScheduler setting learning rate to 0.0005470541566592572.\n",
      "learning rate: 5.47e-04, weight decay: 2.74e-05\n",
      "Epoch 48/100\n",
      "293/295 [============================>.] - ETA: 0s - loss: 4.9612 - accuracy: 0.0354\n",
      "Epoch 48: val_accuracy did not improve from 0.03344\n",
      "295/295 [==============================] - 4s 14ms/step - loss: 4.9619 - accuracy: 0.0353 - val_loss: 4.9900 - val_accuracy: 0.0332 - lr: 5.4705e-04\n",
      "\n",
      "Epoch 49: LearningRateScheduler setting learning rate to 0.0005313952597646568.\n",
      "learning rate: 5.31e-04, weight decay: 2.66e-05\n",
      "Epoch 49/100\n",
      "292/295 [============================>.] - ETA: 0s - loss: 4.9609 - accuracy: 0.0360\n",
      "Epoch 49: val_accuracy did not improve from 0.03344\n",
      "295/295 [==============================] - 4s 14ms/step - loss: 4.9602 - accuracy: 0.0361 - val_loss: 4.9882 - val_accuracy: 0.0331 - lr: 5.3140e-04\n",
      "\n",
      "Epoch 50: LearningRateScheduler setting learning rate to 0.0005157053795390641.\n",
      "learning rate: 5.16e-04, weight decay: 2.58e-05\n",
      "Epoch 50/100\n",
      "292/295 [============================>.] - ETA: 0s - loss: 4.9590 - accuracy: 0.0353\n",
      "Epoch 50: val_accuracy improved from 0.03344 to 0.03398, saving model to models/weights_042323_18_47.h5\n",
      "295/295 [==============================] - 4s 14ms/step - loss: 4.9586 - accuracy: 0.0352 - val_loss: 4.9869 - val_accuracy: 0.0340 - lr: 5.1571e-04\n",
      "\n",
      "Epoch 51: LearningRateScheduler setting learning rate to 0.0005.\n",
      "learning rate: 5.00e-04, weight decay: 2.50e-05\n",
      "Epoch 51/100\n",
      "295/295 [==============================] - ETA: 0s - loss: 4.9570 - accuracy: 0.0358\n",
      "Epoch 51: val_accuracy did not improve from 0.03398\n",
      "295/295 [==============================] - 4s 14ms/step - loss: 4.9570 - accuracy: 0.0358 - val_loss: 4.9857 - val_accuracy: 0.0324 - lr: 5.0000e-04\n",
      "\n",
      "Epoch 52: LearningRateScheduler setting learning rate to 0.00048429462046093585.\n",
      "learning rate: 4.84e-04, weight decay: 2.42e-05\n",
      "Epoch 52/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "293/295 [============================>.] - ETA: 0s - loss: 4.9554 - accuracy: 0.0357\n",
      "Epoch 52: val_accuracy did not improve from 0.03398\n",
      "295/295 [==============================] - 4s 14ms/step - loss: 4.9553 - accuracy: 0.0358 - val_loss: 4.9840 - val_accuracy: 0.0324 - lr: 4.8429e-04\n",
      "\n",
      "Epoch 53: LearningRateScheduler setting learning rate to 0.0004686047402353433.\n",
      "learning rate: 4.69e-04, weight decay: 2.34e-05\n",
      "Epoch 53/100\n",
      "293/295 [============================>.] - ETA: 0s - loss: 4.9538 - accuracy: 0.0364\n",
      "Epoch 53: val_accuracy did not improve from 0.03398\n",
      "295/295 [==============================] - 4s 13ms/step - loss: 4.9540 - accuracy: 0.0363 - val_loss: 4.9834 - val_accuracy: 0.0338 - lr: 4.6860e-04\n",
      "\n",
      "Epoch 54: LearningRateScheduler setting learning rate to 0.00045294584334074284.\n",
      "learning rate: 4.53e-04, weight decay: 2.26e-05\n",
      "Epoch 54/100\n",
      "294/295 [============================>.] - ETA: 0s - loss: 4.9525 - accuracy: 0.0368\n",
      "Epoch 54: val_accuracy did not improve from 0.03398\n",
      "295/295 [==============================] - 4s 13ms/step - loss: 4.9524 - accuracy: 0.0367 - val_loss: 4.9825 - val_accuracy: 0.0336 - lr: 4.5295e-04\n",
      "\n",
      "Epoch 55: LearningRateScheduler setting learning rate to 0.00043733338321784784.\n",
      "learning rate: 4.37e-04, weight decay: 2.19e-05\n",
      "Epoch 55/100\n",
      "293/295 [============================>.] - ETA: 0s - loss: 4.9511 - accuracy: 0.0364\n",
      "Epoch 55: val_accuracy did not improve from 0.03398\n",
      "295/295 [==============================] - 4s 14ms/step - loss: 4.9513 - accuracy: 0.0364 - val_loss: 4.9818 - val_accuracy: 0.0337 - lr: 4.3733e-04\n",
      "\n",
      "Epoch 56: LearningRateScheduler setting learning rate to 0.0004217827674798845.\n",
      "learning rate: 4.22e-04, weight decay: 2.11e-05\n",
      "Epoch 56/100\n",
      "292/295 [============================>.] - ETA: 0s - loss: 4.9503 - accuracy: 0.0373\n",
      "Epoch 56: val_accuracy did not improve from 0.03398\n",
      "295/295 [==============================] - 4s 13ms/step - loss: 4.9500 - accuracy: 0.0373 - val_loss: 4.9805 - val_accuracy: 0.0338 - lr: 4.2178e-04\n",
      "\n",
      "Epoch 57: LearningRateScheduler setting learning rate to 0.0004063093427071376.\n",
      "learning rate: 4.06e-04, weight decay: 2.03e-05\n",
      "Epoch 57/100\n",
      "292/295 [============================>.] - ETA: 0s - loss: 4.9489 - accuracy: 0.0369\n",
      "Epoch 57: val_accuracy improved from 0.03398 to 0.03414, saving model to models/weights_042323_18_47.h5\n",
      "295/295 [==============================] - 4s 13ms/step - loss: 4.9488 - accuracy: 0.0369 - val_loss: 4.9803 - val_accuracy: 0.0341 - lr: 4.0631e-04\n",
      "\n",
      "Epoch 58: LearningRateScheduler setting learning rate to 0.0003909283793017289.\n",
      "learning rate: 3.91e-04, weight decay: 1.95e-05\n",
      "Epoch 58/100\n",
      "294/295 [============================>.] - ETA: 0s - loss: 4.9471 - accuracy: 0.0371\n",
      "Epoch 58: val_accuracy did not improve from 0.03414\n",
      "295/295 [==============================] - 4s 14ms/step - loss: 4.9472 - accuracy: 0.0371 - val_loss: 4.9790 - val_accuracy: 0.0329 - lr: 3.9093e-04\n",
      "\n",
      "Epoch 59: LearningRateScheduler setting learning rate to 0.0003756550564175727.\n",
      "learning rate: 3.76e-04, weight decay: 1.88e-05\n",
      "Epoch 59/100\n",
      "295/295 [==============================] - ETA: 0s - loss: 4.9463 - accuracy: 0.0372\n",
      "Epoch 59: val_accuracy did not improve from 0.03414\n",
      "295/295 [==============================] - 4s 13ms/step - loss: 4.9463 - accuracy: 0.0372 - val_loss: 4.9804 - val_accuracy: 0.0339 - lr: 3.7566e-04\n",
      "\n",
      "Epoch 60: LearningRateScheduler setting learning rate to 0.0003605044469803854.\n",
      "learning rate: 3.61e-04, weight decay: 1.80e-05\n",
      "Epoch 60/100\n",
      "295/295 [==============================] - ETA: 0s - loss: 4.9451 - accuracy: 0.0379\n",
      "Epoch 60: val_accuracy improved from 0.03414 to 0.03473, saving model to models/weights_042323_18_47.h5\n",
      "295/295 [==============================] - 4s 14ms/step - loss: 4.9451 - accuracy: 0.0379 - val_loss: 4.9769 - val_accuracy: 0.0347 - lr: 3.6050e-04\n",
      "\n",
      "Epoch 61: LearningRateScheduler setting learning rate to 0.00034549150281252633.\n",
      "learning rate: 3.45e-04, weight decay: 1.73e-05\n",
      "Epoch 61/100\n",
      "295/295 [==============================] - ETA: 0s - loss: 4.9439 - accuracy: 0.0381\n",
      "Epoch 61: val_accuracy did not improve from 0.03473\n",
      "295/295 [==============================] - 4s 13ms/step - loss: 4.9439 - accuracy: 0.0381 - val_loss: 4.9760 - val_accuracy: 0.0343 - lr: 3.4549e-04\n",
      "\n",
      "Epoch 62: LearningRateScheduler setting learning rate to 0.0003306310398773543.\n",
      "learning rate: 3.31e-04, weight decay: 1.65e-05\n",
      "Epoch 62/100\n",
      "295/295 [==============================] - ETA: 0s - loss: 4.9431 - accuracy: 0.0374\n",
      "Epoch 62: val_accuracy did not improve from 0.03473\n",
      "295/295 [==============================] - 4s 13ms/step - loss: 4.9431 - accuracy: 0.0374 - val_loss: 4.9770 - val_accuracy: 0.0339 - lr: 3.3063e-04\n",
      "\n",
      "Epoch 63: LearningRateScheduler setting learning rate to 0.00031593772365766105.\n",
      "learning rate: 3.16e-04, weight decay: 1.58e-05\n",
      "Epoch 63/100\n",
      "292/295 [============================>.] - ETA: 0s - loss: 4.9426 - accuracy: 0.0379\n",
      "Epoch 63: val_accuracy did not improve from 0.03473\n",
      "295/295 [==============================] - 4s 13ms/step - loss: 4.9420 - accuracy: 0.0378 - val_loss: 4.9748 - val_accuracy: 0.0332 - lr: 3.1594e-04\n",
      "\n",
      "Epoch 64: LearningRateScheduler setting learning rate to 0.00030142605468260977.\n",
      "learning rate: 3.01e-04, weight decay: 1.51e-05\n",
      "Epoch 64/100\n",
      "294/295 [============================>.] - ETA: 0s - loss: 4.9412 - accuracy: 0.0377\n",
      "Epoch 64: val_accuracy improved from 0.03473 to 0.03489, saving model to models/weights_042323_18_47.h5\n",
      "295/295 [==============================] - 4s 14ms/step - loss: 4.9413 - accuracy: 0.0377 - val_loss: 4.9736 - val_accuracy: 0.0349 - lr: 3.0143e-04\n",
      "\n",
      "Epoch 65: LearningRateScheduler setting learning rate to 0.00028711035421746366.\n",
      "learning rate: 2.87e-04, weight decay: 1.44e-05\n",
      "Epoch 65/100\n",
      "294/295 [============================>.] - ETA: 0s - loss: 4.9401 - accuracy: 0.0385\n",
      "Epoch 65: val_accuracy did not improve from 0.03489\n",
      "295/295 [==============================] - 4s 14ms/step - loss: 4.9400 - accuracy: 0.0384 - val_loss: 4.9735 - val_accuracy: 0.0338 - lr: 2.8711e-04\n",
      "\n",
      "Epoch 66: LearningRateScheduler setting learning rate to 0.00027300475013022663.\n",
      "learning rate: 2.73e-04, weight decay: 1.37e-05\n",
      "Epoch 66/100\n",
      "294/295 [============================>.] - ETA: 0s - loss: 4.9393 - accuracy: 0.0383\n",
      "Epoch 66: val_accuracy did not improve from 0.03489\n",
      "295/295 [==============================] - 4s 14ms/step - loss: 4.9395 - accuracy: 0.0383 - val_loss: 4.9745 - val_accuracy: 0.0338 - lr: 2.7300e-04\n",
      "\n",
      "Epoch 67: LearningRateScheduler setting learning rate to 0.0002591231629491423.\n",
      "learning rate: 2.59e-04, weight decay: 1.30e-05\n",
      "Epoch 67/100\n",
      "292/295 [============================>.] - ETA: 0s - loss: 4.9380 - accuracy: 0.0384\n",
      "Epoch 67: val_accuracy did not improve from 0.03489\n",
      "295/295 [==============================] - 4s 14ms/step - loss: 4.9385 - accuracy: 0.0385 - val_loss: 4.9721 - val_accuracy: 0.0347 - lr: 2.5912e-04\n",
      "\n",
      "Epoch 68: LearningRateScheduler setting learning rate to 0.00024547929212481435.\n",
      "learning rate: 2.45e-04, weight decay: 1.23e-05\n",
      "Epoch 68/100\n",
      "293/295 [============================>.] - ETA: 0s - loss: 4.9379 - accuracy: 0.0386\n",
      "Epoch 68: val_accuracy did not improve from 0.03489\n",
      "295/295 [==============================] - 4s 14ms/step - loss: 4.9374 - accuracy: 0.0387 - val_loss: 4.9720 - val_accuracy: 0.0339 - lr: 2.4548e-04\n",
      "\n",
      "Epoch 69: LearningRateScheduler setting learning rate to 0.00023208660251050156.\n",
      "learning rate: 2.32e-04, weight decay: 1.16e-05\n",
      "Epoch 69/100\n",
      "291/295 [============================>.] - ETA: 0s - loss: 4.9366 - accuracy: 0.0386\n",
      "Epoch 69: val_accuracy improved from 0.03489 to 0.03526, saving model to models/weights_042323_18_47.h5\n",
      "295/295 [==============================] - 4s 13ms/step - loss: 4.9367 - accuracy: 0.0385 - val_loss: 4.9717 - val_accuracy: 0.0353 - lr: 2.3209e-04\n",
      "\n",
      "Epoch 70: LearningRateScheduler setting learning rate to 0.0002189583110739348.\n",
      "learning rate: 2.19e-04, weight decay: 1.09e-05\n",
      "Epoch 70/100\n",
      "291/295 [============================>.] - ETA: 0s - loss: 4.9357 - accuracy: 0.0385\n",
      "Epoch 70: val_accuracy improved from 0.03526 to 0.03542, saving model to models/weights_042323_18_47.h5\n",
      "295/295 [==============================] - 4s 13ms/step - loss: 4.9361 - accuracy: 0.0386 - val_loss: 4.9714 - val_accuracy: 0.0354 - lr: 2.1896e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 71: LearningRateScheduler setting learning rate to 0.00020610737385376348.\n",
      "learning rate: 2.06e-04, weight decay: 1.03e-05\n",
      "Epoch 71/100\n",
      "294/295 [============================>.] - ETA: 0s - loss: 4.9356 - accuracy: 0.0388\n",
      "Epoch 71: val_accuracy did not improve from 0.03542\n",
      "295/295 [==============================] - 4s 14ms/step - loss: 4.9356 - accuracy: 0.0387 - val_loss: 4.9701 - val_accuracy: 0.0344 - lr: 2.0611e-04\n",
      "\n",
      "Epoch 72: LearningRateScheduler setting learning rate to 0.00019354647317351188.\n",
      "learning rate: 1.94e-04, weight decay: 9.68e-06\n",
      "Epoch 72/100\n",
      "294/295 [============================>.] - ETA: 0s - loss: 4.9346 - accuracy: 0.0389\n",
      "Epoch 72: val_accuracy did not improve from 0.03542\n",
      "295/295 [==============================] - 4s 13ms/step - loss: 4.9348 - accuracy: 0.0388 - val_loss: 4.9699 - val_accuracy: 0.0346 - lr: 1.9355e-04\n",
      "\n",
      "Epoch 73: LearningRateScheduler setting learning rate to 0.00018128800512565513.\n",
      "learning rate: 1.81e-04, weight decay: 9.06e-06\n",
      "Epoch 73/100\n",
      "291/295 [============================>.] - ETA: 0s - loss: 4.9342 - accuracy: 0.0391\n",
      "Epoch 73: val_accuracy did not improve from 0.03542\n",
      "295/295 [==============================] - 4s 13ms/step - loss: 4.9342 - accuracy: 0.0391 - val_loss: 4.9697 - val_accuracy: 0.0351 - lr: 1.8129e-04\n",
      "\n",
      "Epoch 74: LearningRateScheduler setting learning rate to 0.00016934406733817414.\n",
      "learning rate: 1.69e-04, weight decay: 8.47e-06\n",
      "Epoch 74/100\n",
      "291/295 [============================>.] - ETA: 0s - loss: 4.9332 - accuracy: 0.0391\n",
      "Epoch 74: val_accuracy did not improve from 0.03542\n",
      "295/295 [==============================] - 4s 14ms/step - loss: 4.9335 - accuracy: 0.0391 - val_loss: 4.9697 - val_accuracy: 0.0348 - lr: 1.6934e-04\n",
      "\n",
      "Epoch 75: LearningRateScheduler setting learning rate to 0.00015772644703565563.\n",
      "learning rate: 1.58e-04, weight decay: 7.89e-06\n",
      "Epoch 75/100\n",
      "291/295 [============================>.] - ETA: 0s - loss: 4.9330 - accuracy: 0.0393\n",
      "Epoch 75: val_accuracy did not improve from 0.03542\n",
      "295/295 [==============================] - 4s 13ms/step - loss: 4.9329 - accuracy: 0.0392 - val_loss: 4.9691 - val_accuracy: 0.0348 - lr: 1.5773e-04\n",
      "\n",
      "Epoch 76: LearningRateScheduler setting learning rate to 0.00014644660940672628.\n",
      "learning rate: 1.46e-04, weight decay: 7.32e-06\n",
      "Epoch 76/100\n",
      "294/295 [============================>.] - ETA: 0s - loss: 4.9323 - accuracy: 0.0391\n",
      "Epoch 76: val_accuracy did not improve from 0.03542\n",
      "295/295 [==============================] - 4s 13ms/step - loss: 4.9324 - accuracy: 0.0390 - val_loss: 4.9684 - val_accuracy: 0.0351 - lr: 1.4645e-04\n",
      "\n",
      "Epoch 77: LearningRateScheduler setting learning rate to 0.00013551568628929433.\n",
      "learning rate: 1.36e-04, weight decay: 6.78e-06\n",
      "Epoch 77/100\n",
      "293/295 [============================>.] - ETA: 0s - loss: 4.9322 - accuracy: 0.0396\n",
      "Epoch 77: val_accuracy did not improve from 0.03542\n",
      "295/295 [==============================] - 4s 14ms/step - loss: 4.9319 - accuracy: 0.0395 - val_loss: 4.9685 - val_accuracy: 0.0343 - lr: 1.3552e-04\n",
      "\n",
      "Epoch 78: LearningRateScheduler setting learning rate to 0.0001249444651847702.\n",
      "learning rate: 1.25e-04, weight decay: 6.25e-06\n",
      "Epoch 78/100\n",
      "292/295 [============================>.] - ETA: 0s - loss: 4.9316 - accuracy: 0.0393\n",
      "Epoch 78: val_accuracy did not improve from 0.03542\n",
      "295/295 [==============================] - 4s 13ms/step - loss: 4.9313 - accuracy: 0.0393 - val_loss: 4.9683 - val_accuracy: 0.0344 - lr: 1.2494e-04\n",
      "\n",
      "Epoch 79: LearningRateScheduler setting learning rate to 0.00011474337861210544.\n",
      "learning rate: 1.15e-04, weight decay: 5.74e-06\n",
      "Epoch 79/100\n",
      "294/295 [============================>.] - ETA: 0s - loss: 4.9310 - accuracy: 0.0398\n",
      "Epoch 79: val_accuracy did not improve from 0.03542\n",
      "295/295 [==============================] - 4s 14ms/step - loss: 4.9308 - accuracy: 0.0397 - val_loss: 4.9678 - val_accuracy: 0.0342 - lr: 1.1474e-04\n",
      "\n",
      "Epoch 80: LearningRateScheduler setting learning rate to 0.00010492249381215479.\n",
      "learning rate: 1.05e-04, weight decay: 5.25e-06\n",
      "Epoch 80/100\n",
      "292/295 [============================>.] - ETA: 0s - loss: 4.9307 - accuracy: 0.0393\n",
      "Epoch 80: val_accuracy did not improve from 0.03542\n",
      "295/295 [==============================] - 4s 14ms/step - loss: 4.9306 - accuracy: 0.0392 - val_loss: 4.9676 - val_accuracy: 0.0350 - lr: 1.0492e-04\n",
      "\n",
      "Epoch 81: LearningRateScheduler setting learning rate to 9.549150281252633e-05.\n",
      "learning rate: 9.55e-05, weight decay: 4.77e-06\n",
      "Epoch 81/100\n",
      "294/295 [============================>.] - ETA: 0s - loss: 4.9300 - accuracy: 0.0396\n",
      "Epoch 81: val_accuracy did not improve from 0.03542\n",
      "295/295 [==============================] - 4s 13ms/step - loss: 4.9299 - accuracy: 0.0396 - val_loss: 4.9676 - val_accuracy: 0.0345 - lr: 9.5492e-05\n",
      "\n",
      "Epoch 82: LearningRateScheduler setting learning rate to 8.645971286271903e-05.\n",
      "learning rate: 8.65e-05, weight decay: 4.32e-06\n",
      "Epoch 82/100\n",
      "291/295 [============================>.] - ETA: 0s - loss: 4.9297 - accuracy: 0.0398\n",
      "Epoch 82: val_accuracy did not improve from 0.03542\n",
      "295/295 [==============================] - 4s 13ms/step - loss: 4.9298 - accuracy: 0.0397 - val_loss: 4.9672 - val_accuracy: 0.0350 - lr: 8.6460e-05\n",
      "\n",
      "Epoch 83: LearningRateScheduler setting learning rate to 7.783603724899258e-05.\n",
      "learning rate: 7.78e-05, weight decay: 3.89e-06\n",
      "Epoch 83/100\n",
      "294/295 [============================>.] - ETA: 0s - loss: 4.9293 - accuracy: 0.0399\n",
      "Epoch 83: val_accuracy did not improve from 0.03542\n",
      "295/295 [==============================] - 4s 13ms/step - loss: 4.9293 - accuracy: 0.0400 - val_loss: 4.9670 - val_accuracy: 0.0350 - lr: 7.7836e-05\n",
      "\n",
      "Epoch 84: LearningRateScheduler setting learning rate to 6.962898649802824e-05.\n",
      "learning rate: 6.96e-05, weight decay: 3.48e-06\n",
      "Epoch 84/100\n",
      "293/295 [============================>.] - ETA: 0s - loss: 4.9295 - accuracy: 0.0397\n",
      "Epoch 84: val_accuracy did not improve from 0.03542\n",
      "295/295 [==============================] - 4s 13ms/step - loss: 4.9289 - accuracy: 0.0398 - val_loss: 4.9674 - val_accuracy: 0.0348 - lr: 6.9629e-05\n",
      "\n",
      "Epoch 85: LearningRateScheduler setting learning rate to 6.184665997806832e-05.\n",
      "learning rate: 6.18e-05, weight decay: 3.09e-06\n",
      "Epoch 85/100\n",
      "293/295 [============================>.] - ETA: 0s - loss: 4.9284 - accuracy: 0.0400\n",
      "Epoch 85: val_accuracy did not improve from 0.03542\n",
      "295/295 [==============================] - 4s 14ms/step - loss: 4.9287 - accuracy: 0.0400 - val_loss: 4.9668 - val_accuracy: 0.0348 - lr: 6.1847e-05\n",
      "\n",
      "Epoch 86: LearningRateScheduler setting learning rate to 5.449673790581611e-05.\n",
      "learning rate: 5.45e-05, weight decay: 2.72e-06\n",
      "Epoch 86/100\n",
      "294/295 [============================>.] - ETA: 0s - loss: 4.9283 - accuracy: 0.0401\n",
      "Epoch 86: val_accuracy did not improve from 0.03542\n",
      "295/295 [==============================] - 4s 14ms/step - loss: 4.9284 - accuracy: 0.0400 - val_loss: 4.9666 - val_accuracy: 0.0349 - lr: 5.4497e-05\n",
      "\n",
      "Epoch 87: LearningRateScheduler setting learning rate to 4.758647376699032e-05.\n",
      "learning rate: 4.76e-05, weight decay: 2.38e-06\n",
      "Epoch 87/100\n",
      "292/295 [============================>.] - ETA: 0s - loss: 4.9284 - accuracy: 0.0400\n",
      "Epoch 87: val_accuracy did not improve from 0.03542\n",
      "295/295 [==============================] - 4s 13ms/step - loss: 4.9282 - accuracy: 0.0399 - val_loss: 4.9665 - val_accuracy: 0.0352 - lr: 4.7586e-05\n",
      "\n",
      "Epoch 88: LearningRateScheduler setting learning rate to 4.112268715800943e-05.\n",
      "learning rate: 4.11e-05, weight decay: 2.06e-06\n",
      "Epoch 88/100\n",
      "291/295 [============================>.] - ETA: 0s - loss: 4.9277 - accuracy: 0.0401\n",
      "Epoch 88: val_accuracy did not improve from 0.03542\n",
      "295/295 [==============================] - 4s 13ms/step - loss: 4.9279 - accuracy: 0.0400 - val_loss: 4.9665 - val_accuracy: 0.0348 - lr: 4.1123e-05\n",
      "\n",
      "Epoch 89: LearningRateScheduler setting learning rate to 3.5111757055874326e-05.\n",
      "learning rate: 3.51e-05, weight decay: 1.76e-06\n",
      "Epoch 89/100\n",
      "291/295 [============================>.] - ETA: 0s - loss: 4.9271 - accuracy: 0.0400\n",
      "Epoch 89: val_accuracy did not improve from 0.03542\n",
      "295/295 [==============================] - 4s 13ms/step - loss: 4.9277 - accuracy: 0.0400 - val_loss: 4.9666 - val_accuracy: 0.0353 - lr: 3.5112e-05\n",
      "\n",
      "Epoch 90: LearningRateScheduler setting learning rate to 2.9559615522887274e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate: 2.96e-05, weight decay: 1.48e-06\n",
      "Epoch 90/100\n",
      "293/295 [============================>.] - ETA: 0s - loss: 4.9273 - accuracy: 0.0399\n",
      "Epoch 90: val_accuracy did not improve from 0.03542\n",
      "295/295 [==============================] - 4s 14ms/step - loss: 4.9275 - accuracy: 0.0399 - val_loss: 4.9664 - val_accuracy: 0.0347 - lr: 2.9560e-05\n",
      "\n",
      "Epoch 91: LearningRateScheduler setting learning rate to 2.4471741852423235e-05.\n",
      "learning rate: 2.45e-05, weight decay: 1.22e-06\n",
      "Epoch 91/100\n",
      "293/295 [============================>.] - ETA: 0s - loss: 4.9271 - accuracy: 0.0401\n",
      "Epoch 91: val_accuracy did not improve from 0.03542\n",
      "295/295 [==============================] - 4s 14ms/step - loss: 4.9273 - accuracy: 0.0401 - val_loss: 4.9664 - val_accuracy: 0.0352 - lr: 2.4472e-05\n",
      "\n",
      "Epoch 92: LearningRateScheduler setting learning rate to 1.985315716152847e-05.\n",
      "learning rate: 1.99e-05, weight decay: 9.93e-07\n",
      "Epoch 92/100\n",
      "295/295 [==============================] - ETA: 0s - loss: 4.9272 - accuracy: 0.0399\n",
      "Epoch 92: val_accuracy did not improve from 0.03542\n",
      "295/295 [==============================] - 4s 14ms/step - loss: 4.9272 - accuracy: 0.0399 - val_loss: 4.9663 - val_accuracy: 0.0354 - lr: 1.9853e-05\n",
      "\n",
      "Epoch 93: LearningRateScheduler setting learning rate to 1.5708419435684463e-05.\n",
      "learning rate: 1.57e-05, weight decay: 7.85e-07\n",
      "Epoch 93/100\n",
      "293/295 [============================>.] - ETA: 0s - loss: 4.9273 - accuracy: 0.0406\n",
      "Epoch 93: val_accuracy did not improve from 0.03542\n",
      "295/295 [==============================] - 4s 14ms/step - loss: 4.9270 - accuracy: 0.0405 - val_loss: 4.9663 - val_accuracy: 0.0350 - lr: 1.5708e-05\n",
      "\n",
      "Epoch 94: LearningRateScheduler setting learning rate to 1.2041619030626282e-05.\n",
      "learning rate: 1.20e-05, weight decay: 6.02e-07\n",
      "Epoch 94/100\n",
      "291/295 [============================>.] - ETA: 0s - loss: 4.9265 - accuracy: 0.0401\n",
      "Epoch 94: val_accuracy improved from 0.03542 to 0.03553, saving model to models/weights_042323_18_47.h5\n",
      "295/295 [==============================] - 4s 15ms/step - loss: 4.9269 - accuracy: 0.0401 - val_loss: 4.9663 - val_accuracy: 0.0355 - lr: 1.2042e-05\n",
      "\n",
      "Epoch 95: LearningRateScheduler setting learning rate to 8.856374635655695e-06.\n",
      "learning rate: 8.86e-06, weight decay: 4.43e-07\n",
      "Epoch 95/100\n",
      "295/295 [==============================] - ETA: 0s - loss: 4.9268 - accuracy: 0.0400\n",
      "Epoch 95: val_accuracy did not improve from 0.03553\n",
      "295/295 [==============================] - 4s 14ms/step - loss: 4.9268 - accuracy: 0.0400 - val_loss: 4.9663 - val_accuracy: 0.0350 - lr: 8.8564e-06\n",
      "\n",
      "Epoch 96: LearningRateScheduler setting learning rate to 6.15582970243117e-06.\n",
      "learning rate: 6.16e-06, weight decay: 3.08e-07\n",
      "Epoch 96/100\n",
      "294/295 [============================>.] - ETA: 0s - loss: 4.9267 - accuracy: 0.0402\n",
      "Epoch 96: val_accuracy did not improve from 0.03553\n",
      "295/295 [==============================] - 4s 14ms/step - loss: 4.9267 - accuracy: 0.0402 - val_loss: 4.9663 - val_accuracy: 0.0350 - lr: 6.1558e-06\n",
      "\n",
      "Epoch 97: LearningRateScheduler setting learning rate to 3.942649342761117e-06.\n",
      "learning rate: 3.94e-06, weight decay: 1.97e-07\n",
      "Epoch 97/100\n",
      "293/295 [============================>.] - ETA: 0s - loss: 4.9264 - accuracy: 0.0399\n",
      "Epoch 97: val_accuracy did not improve from 0.03553\n",
      "295/295 [==============================] - 4s 13ms/step - loss: 4.9267 - accuracy: 0.0400 - val_loss: 4.9662 - val_accuracy: 0.0350 - lr: 3.9426e-06\n",
      "\n",
      "Epoch 98: LearningRateScheduler setting learning rate to 2.219017698460002e-06.\n",
      "learning rate: 2.22e-06, weight decay: 1.11e-07\n",
      "Epoch 98/100\n",
      "292/295 [============================>.] - ETA: 0s - loss: 4.9266 - accuracy: 0.0400\n",
      "Epoch 98: val_accuracy did not improve from 0.03553\n",
      "295/295 [==============================] - 4s 13ms/step - loss: 4.9266 - accuracy: 0.0400 - val_loss: 4.9662 - val_accuracy: 0.0352 - lr: 2.2190e-06\n",
      "\n",
      "Epoch 99: LearningRateScheduler setting learning rate to 9.866357858642206e-07.\n",
      "learning rate: 9.87e-07, weight decay: 4.93e-08\n",
      "Epoch 99/100\n",
      "295/295 [==============================] - ETA: 0s - loss: 4.9266 - accuracy: 0.0400\n",
      "Epoch 99: val_accuracy did not improve from 0.03553\n",
      "295/295 [==============================] - 4s 13ms/step - loss: 4.9266 - accuracy: 0.0400 - val_loss: 4.9662 - val_accuracy: 0.0351 - lr: 9.8664e-07\n",
      "\n",
      "Epoch 100: LearningRateScheduler setting learning rate to 2.467198171342e-07.\n",
      "learning rate: 2.47e-07, weight decay: 1.23e-08\n",
      "Epoch 100/100\n",
      "291/295 [============================>.] - ETA: 0s - loss: 4.9265 - accuracy: 0.0400\n",
      "Epoch 100: val_accuracy did not improve from 0.03553\n",
      "295/295 [==============================] - 4s 14ms/step - loss: 4.9266 - accuracy: 0.0400 - val_loss: 4.9662 - val_accuracy: 0.0352 - lr: 2.4672e-07\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Get new fresh model\n",
    "model = build_gcn((20,61,3),250)\n",
    "file_name = 'models/weights_042323_18_47.h5'\n",
    "#model = tf.keras.models.load_model('models/041423_21_02.h5')\n",
    "\n",
    "# Sanity Check\n",
    "model.summary()\n",
    "\n",
    "# Actual Training\n",
    "history = model.fit(\n",
    "        x=x_train,\n",
    "        y=y_train,\n",
    "        epochs=100,\n",
    "        # Only used for validation data since training data is a generator\n",
    "        batch_size=256,\n",
    "        validation_data=(x_test,y_test),\n",
    "        callbacks=[\n",
    "            tf.keras.callbacks.ModelCheckpoint(\n",
    "            file_name,\n",
    "            save_weights_only = True,\n",
    "            save_best_only=True, \n",
    "            monitor=\"val_accuracy\",\n",
    "            mode=\"max\",\n",
    "            verbose = 1),\n",
    "            lr_callback,\n",
    "            WeightDecayCallback(),\n",
    "        ],\n",
    "        verbose = 1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256,)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:256].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>participant_id</th>\n",
       "      <th>sequence_id</th>\n",
       "      <th>sign</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28332</th>\n",
       "      <td>train_landmark_files/34503/2167338153.parquet</td>\n",
       "      <td>34503</td>\n",
       "      <td>2167338153</td>\n",
       "      <td>zipper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85588</th>\n",
       "      <td>train_landmark_files/4718/633430216.parquet</td>\n",
       "      <td>4718</td>\n",
       "      <td>633430216</td>\n",
       "      <td>finger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5325</th>\n",
       "      <td>train_landmark_files/2044/1220910156.parquet</td>\n",
       "      <td>2044</td>\n",
       "      <td>1220910156</td>\n",
       "      <td>nap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17262</th>\n",
       "      <td>train_landmark_files/26734/1712048579.parquet</td>\n",
       "      <td>26734</td>\n",
       "      <td>1712048579</td>\n",
       "      <td>if</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77761</th>\n",
       "      <td>train_landmark_files/37779/4182573073.parquet</td>\n",
       "      <td>37779</td>\n",
       "      <td>4182573073</td>\n",
       "      <td>fine</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path  participant_id  \\\n",
       "28332  train_landmark_files/34503/2167338153.parquet           34503   \n",
       "85588    train_landmark_files/4718/633430216.parquet            4718   \n",
       "5325    train_landmark_files/2044/1220910156.parquet            2044   \n",
       "17262  train_landmark_files/26734/1712048579.parquet           26734   \n",
       "77761  train_landmark_files/37779/4182573073.parquet           37779   \n",
       "\n",
       "       sequence_id    sign  \n",
       "28332   2167338153  zipper  \n",
       "85588    633430216  finger  \n",
       "5325    1220910156     nap  \n",
       "17262   1712048579      if  \n",
       "77761   4182573073    fine  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('models/042223_20_52.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['prediction'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'yes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [46], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index,row \u001b[38;5;129;01min\u001b[39;00m test_df\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m      2\u001b[0m     label \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msign\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 3\u001b[0m     left_hand,lips \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_datagen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpath\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(np\u001b[38;5;241m.\u001b[39mexpand_dims(np\u001b[38;5;241m.\u001b[39mconcatenate([left_hand,lips],axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m),axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m),verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      5\u001b[0m     test_df\u001b[38;5;241m.\u001b[39mloc[index,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m decoder(np\u001b[38;5;241m.\u001b[39margmax(pred[\u001b[38;5;241m0\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "Cell \u001b[0;32mIn [7], line 30\u001b[0m, in \u001b[0;36mCustomData.load_video\u001b[0;34m(self, video_path)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_video\u001b[39m(\u001b[38;5;28mself\u001b[39m,video_path):\n\u001b[0;32m---> 30\u001b[0m     video_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpyarrow\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m#video_df.dropna(inplace=True)\u001b[39;00m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m video_df[video_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft_hand\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misna()\u001b[38;5;241m.\u001b[39mmean() \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \\\n\u001b[1;32m     34\u001b[0m   video_df[video_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mright_hand\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misna()\u001b[38;5;241m.\u001b[39mmean():\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parquet.py:503\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, **kwargs)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;124;03mLoad a parquet object from the file path, returning a DataFrame.\u001b[39;00m\n\u001b[1;32m    458\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;124;03mDataFrame\u001b[39;00m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    501\u001b[0m impl \u001b[38;5;241m=\u001b[39m get_engine(engine)\n\u001b[0;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parquet.py:244\u001b[0m, in \u001b[0;36mPyArrowImpl.read\u001b[0;34m(self, path, columns, use_nullable_dtypes, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    242\u001b[0m     to_pandas_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit_blocks\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m--> 244\u001b[0m path_or_handle, handles, kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilesystem\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43m_get_path_or_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfilesystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    251\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mparquet\u001b[38;5;241m.\u001b[39mread_table(\n\u001b[1;32m    252\u001b[0m         path_or_handle, columns\u001b[38;5;241m=\u001b[39mcolumns, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    253\u001b[0m     )\u001b[38;5;241m.\u001b[39mto_pandas(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mto_pandas_kwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parquet.py:102\u001b[0m, in \u001b[0;36m_get_path_or_handle\u001b[0;34m(path, fs, storage_options, mode, is_dir)\u001b[0m\n\u001b[1;32m     92\u001b[0m handles \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m fs\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dir\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;66;03m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;66;03m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m     handles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m     fs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    106\u001b[0m     path_or_handle \u001b[38;5;241m=\u001b[39m handles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/common.py:865\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    857\u001b[0m             handle,\n\u001b[1;32m    858\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    861\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    862\u001b[0m         )\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    866\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[1;32m    868\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'yes'"
     ]
    }
   ],
   "source": [
    "for index,row in test_df.iterrows():\n",
    "    label = row['sign']\n",
    "    left_hand,lips = train_datagen.load_video(row['path'])\n",
    "    pred = model.predict(np.expand_dims(np.concatenate([left_hand,lips],axis=-2),axis=0),verbose=0)\n",
    "    test_df.loc[index,'prediction'] = decoder(np.argmax(pred[0], axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_df['path'] = complete_df['path'].str.replace('/kaggle/input/asl-signs/','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7631244707874683"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(seg_df['sign_x'] == seg_df['prediction'])/(len(seg_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = test_df.merge(complete_df,on=['path'],how='outer',indicator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94477"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_df = combined[combined['_merge']=='both']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         zipper\n",
       "1         finger\n",
       "2            nap\n",
       "3             if\n",
       "4           fine\n",
       "          ...   \n",
       "18891    fireman\n",
       "18892       home\n",
       "18893      clean\n",
       "18894      taste\n",
       "18895       loud\n",
       "Name: sign_x, Length: 18896, dtype: object"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seg_df['sign_x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37779    299\n",
       "30680    196\n",
       "25571    184\n",
       "18796    177\n",
       "53618    162\n",
       "2044     138\n",
       "28656    125\n",
       "16069    111\n",
       "36257    103\n",
       "37055     94\n",
       "4718      93\n",
       "34503     81\n",
       "27610     74\n",
       "62590     72\n",
       "29302     72\n",
       "22343     72\n",
       "55372     69\n",
       "26734     66\n",
       "49445     59\n",
       "32319     50\n",
       "61333     36\n",
       "Name: participant_id_x, dtype: int64"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seg_df[seg_df['total_frames']==6]['participant_id_x'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6     1128\n",
       "16     103\n",
       "8      102\n",
       "7       96\n",
       "15      90\n",
       "13      89\n",
       "9       87\n",
       "14      83\n",
       "12      82\n",
       "17      81\n",
       "Name: total_frames, dtype: int64"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seg_df[seg_df['sign_x'] != seg_df['prediction']]['total_frames'].value_counts()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_file(file_path):\n",
    "    \"\"\"Read a JSON file and parse it into a Python object.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the JSON file to read.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary object representing the JSON data.\n",
    "        \n",
    "    Raises:\n",
    "        FileNotFoundError: If the specified file path does not exist.\n",
    "        ValueError: If the specified file path does not contain valid JSON data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Open the file and load the JSON data into a Python object\n",
    "        with open(file_path, 'r') as file:\n",
    "            json_data = json.load(file)\n",
    "        return json_data\n",
    "    except FileNotFoundError:\n",
    "        # Raise an error if the file path does not exist\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    except ValueError:\n",
    "        # Raise an error if the file does not contain valid JSON data\n",
    "        raise ValueError(f\"Invalid JSON data in file: {file_path}\")\n",
    "p2s_map = {v:k for k,v in read_json_file(\"sign_to_prediction_index_map.json\").items()}\n",
    "encoder = lambda x: s2p_map.get(x.lower())\n",
    "decoder = lambda x: p2s_map.get(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m preds \u001b[38;5;241m=\u001b[39m [decoder(np\u001b[38;5;241m.\u001b[39margmax(i, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[43mpred\u001b[49m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pred' is not defined"
     ]
    }
   ],
   "source": [
    "preds = [decoder(np.argmax(i, axis=-1)) for i in pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [decoder(i) for i in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7688356164383562\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "for i , j in zip(preds,labels):\n",
    "    if i == j:\n",
    "        cnt+=1\n",
    "print(cnt/len(preds)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
