{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.008117,
     "end_time": "2023-03-02T08:44:33.967124",
     "exception": false,
     "start_time": "2023-03-02T08:44:33.959007",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Isolated Sign Language Recognition with STGCN\n",
    "\n",
    "In this notebook, I will create Sign Language Recognition model using STGCN. To build an efficient training pipeline, I will use TFRecord Dataset from https://www.kaggle.com/datasets/lonnieqin/islr-12-time-steps-tfrecords created by notebook https://www.kaggle.com/code/lonnieqin/islr-create-tfrecord for training.\n",
    "The ST-GCN model archetecture was adapated from https://github.com/kdkalvik/ST-GCN\n",
    "It will take about 1 hour to finish runing this notebook using GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.006539,
     "end_time": "2023-03-02T08:44:33.994281",
     "exception": false,
     "start_time": "2023-03-02T08:44:33.987742",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-28T10:00:12.442535Z",
     "iopub.status.busy": "2023-04-28T10:00:12.442080Z",
     "iopub.status.idle": "2023-04-28T10:00:12.470153Z",
     "shell.execute_reply": "2023-04-28T10:00:12.469255Z",
     "shell.execute_reply.started": "2023-04-28T10:00:12.442503Z"
    },
    "papermill": {
     "duration": 0.022144,
     "end_time": "2023-03-02T08:44:34.023146",
     "exception": false,
     "start_time": "2023-03-02T08:44:34.001002",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    data_path = \"\"\n",
    "    tf_record_path = \"/kaggle/input/islr-12-time-steps-tfrecords/\"\n",
    "    sequence_length = 12\n",
    "    rows_per_frame = 543\n",
    "    is_training = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.006464,
     "end_time": "2023-03-02T08:44:34.036262",
     "exception": false,
     "start_time": "2023-03-02T08:44:34.029798",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-28T10:00:12.472531Z",
     "iopub.status.busy": "2023-04-28T10:00:12.472164Z",
     "iopub.status.idle": "2023-04-28T10:00:21.151336Z",
     "shell.execute_reply": "2023-04-28T10:00:21.150218Z",
     "shell.execute_reply.started": "2023-04-28T10:00:12.472493Z"
    },
    "papermill": {
     "duration": 7.496016,
     "end_time": "2023-03-02T08:44:41.539085",
     "exception": false,
     "start_time": "2023-03-02T08:44:34.043069",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import math\n",
    "np.random.seed(16)\n",
    "tf.random.set_seed(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.006496,
     "end_time": "2023-03-02T08:44:41.597734",
     "exception": false,
     "start_time": "2023-03-02T08:44:41.591238",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-28T10:00:28.032382Z",
     "iopub.status.busy": "2023-04-28T10:00:28.031796Z",
     "iopub.status.idle": "2023-04-28T10:00:28.042057Z",
     "shell.execute_reply": "2023-04-28T10:00:28.040895Z",
     "shell.execute_reply.started": "2023-04-28T10:00:28.032346Z"
    },
    "papermill": {
     "duration": 0.017828,
     "end_time": "2023-03-02T08:44:41.622261",
     "exception": false,
     "start_time": "2023-03-02T08:44:41.604433",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ROWS_PER_FRAME = 543  # number of landmarks per frame\n",
    "\n",
    "def load_relevant_data_subset_with_imputation(pq_path):\n",
    "    data_columns = ['x', 'y']\n",
    "    data = pd.read_parquet(pq_path, columns=data_columns)\n",
    "    data.replace(np.nan, 0, inplace=True)\n",
    "    n_frames = int(len(data) / ROWS_PER_FRAME)\n",
    "    data = data.values.reshape(n_frames, ROWS_PER_FRAME, len(data_columns))\n",
    "    return data.astype(np.float16)\n",
    "\n",
    "def load_relevant_data_subset(pq_path):\n",
    "    data_columns = ['x', 'y']\n",
    "    data = pd.read_parquet(pq_path, columns=data_columns)\n",
    "    n_frames = int(len(data) / ROWS_PER_FRAME)\n",
    "    data = data.values.reshape(n_frames, ROWS_PER_FRAME, len(data_columns))\n",
    "    return data.astype(np.float32)\n",
    "\n",
    "def read_dict(file_path):\n",
    "    path = os.path.expanduser(file_path)\n",
    "    with open(path, \"r\") as f:\n",
    "        dic = json.load(f)\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.006542,
     "end_time": "2023-03-02T08:44:41.635478",
     "exception": false,
     "start_time": "2023-03-02T08:44:41.628936",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-28T10:00:28.576086Z",
     "iopub.status.busy": "2023-04-28T10:00:28.575721Z",
     "iopub.status.idle": "2023-04-28T10:00:28.762511Z",
     "shell.execute_reply": "2023-04-28T10:00:28.761424Z",
     "shell.execute_reply.started": "2023-04-28T10:00:28.576053Z"
    },
    "papermill": {
     "duration": 0.206559,
     "end_time": "2023-03-02T08:44:41.848795",
     "exception": false,
     "start_time": "2023-03-02T08:44:41.642236",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>participant_id</th>\n",
       "      <th>sequence_id</th>\n",
       "      <th>sign</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_landmark_files/26734/1000035562.parquet</td>\n",
       "      <td>26734</td>\n",
       "      <td>1000035562</td>\n",
       "      <td>blow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_landmark_files/28656/1000106739.parquet</td>\n",
       "      <td>28656</td>\n",
       "      <td>1000106739</td>\n",
       "      <td>wait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_landmark_files/16069/100015657.parquet</td>\n",
       "      <td>16069</td>\n",
       "      <td>100015657</td>\n",
       "      <td>cloud</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_landmark_files/25571/1000210073.parquet</td>\n",
       "      <td>25571</td>\n",
       "      <td>1000210073</td>\n",
       "      <td>bird</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_landmark_files/62590/1000240708.parquet</td>\n",
       "      <td>62590</td>\n",
       "      <td>1000240708</td>\n",
       "      <td>owie</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            path  participant_id  sequence_id  \\\n",
       "0  train_landmark_files/26734/1000035562.parquet           26734   1000035562   \n",
       "1  train_landmark_files/28656/1000106739.parquet           28656   1000106739   \n",
       "2   train_landmark_files/16069/100015657.parquet           16069    100015657   \n",
       "3  train_landmark_files/25571/1000210073.parquet           25571   1000210073   \n",
       "4  train_landmark_files/62590/1000240708.parquet           62590   1000240708   \n",
       "\n",
       "    sign  \n",
       "0   blow  \n",
       "1   wait  \n",
       "2  cloud  \n",
       "3   bird  \n",
       "4   owie  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(f\"{CFG.data_path}train.csv\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.007434,
     "end_time": "2023-03-02T08:44:41.8635",
     "exception": false,
     "start_time": "2023-03-02T08:44:41.856066",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "There are 21 participants. Each of them created about 3000 to 5000 training records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-28T10:00:21.429554Z",
     "iopub.status.busy": "2023-04-28T10:00:21.428647Z",
     "iopub.status.idle": "2023-04-28T10:00:21.441674Z",
     "shell.execute_reply": "2023-04-28T10:00:21.440628Z",
     "shell.execute_reply.started": "2023-04-28T10:00:21.429504Z"
    },
    "papermill": {
     "duration": 0.024082,
     "end_time": "2023-03-02T08:44:41.894435",
     "exception": false,
     "start_time": "2023-03-02T08:44:41.870353",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.participant_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-28T10:00:21.443808Z",
     "iopub.status.busy": "2023-04-28T10:00:21.443178Z",
     "iopub.status.idle": "2023-04-28T10:00:21.780692Z",
     "shell.execute_reply": "2023-04-28T10:00:21.779677Z",
     "shell.execute_reply.started": "2023-04-28T10:00:21.443772Z"
    },
    "papermill": {
     "duration": 0.339293,
     "end_time": "2023-03-02T08:44:42.240816",
     "exception": false,
     "start_time": "2023-03-02T08:44:41.901523",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot: >"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.participant_id.value_counts().plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.007382,
     "end_time": "2023-03-02T08:44:42.255978",
     "exception": false,
     "start_time": "2023-03-02T08:44:42.248596",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "There are 94477 training samples in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-28T10:00:33.098748Z",
     "iopub.status.busy": "2023-04-28T10:00:33.098364Z",
     "iopub.status.idle": "2023-04-28T10:00:33.105587Z",
     "shell.execute_reply": "2023-04-28T10:00:33.104499Z",
     "shell.execute_reply.started": "2023-04-28T10:00:33.098691Z"
    },
    "papermill": {
     "duration": 0.016757,
     "end_time": "2023-03-02T08:44:42.280157",
     "exception": false,
     "start_time": "2023-03-02T08:44:42.2634",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94477"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.007305,
     "end_time": "2023-03-02T08:44:42.294963",
     "exception": false,
     "start_time": "2023-03-02T08:44:42.287658",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "There are 250 kinds of sign languages that we need to make prediction on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-28T10:00:34.919532Z",
     "iopub.status.busy": "2023-04-28T10:00:34.918776Z",
     "iopub.status.idle": "2023-04-28T10:00:34.974299Z",
     "shell.execute_reply": "2023-04-28T10:00:34.973172Z",
     "shell.execute_reply.started": "2023-04-28T10:00:34.919491Z"
    },
    "papermill": {
     "duration": 0.059155,
     "end_time": "2023-03-02T08:44:42.361615",
     "exception": false,
     "start_time": "2023-03-02T08:44:42.30246",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'TV': 0, 'after': 1, 'airplane': 2, 'all': 3, 'alligator': 4, 'animal': 5, 'another': 6, 'any': 7, 'apple': 8, 'arm': 9, 'aunt': 10, 'awake': 11, 'backyard': 12, 'bad': 13, 'balloon': 14, 'bath': 15, 'because': 16, 'bed': 17, 'bedroom': 18, 'bee': 19, 'before': 20, 'beside': 21, 'better': 22, 'bird': 23, 'black': 24, 'blow': 25, 'blue': 26, 'boat': 27, 'book': 28, 'boy': 29, 'brother': 30, 'brown': 31, 'bug': 32, 'bye': 33, 'callonphone': 34, 'can': 35, 'car': 36, 'carrot': 37, 'cat': 38, 'cereal': 39, 'chair': 40, 'cheek': 41, 'child': 42, 'chin': 43, 'chocolate': 44, 'clean': 45, 'close': 46, 'closet': 47, 'cloud': 48, 'clown': 49, 'cow': 50, 'cowboy': 51, 'cry': 52, 'cut': 53, 'cute': 54, 'dad': 55, 'dance': 56, 'dirty': 57, 'dog': 58, 'doll': 59, 'donkey': 60, 'down': 61, 'drawer': 62, 'drink': 63, 'drop': 64, 'dry': 65, 'dryer': 66, 'duck': 67, 'ear': 68, 'elephant': 69, 'empty': 70, 'every': 71, 'eye': 72, 'face': 73, 'fall': 74, 'farm': 75, 'fast': 76, 'feet': 77, 'find': 78, 'fine': 79, 'finger': 80, 'finish': 81, 'fireman': 82, 'first': 83, 'fish': 84, 'flag': 85, 'flower': 86, 'food': 87, 'for': 88, 'frenchfries': 89, 'frog': 90, 'garbage': 91, 'gift': 92, 'giraffe': 93, 'girl': 94, 'give': 95, 'glasswindow': 96, 'go': 97, 'goose': 98, 'grandma': 99, 'grandpa': 100, 'grass': 101, 'green': 102, 'gum': 103, 'hair': 104, 'happy': 105, 'hat': 106, 'hate': 107, 'have': 108, 'haveto': 109, 'head': 110, 'hear': 111, 'helicopter': 112, 'hello': 113, 'hen': 114, 'hesheit': 115, 'hide': 116, 'high': 117, 'home': 118, 'horse': 119, 'hot': 120, 'hungry': 121, 'icecream': 122, 'if': 123, 'into': 124, 'jacket': 125, 'jeans': 126, 'jump': 127, 'kiss': 128, 'kitty': 129, 'lamp': 130, 'later': 131, 'like': 132, 'lion': 133, 'lips': 134, 'listen': 135, 'look': 136, 'loud': 137, 'mad': 138, 'make': 139, 'man': 140, 'many': 141, 'milk': 142, 'minemy': 143, 'mitten': 144, 'mom': 145, 'moon': 146, 'morning': 147, 'mouse': 148, 'mouth': 149, 'nap': 150, 'napkin': 151, 'night': 152, 'no': 153, 'noisy': 154, 'nose': 155, 'not': 156, 'now': 157, 'nuts': 158, 'old': 159, 'on': 160, 'open': 161, 'orange': 162, 'outside': 163, 'owie': 164, 'owl': 165, 'pajamas': 166, 'pen': 167, 'pencil': 168, 'penny': 169, 'person': 170, 'pig': 171, 'pizza': 172, 'please': 173, 'police': 174, 'pool': 175, 'potty': 176, 'pretend': 177, 'pretty': 178, 'puppy': 179, 'puzzle': 180, 'quiet': 181, 'radio': 182, 'rain': 183, 'read': 184, 'red': 185, 'refrigerator': 186, 'ride': 187, 'room': 188, 'sad': 189, 'same': 190, 'say': 191, 'scissors': 192, 'see': 193, 'shhh': 194, 'shirt': 195, 'shoe': 196, 'shower': 197, 'sick': 198, 'sleep': 199, 'sleepy': 200, 'smile': 201, 'snack': 202, 'snow': 203, 'stairs': 204, 'stay': 205, 'sticky': 206, 'store': 207, 'story': 208, 'stuck': 209, 'sun': 210, 'table': 211, 'talk': 212, 'taste': 213, 'thankyou': 214, 'that': 215, 'there': 216, 'think': 217, 'thirsty': 218, 'tiger': 219, 'time': 220, 'tomorrow': 221, 'tongue': 222, 'tooth': 223, 'toothbrush': 224, 'touch': 225, 'toy': 226, 'tree': 227, 'uncle': 228, 'underwear': 229, 'up': 230, 'vacuum': 231, 'wait': 232, 'wake': 233, 'water': 234, 'wet': 235, 'weus': 236, 'where': 237, 'white': 238, 'who': 239, 'why': 240, 'will': 241, 'wolf': 242, 'yellow': 243, 'yes': 244, 'yesterday': 245, 'yourself': 246, 'yucky': 247, 'zebra': 248, 'zipper': 249}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>participant_id</th>\n",
       "      <th>sequence_id</th>\n",
       "      <th>sign</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_landmark_files/26734/1000035562.parquet</td>\n",
       "      <td>26734</td>\n",
       "      <td>1000035562</td>\n",
       "      <td>blow</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_landmark_files/28656/1000106739.parquet</td>\n",
       "      <td>28656</td>\n",
       "      <td>1000106739</td>\n",
       "      <td>wait</td>\n",
       "      <td>232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_landmark_files/16069/100015657.parquet</td>\n",
       "      <td>16069</td>\n",
       "      <td>100015657</td>\n",
       "      <td>cloud</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_landmark_files/25571/1000210073.parquet</td>\n",
       "      <td>25571</td>\n",
       "      <td>1000210073</td>\n",
       "      <td>bird</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_landmark_files/62590/1000240708.parquet</td>\n",
       "      <td>62590</td>\n",
       "      <td>1000240708</td>\n",
       "      <td>owie</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            path  participant_id  sequence_id  \\\n",
       "0  train_landmark_files/26734/1000035562.parquet           26734   1000035562   \n",
       "1  train_landmark_files/28656/1000106739.parquet           28656   1000106739   \n",
       "2   train_landmark_files/16069/100015657.parquet           16069    100015657   \n",
       "3  train_landmark_files/25571/1000210073.parquet           25571   1000210073   \n",
       "4  train_landmark_files/62590/1000240708.parquet           62590   1000240708   \n",
       "\n",
       "    sign  label  \n",
       "0   blow     25  \n",
       "1   wait    232  \n",
       "2  cloud     48  \n",
       "3   bird     23  \n",
       "4   owie    164  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_index = read_dict(f\"{CFG.data_path}sign_to_prediction_index_map.json\")\n",
    "index_label = dict([(label_index[key], key) for key in label_index])\n",
    "print(label_index)\n",
    "train[\"label\"] = train[\"sign\"].map(lambda sign: label_index[sign])\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.007629,
     "end_time": "2023-03-02T08:44:42.377",
     "exception": false,
     "start_time": "2023-03-02T08:44:42.369371",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Create Tensorflow Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-28T10:00:37.735320Z",
     "iopub.status.busy": "2023-04-28T10:00:37.734393Z",
     "iopub.status.idle": "2023-04-28T10:00:37.745983Z",
     "shell.execute_reply": "2023-04-28T10:00:37.744961Z",
     "shell.execute_reply.started": "2023-04-28T10:00:37.735279Z"
    },
    "papermill": {
     "duration": 0.019748,
     "end_time": "2023-03-02T08:44:42.404541",
     "exception": false,
     "start_time": "2023-03-02T08:44:42.384793",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def decode_function(record_bytes):\n",
    "    return tf.io.parse_single_example(\n",
    "          # Data\n",
    "          record_bytes,\n",
    "          # Schema\n",
    "          {\n",
    "              \"feature\": tf.io.FixedLenFeature([12 * 543 * 3], dtype=tf.float32),\n",
    "              \"label\": tf.io.FixedLenFeature([], dtype=tf.int64)\n",
    "          }\n",
    "      )\n",
    "def preprocess(item):\n",
    "    features = item[\"feature\"]\n",
    "#     features = tf.reshape(features, (1,CFG.sequence_length, 543,3))\n",
    "    features=tf.reshape(features, (1,12, 543, 3))\n",
    "#         \"face\"       : np.arange(0, 468),\n",
    "#     \"left_hand\"  : np.arange(468, 489),\n",
    "#     \"pose\"       : np.arange(489, 522),\n",
    "#     \"right_hand\" : np.arange(522, 543),\n",
    "    features=tf.transpose(features, perm=[3, 1, 2, 0])\n",
    "    features1=features[:,-5:,468:489,:]\n",
    "    features2=features[:,-5:,522:543,:]\n",
    "    features=tf.concat([features1, features2],2)\n",
    "    print(features.shape)\n",
    "    return features, item[\"label\"]         \n",
    "def make_dataset(file_paths, batch_size=128, mode=\"train\"):\n",
    "    ds = tf.data.TFRecordDataset(file_paths)\n",
    "    ds = ds.map(decode_function)\n",
    "    ds = ds.map(preprocess)\n",
    "    options = tf.data.Options()\n",
    "    if mode == \"train\":\n",
    "        ds = ds.shuffle(1024)\n",
    "        options.experimental_deterministic = False\n",
    "    ds = ds.batch(batch_size, drop_remainder=True)\n",
    "    ds = ds.with_options(options) \n",
    "    ds = ds.cache().prefetch(tf.data.AUTOTUNE)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.load('X_train_20x61_left.npy')\n",
    "y_train = np.load('y_train_20x61_left.npy')\n",
    "x_test = np.load('X_test_20x61_left.npy')\n",
    "y_test = np.load('y_test_20x61_left.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train[:,:,:21,:2]\n",
    "x_test = x_test[:,:,:21,:2]\n",
    "\n",
    "x_train = np.transpose(x_train,(0, 3,1, 2))\n",
    "x_test = np.transpose(x_test,(0,3, 1, 2))\n",
    "x_train = np.expand_dims(x_train,axis=-1)\n",
    "x_test = np.expand_dims(x_test,axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-28T10:00:39.323274Z",
     "iopub.status.busy": "2023-04-28T10:00:39.322502Z",
     "iopub.status.idle": "2023-04-28T10:00:42.286313Z",
     "shell.execute_reply": "2023-04-28T10:00:42.285220Z",
     "shell.execute_reply.started": "2023-04-28T10:00:39.323222Z"
    },
    "papermill": {
     "duration": 0.017938,
     "end_time": "2023-03-02T08:44:42.43015",
     "exception": false,
     "start_time": "2023-03-02T08:44:42.412212",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "unique_ids = np.array(sorted(train.participant_id.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-28T10:01:29.918426Z",
     "iopub.status.busy": "2023-04-28T10:01:29.917624Z",
     "iopub.status.idle": "2023-04-28T10:01:29.928310Z",
     "shell.execute_reply": "2023-04-28T10:01:29.926731Z",
     "shell.execute_reply.started": "2023-04-28T10:01:29.918383Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def edge2mat(link, num_node):\n",
    "    A = np.zeros((num_node, num_node))\n",
    "    for i, j in link:\n",
    "        A[j, i] = 1\n",
    "    return A\n",
    "\n",
    "\n",
    "def normalize_digraph(A):  # 除以每列的和\n",
    "    Dl = np.sum(A, 0)\n",
    "    h, w = A.shape\n",
    "    Dn = np.zeros((w, w))\n",
    "    for i in range(w):\n",
    "        if Dl[i] > 0:\n",
    "            Dn[i, i] = Dl[i] ** (-1)\n",
    "    AD = np.dot(A, Dn)\n",
    "    return AD\n",
    "\n",
    "\n",
    "def get_spatial_graph(num_node, self_link, inward, outward):\n",
    "    I = edge2mat(self_link, num_node)\n",
    "    In = normalize_digraph(edge2mat(inward, num_node))\n",
    "    Out = normalize_digraph(edge2mat(outward, num_node))\n",
    "    A = np.stack((I, In, Out))\n",
    "    return A\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![handlandmark](https://developers.google.com/static/mediapipe/images/solutions/hand-landmarks.png) create the node graph for hand landmarks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-28T10:01:35.614777Z",
     "iopub.status.busy": "2023-04-28T10:01:35.614391Z",
     "iopub.status.idle": "2023-04-28T10:01:35.626798Z",
     "shell.execute_reply": "2023-04-28T10:01:35.624837Z",
     "shell.execute_reply.started": "2023-04-28T10:01:35.614740Z"
    }
   },
   "outputs": [],
   "source": [
    "num_node = 21\n",
    "self_link = [(i, i) for i in range(num_node)]\n",
    "inward_ori_index = [(1, 2), (2, 3), (3, 4), (4, 5), (1, 6), (6, 7), (7, 8),\n",
    "                    (8, 9), (6, 10), (10, 11), (11, 12), (12, 13), (10, 14),\n",
    "                    (14, 15), (15, 16), (16, 17), (14, 18), (18, 19), (19, 20),\n",
    "                    (20, 21), (18, 1)]\n",
    "inward_ori_index2=[(1+21, 2+21), (2+21, 3+21), (3+21, 4+21), (4+21, 5+21), (1+21, 6), \n",
    "                   (6+21, 7+21), (7+21, 8+21), (8+21, 9+21), (6+21, 10+21), \n",
    "                    (10+21, 11+21), (11+21, 12+21), (12+21, 13+21), (10+21, 14+21),\n",
    "                    (14+21, 15+21), (15+21, 16+21), (16+21, 17+21), (14+21, 18+21), (18+21, 19+21), (19+21, 20+21),\n",
    "                    (20+21, 21+21), (18+21, 1+21)]\n",
    "#inward_ori_index.extend(inward_ori_index2)\n",
    "inward = [(i - 1, j - 1) for (i, j) in inward_ori_index]\n",
    "outward = [(j, i) for (i, j) in inward]\n",
    "neighbor = inward + outward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(outward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-28T10:02:02.720112Z",
     "iopub.status.busy": "2023-04-28T10:02:02.719314Z",
     "iopub.status.idle": "2023-04-28T10:02:02.726845Z",
     "shell.execute_reply": "2023-04-28T10:02:02.725718Z",
     "shell.execute_reply.started": "2023-04-28T10:02:02.720072Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class Graph:\n",
    "    def __init__(self, labeling_mode='spatial'):\n",
    "        self.A = self.get_adjacency_matrix(labeling_mode)\n",
    "        self.num_node = num_node\n",
    "        self.self_link = self_link\n",
    "        self.inward = inward\n",
    "        self.outward = outward\n",
    "        self.neighbor = neighbor\n",
    "\n",
    "    def get_adjacency_matrix(self, labeling_mode=None):\n",
    "        if labeling_mode is None:\n",
    "            return self.A\n",
    "        if labeling_mode == 'spatial':\n",
    "            A = get_spatial_graph(num_node, self_link, inward, outward)\n",
    "        else:\n",
    "            raise ValueError()\n",
    "        return A\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.007677,
     "end_time": "2023-03-02T08:44:42.445922",
     "exception": false,
     "start_time": "2023-03-02T08:44:42.438245",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-28T10:31:59.243333Z",
     "iopub.status.busy": "2023-04-28T10:31:59.242459Z",
     "iopub.status.idle": "2023-04-28T10:31:59.271826Z",
     "shell.execute_reply": "2023-04-28T10:31:59.270679Z",
     "shell.execute_reply.started": "2023-04-28T10:31:59.243271Z"
    }
   },
   "outputs": [],
   "source": [
    "REGULARIZER = tf.keras.regularizers.l2(l=0.001)\n",
    "INITIALIZER = tf.keras.initializers.VarianceScaling(scale=2.,\n",
    "                                                    mode=\"fan_out\",\n",
    "                                                    distribution=\"truncated_normal\")\n",
    "class SGCN(tf.keras.Model):\n",
    "    def __init__(self, filters, kernel_size=2):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.conv = tf.keras.layers.Conv2D(filters*kernel_size,\n",
    "                                           kernel_size=1,\n",
    "                                           padding='same',\n",
    "                                           kernel_initializer=INITIALIZER,\n",
    "                                           data_format='channels_first',\n",
    "                                           kernel_regularizer=REGULARIZER)\n",
    "\n",
    "    # N, C, T, V\n",
    "    def call(self, x, A, training):\n",
    "        x = self.conv(x)\n",
    "\n",
    "        N = tf.shape(x)[0]\n",
    "        C = tf.shape(x)[1]\n",
    "        T = tf.shape(x)[2]\n",
    "        V = tf.shape(x)[3]\n",
    "\n",
    "        x = tf.reshape(x, [N, self.kernel_size, C//self.kernel_size, T, V])\n",
    "        x = tf.einsum('nkctv,kvw->nctw', x, A)\n",
    "        return x, A\n",
    "\n",
    "\n",
    "\"\"\"Applies a spatial temporal graph convolution over an input graph sequence.\n",
    "    Args:\n",
    "        filters (int): Number of channels produced by the convolution\n",
    "        kernel_size (tuple): Size of the temporal convolving kernel and graph convolving kernel\n",
    "        stride (int, optional): Stride of the temporal convolution. Default: 1\n",
    "        activation (activation function/name, optional): activation function to use\n",
    "        residual (bool, optional): If ``True``, applies a residual mechanism. Default: ``True``\n",
    "        downsample (bool, optional): If ``True``, applies a downsampling residual mechanism. Default: ``True``\n",
    "                                     the value is used only when residual is ``True``\n",
    "    Shape:\n",
    "        - Input[0]: Input graph sequence in :math:`(N, in_channels, T_{in}, V)` format\n",
    "        - Input[1]: Input graph adjacency matrix in :math:`(K, V, V)` format\n",
    "        - Output[0]: Outpu graph sequence in :math:`(N, out_channels, T_{out}, V)` format\n",
    "        - Output[1]: Graph adjacency matrix for output data in :math:`(K, V, V)` format\n",
    "        where\n",
    "            :math:`N` is a batch size,\n",
    "            :math:`K` is the spatial kernel size, as :math:`K == kernel_size[1]`,\n",
    "            :math:`T_{in}/T_{out}` is a length of input/output sequence,\n",
    "            :math:`V` is the number of graph nodes.\n",
    "\"\"\"\n",
    "class STGCN(tf.keras.Model):\n",
    "    def __init__(self, filters, kernel_size=[9, 3], stride=1, activation='relu',\n",
    "                 residual=True, downsample=False):\n",
    "        super().__init__()\n",
    "        self.sgcn = SGCN(filters, kernel_size=kernel_size[1])\n",
    "\n",
    "        self.tgcn = tf.keras.Sequential()\n",
    "        self.tgcn.add(tf.keras.layers.BatchNormalization(axis=1))\n",
    "        self.tgcn.add(tf.keras.layers.Activation(activation))\n",
    "        self.tgcn.add(tf.keras.layers.Conv2D(filters,\n",
    "                                                kernel_size=[kernel_size[0], 1],\n",
    "                                                strides=[stride, 1],\n",
    "                                                padding='same',\n",
    "                                                kernel_initializer=INITIALIZER,\n",
    "                                                data_format='channels_first',\n",
    "                                                kernel_regularizer=REGULARIZER))\n",
    "        self.tgcn.add(tf.keras.layers.BatchNormalization(axis=1))\n",
    "\n",
    "        self.act = tf.keras.layers.Activation(activation)\n",
    "\n",
    "        if not residual:\n",
    "            self.residual = lambda x, training=False: 0\n",
    "        elif residual and stride == 1 and not downsample:\n",
    "            self.residual = lambda x, training=False: x\n",
    "        else:\n",
    "            self.residual = tf.keras.Sequential()\n",
    "            self.residual.add(tf.keras.layers.Conv2D(filters,\n",
    "                                                        kernel_size=[1, 1],\n",
    "                                                        strides=[stride, 1],\n",
    "                                                        padding='same',\n",
    "                                                        kernel_initializer=INITIALIZER,\n",
    "                                                        data_format='channels_first',\n",
    "                                                        kernel_regularizer=REGULARIZER))\n",
    "            self.residual.add(tf.keras.layers.BatchNormalization(axis=1))\n",
    "\n",
    "    def call(self, x, A, training=True):\n",
    "        res = self.residual(x, training=training)\n",
    "        x, A = self.sgcn(x, A, training=training)\n",
    "        x = self.tgcn(x, training=training)\n",
    "        x += res\n",
    "        x = self.act(x)\n",
    "        return x, A\n",
    "\n",
    "\n",
    "\"\"\"Spatial temporal graph convolutional networks.\n",
    "    Args:\n",
    "        num_class (int): Number of classes for the classification task\n",
    "    Shape:(3, 5, 42, 1)\n",
    "        - Input: :math:`(N, in_channels, T_{in}, V_{in}, M_{in})`\n",
    "        - Output: :math:`(N, num_class)` where\n",
    "            :math:`N` is a batch size,\n",
    "            :math:`T_{in}` is a length of input sequence,\n",
    "            :math:`V_{in}` is the number of graph nodes,\n",
    "            :math:`M_{in}` is the number of instance in a frame.\n",
    "\"\"\"\n",
    "class Model(tf.keras.Model):\n",
    "    def __init__(self, num_classes=250):\n",
    "        super().__init__()\n",
    "\n",
    "        graph = Graph()\n",
    "        self.A = tf.Variable(graph.A,\n",
    "                             dtype=tf.float32,\n",
    "                             trainable=False,\n",
    "                             name='adjacency_matrix')\n",
    "\n",
    "        self.data_bn = tf.keras.layers.BatchNormalization(axis=1)\n",
    "\n",
    "        self.STGCN_layers = []\n",
    "        self.STGCN_layers.append(STGCN(64, residual=False))\n",
    "        self.STGCN_layers.append(STGCN(64))\n",
    "        self.STGCN_layers.append(STGCN(64))\n",
    "        self.STGCN_layers.append(STGCN(64))\n",
    "        self.STGCN_layers.append(STGCN(128, stride=2, downsample=True))\n",
    "        self.STGCN_layers.append(STGCN(128))\n",
    "        self.STGCN_layers.append(STGCN(128))\n",
    "        self.STGCN_layers.append(STGCN(256, stride=2, downsample=True))\n",
    "        self.STGCN_layers.append(STGCN(256))\n",
    "        self.STGCN_layers.append(STGCN(256))\n",
    "\n",
    "        self.pool = tf.keras.layers.GlobalAveragePooling2D(data_format='channels_first')\n",
    "\n",
    "        self.logits = tf.keras.layers.Conv2D(num_classes,\n",
    "                                             kernel_size=1,\n",
    "                                             padding='same',\n",
    "                                             kernel_initializer=INITIALIZER,\n",
    "                                             data_format='channels_first',\n",
    "                                             kernel_regularizer=REGULARIZER)\n",
    "\n",
    "    def call(self, x, training):\n",
    "        N = tf.shape(x)[0]\n",
    "        C = tf.shape(x)[1]\n",
    "        T = tf.shape(x)[2]\n",
    "        V = tf.shape(x)[3]\n",
    "        M = tf.shape(x)[4]\n",
    "\n",
    "        x = tf.transpose(x, perm=[0, 4, 3, 1, 2])\n",
    "        x = tf.reshape(x, [N * M, V * C, T])\n",
    "        x = self.data_bn(x, training=training)\n",
    "        x = tf.reshape(x, [N, M, V, C, T])\n",
    "        x = tf.transpose(x, perm=[0, 1, 3, 4, 2])\n",
    "        x = tf.reshape(x, [N * M, C, T, V])\n",
    "\n",
    "        A = self.A\n",
    "        for layer in self.STGCN_layers:\n",
    "            x, A = layer(x, A, training=training)\n",
    "\n",
    "        # N*M,C,T,V\n",
    "        x = self.pool(x)\n",
    "        x = tf.reshape(x, [N, M, -1, 1, 1])\n",
    "        x = tf.reduce_mean(x, axis=1)\n",
    "        x = self.logits(x)\n",
    "        x = tf.reshape(x, [N, -1])\n",
    "        x = tf.nn.softmax(x,axis=-1)\n",
    "        print(x.shape)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-28T10:31:59.350200Z",
     "iopub.status.busy": "2023-04-28T10:31:59.349922Z",
     "iopub.status.idle": "2023-04-28T10:31:59.496392Z",
     "shell.execute_reply": "2023-04-28T10:31:59.495394Z",
     "shell.execute_reply.started": "2023-04-28T10:31:59.350174Z"
    }
   },
   "outputs": [],
   "source": [
    "model =  Model(num_classes=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-28T10:32:38.625175Z",
     "iopub.status.busy": "2023-04-28T10:32:38.624133Z",
     "iopub.status.idle": "2023-04-28T10:32:38.632021Z",
     "shell.execute_reply": "2023-04-28T10:32:38.630992Z",
     "shell.execute_reply.started": "2023-04-28T10:32:38.625127Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_model():\n",
    "          \n",
    "    model =  Model(num_classes=250)\n",
    "    model.build((128,2,20,21,1))\n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics=[\n",
    "            \"accuracy\",\n",
    "        ]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-28T10:33:10.431060Z",
     "iopub.status.busy": "2023-04-28T10:33:10.430388Z",
     "iopub.status.idle": "2023-04-28T10:33:11.862496Z",
     "shell.execute_reply": "2023-04-28T10:33:11.861593Z",
     "shell.execute_reply.started": "2023-04-28T10:33:10.431020Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 250)\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization_23 (Bat  multiple                 168       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " stgcn_10 (STGCN)            multiple                  38016     \n",
      "                                                                 \n",
      " stgcn_11 (STGCN)            multiple                  49920     \n",
      "                                                                 \n",
      " stgcn_12 (STGCN)            multiple                  49920     \n",
      "                                                                 \n",
      " stgcn_13 (STGCN)            multiple                  49920     \n",
      "                                                                 \n",
      " stgcn_14 (STGCN)            multiple                  182400    \n",
      "                                                                 \n",
      " stgcn_15 (STGCN)            multiple                  198144    \n",
      "                                                                 \n",
      " stgcn_16 (STGCN)            multiple                  198144    \n",
      "                                                                 \n",
      " stgcn_17 (STGCN)            multiple                  725248    \n",
      "                                                                 \n",
      " stgcn_18 (STGCN)            multiple                  789504    \n",
      "                                                                 \n",
      " stgcn_19 (STGCN)            multiple                  789504    \n",
      "                                                                 \n",
      " global_average_pooling2d_1   multiple                 0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " conv2d_45 (Conv2D)          multiple                  64250     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,136,461\n",
      "Trainable params: 3,128,654\n",
      "Non-trainable params: 7,807\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-11T13:14:02.971926Z",
     "iopub.status.busy": "2023-04-11T13:14:02.971387Z",
     "iopub.status.idle": "2023-04-11T13:14:02.986496Z",
     "shell.execute_reply": "2023-04-11T13:14:02.985017Z",
     "shell.execute_reply.started": "2023-04-11T13:14:02.971887Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate an optimizer to train the model.\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# Instantiate a loss function\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "# Prepare the metrics.\n",
    "train_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
    "val_acc_metric   = tf.keras.metrics.CategoricalAccuracy()\n",
    "\n",
    "# tensorboard writer \n",
    "train_writer = tf.summary.create_file_writer('logs/train/')\n",
    "test_writer  = tf.summary.create_file_writer('logs/test/')\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(step, x, y):\n",
    "   '''\n",
    "   input: x, y <- typically batches \n",
    "   input: step <- batch step\n",
    "   return: loss value\n",
    "   '''\n",
    "\n",
    "    # start the scope of gradient \n",
    "   with tf.GradientTape() as tape:\n",
    "        logits = model(x, training=True) # forward pass\n",
    "        train_loss_value = loss_fn(y, logits) # compute loss \n",
    "\n",
    "    # compute gradient \n",
    "   grads = tape.gradient(train_loss_value, model.trainable_weights)\n",
    "\n",
    "    # update weights\n",
    "   optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "    # update metrics\n",
    "   train_acc_metric.update_state(y, logits)\n",
    "    \n",
    "    # write training loss and accuracy to the tensorboard\n",
    "   with train_writer.as_default():\n",
    "        tf.summary.scalar('loss', train_loss_value, step=step)\n",
    "        tf.summary.scalar(\n",
    "            'accuracy', train_acc_metric.result(), step=step\n",
    "        ) \n",
    "   return train_loss_value\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def test_step(step, x, y):\n",
    "   '''\n",
    "   input: x, y <- typically batches \n",
    "   input: step <- batch step\n",
    "   return: loss value\n",
    "   '''\n",
    "    # forward pass, no backprop, inference mode \n",
    "    val_logits = model(x, training=False) \n",
    "\n",
    "    # Compute the loss value \n",
    "    val_loss_value = loss_fn(y, val_logits)\n",
    "\n",
    "    # Update val metrics\n",
    "    val_acc_metric.update_state(y, val_logits)\n",
    "    \n",
    "    # write test loss and accuracy to the tensorboard\n",
    "    with test_writer.as_default():\n",
    "        tf.summary.scalar('val loss', val_loss_value, step=step)\n",
    "        tf.summary.scalar(\n",
    "            'val accuracy', val_acc_metric.result(), step=step\n",
    "        ) \n",
    "    return val_loss_value\n",
    "\n",
    "\n",
    "# custom training loop \n",
    "for epoch in range(100):\n",
    "    t = time.time()\n",
    "    # batch training \n",
    "\n",
    "    # Iterate over the batches of the train dataset.\n",
    "    for train_batch_step, (x_batch_train, \\\n",
    "                           y_batch_train) in enumerate(train_dataset):\n",
    "        train_batch_step = tf.convert_to_tensor(\n",
    "                                train_batch_step, dtype=tf.int64\n",
    "                           )\n",
    "        train_loss_value = train_step(\n",
    "                                train_batch_step, \n",
    "                                x_batch_train, y_batch_train\n",
    "                           )\n",
    "\n",
    "    # evaluation on validation set \n",
    "    # Run a validation loop at the end of each epoch.\n",
    "    for test_batch_step, (x_batch_val, \\\n",
    "                          y_batch_val) in enumerate(val_dataset):\n",
    "        test_batch_step = tf.convert_to_tensor(\n",
    "                               test_batch_step, dtype=tf.int64\n",
    "                          )\n",
    "        val_loss_value = test_step(\n",
    "                                test_batch_step, x_batch_val, y_batch_val\n",
    "                          )\n",
    "\n",
    "\n",
    "    template = '\n",
    "        ETA: {} - epoch: {} loss: {}  acc: {} val loss: {} val acc: {}\\n\n",
    "    '\n",
    "    print(template.format(\n",
    "        round((time.time() - t)/60, 2), epoch + 1,\n",
    "        train_loss_value, float(train_acc_metric.result()),\n",
    "        val_loss_value, float(val_acc_metric.result())\n",
    "    ))\n",
    "        \n",
    "    # Reset metrics at the end of each epoch\n",
    "    train_acc_metric.reset_states()\n",
    "    val_acc_metric.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-11T12:58:57.432016Z",
     "iopub.status.busy": "2023-04-11T12:58:57.431467Z",
     "iopub.status.idle": "2023-04-11T12:58:58.610898Z",
     "shell.execute_reply": "2023-04-11T12:58:58.609576Z",
     "shell.execute_reply.started": "2023-04-11T12:58:57.431981Z"
    },
    "papermill": {
     "duration": 3.974692,
     "end_time": "2023-03-02T08:44:46.689625",
     "exception": false,
     "start_time": "2023-03-02T08:44:42.714933",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# If True, processing data from scratch\n",
    "# If False, loads preprocessed data\n",
    "PREPROCESS_DATA = False\n",
    "TRAIN_MODEL = True\n",
    "# True: use 10% of participants as validation set\n",
    "# False: use all data for training -> gives better LB result\n",
    "USE_VAL = False\n",
    "N_ROWS = 543\n",
    "N_DIMS = 3\n",
    "DIM_NAMES = ['x', 'y']\n",
    "SEED = 42\n",
    "NUM_CLASSES = 250\n",
    "INPUT_SIZE = 64\n",
    "BATCH_ALL_SIGNS_N = 4\n",
    "BATCH_SIZE = 256\n",
    "N_EPOCHS = 100\n",
    "LR_MAX = 1e-3\n",
    "N_WARMUP_EPOCHS = 0\n",
    "WD_RATIO = 0.05\n",
    "MASK_VAL = 4237\n",
    "N_COLS = 61\n",
    "# Custom callback to update weight decay with learning rate\n",
    "class WeightDecayCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, wd_ratio=WD_RATIO):\n",
    "        self.step_counter = 0\n",
    "        self.wd_ratio = wd_ratio\n",
    "    \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        model.optimizer.weight_decay = model.optimizer.learning_rate * self.wd_ratio\n",
    "        print(f'learning rate: {model.optimizer.learning_rate.numpy():.2e}, weight decay: {model.optimizer.weight_decay.numpy():.2e}')\n",
    "\n",
    "def lrfn(current_step, num_warmup_steps, lr_max, num_cycles=0.50, num_training_steps=N_EPOCHS):\n",
    "    \n",
    "    if current_step < num_warmup_steps:\n",
    "        if WARMUP_METHOD == 'log':\n",
    "            return lr_max * 0.10 ** (num_warmup_steps - current_step)\n",
    "        else:\n",
    "            return lr_max * 2 ** -(num_warmup_steps - current_step)\n",
    "    else:\n",
    "        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n",
    "\n",
    "        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress))) * lr_max\n",
    "# Learning rate for encoder\n",
    "LR_SCHEDULE = [lrfn(step, num_warmup_steps=N_WARMUP_EPOCHS, lr_max=LR_MAX, num_cycles=0.50) for step in range(N_EPOCHS)]\n",
    "lr_callback = tf.keras.callbacks.LearningRateScheduler(lambda step: LR_SCHEDULE[step], verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization_23 (Bat  multiple                 168       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " stgcn_10 (STGCN)            multiple                  38016     \n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| sgcn_10 (SGCN)            multiple                  576       |\n",
      "||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯||\n",
      "|| conv2d_23 (Conv2D)      multiple                  576       ||\n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| sequential_12 (Sequential)  (128, 64, 20, 21)       37440     |\n",
      "||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯||\n",
      "|| batch_normalization_24 (Bat  (128, 64, 20, 21)    256       ||\n",
      "|| chNormalization)                                            ||\n",
      "||                                                             ||\n",
      "|| activation_20 (Activation)  (128, 64, 20, 21)     0         ||\n",
      "||                                                             ||\n",
      "|| conv2d_24 (Conv2D)      (128, 64, 20, 21)         36928     ||\n",
      "||                                                             ||\n",
      "|| batch_normalization_25 (Bat  (128, 64, 20, 21)    256       ||\n",
      "|| chNormalization)                                            ||\n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| activation_21 (Activation)  multiple                0         |\n",
      "¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n",
      " stgcn_11 (STGCN)            multiple                  49920     \n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| sgcn_11 (SGCN)            multiple                  12480     |\n",
      "||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯||\n",
      "|| conv2d_25 (Conv2D)      multiple                  12480     ||\n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| sequential_13 (Sequential)  (128, 64, 20, 21)       37440     |\n",
      "||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯||\n",
      "|| batch_normalization_26 (Bat  (128, 64, 20, 21)    256       ||\n",
      "|| chNormalization)                                            ||\n",
      "||                                                             ||\n",
      "|| activation_22 (Activation)  (128, 64, 20, 21)     0         ||\n",
      "||                                                             ||\n",
      "|| conv2d_26 (Conv2D)      (128, 64, 20, 21)         36928     ||\n",
      "||                                                             ||\n",
      "|| batch_normalization_27 (Bat  (128, 64, 20, 21)    256       ||\n",
      "|| chNormalization)                                            ||\n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| activation_23 (Activation)  multiple                0         |\n",
      "¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n",
      " stgcn_12 (STGCN)            multiple                  49920     \n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| sgcn_12 (SGCN)            multiple                  12480     |\n",
      "||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯||\n",
      "|| conv2d_27 (Conv2D)      multiple                  12480     ||\n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| sequential_14 (Sequential)  (128, 64, 20, 21)       37440     |\n",
      "||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯||\n",
      "|| batch_normalization_28 (Bat  (128, 64, 20, 21)    256       ||\n",
      "|| chNormalization)                                            ||\n",
      "||                                                             ||\n",
      "|| activation_24 (Activation)  (128, 64, 20, 21)     0         ||\n",
      "||                                                             ||\n",
      "|| conv2d_28 (Conv2D)      (128, 64, 20, 21)         36928     ||\n",
      "||                                                             ||\n",
      "|| batch_normalization_29 (Bat  (128, 64, 20, 21)    256       ||\n",
      "|| chNormalization)                                            ||\n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| activation_25 (Activation)  multiple                0         |\n",
      "¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n",
      " stgcn_13 (STGCN)            multiple                  49920     \n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| sgcn_13 (SGCN)            multiple                  12480     |\n",
      "||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯||\n",
      "|| conv2d_29 (Conv2D)      multiple                  12480     ||\n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| sequential_15 (Sequential)  (128, 64, 20, 21)       37440     |\n",
      "||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯||\n",
      "|| batch_normalization_30 (Bat  (128, 64, 20, 21)    256       ||\n",
      "|| chNormalization)                                            ||\n",
      "||                                                             ||\n",
      "|| activation_26 (Activation)  (128, 64, 20, 21)     0         ||\n",
      "||                                                             ||\n",
      "|| conv2d_30 (Conv2D)      (128, 64, 20, 21)         36928     ||\n",
      "||                                                             ||\n",
      "|| batch_normalization_31 (Bat  (128, 64, 20, 21)    256       ||\n",
      "|| chNormalization)                                            ||\n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| activation_27 (Activation)  multiple                0         |\n",
      "¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n",
      " stgcn_14 (STGCN)            multiple                  182400    \n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| sgcn_14 (SGCN)            multiple                  24960     |\n",
      "||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯||\n",
      "|| conv2d_31 (Conv2D)      multiple                  24960     ||\n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| sequential_16 (Sequential)  (128, 128, 10, 21)      148608    |\n",
      "||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯||\n",
      "|| batch_normalization_32 (Bat  (128, 128, 20, 21)   512       ||\n",
      "|| chNormalization)                                            ||\n",
      "||                                                             ||\n",
      "|| activation_28 (Activation)  (128, 128, 20, 21)    0         ||\n",
      "||                                                             ||\n",
      "|| conv2d_32 (Conv2D)      (128, 128, 10, 21)        147584    ||\n",
      "||                                                             ||\n",
      "|| batch_normalization_33 (Bat  (128, 128, 10, 21)   512       ||\n",
      "|| chNormalization)                                            ||\n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| activation_29 (Activation)  multiple                0         |\n",
      "|                                                               |\n",
      "| sequential_17 (Sequential)  (128, 128, 10, 21)      8832      |\n",
      "||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯||\n",
      "|| conv2d_33 (Conv2D)      (128, 128, 10, 21)        8320      ||\n",
      "||                                                             ||\n",
      "|| batch_normalization_34 (Bat  (128, 128, 10, 21)   512       ||\n",
      "|| chNormalization)                                            ||\n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n",
      " stgcn_15 (STGCN)            multiple                  198144    \n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| sgcn_15 (SGCN)            multiple                  49536     |\n",
      "||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯||\n",
      "|| conv2d_34 (Conv2D)      multiple                  49536     ||\n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| sequential_18 (Sequential)  (128, 128, 10, 21)      148608    |\n",
      "||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯||\n",
      "|| batch_normalization_35 (Bat  (128, 128, 10, 21)   512       ||\n",
      "|| chNormalization)                                            ||\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "||                                                             ||\n",
      "|| activation_30 (Activation)  (128, 128, 10, 21)    0         ||\n",
      "||                                                             ||\n",
      "|| conv2d_35 (Conv2D)      (128, 128, 10, 21)        147584    ||\n",
      "||                                                             ||\n",
      "|| batch_normalization_36 (Bat  (128, 128, 10, 21)   512       ||\n",
      "|| chNormalization)                                            ||\n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| activation_31 (Activation)  multiple                0         |\n",
      "¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n",
      " stgcn_16 (STGCN)            multiple                  198144    \n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| sgcn_16 (SGCN)            multiple                  49536     |\n",
      "||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯||\n",
      "|| conv2d_36 (Conv2D)      multiple                  49536     ||\n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| sequential_19 (Sequential)  (128, 128, 10, 21)      148608    |\n",
      "||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯||\n",
      "|| batch_normalization_37 (Bat  (128, 128, 10, 21)   512       ||\n",
      "|| chNormalization)                                            ||\n",
      "||                                                             ||\n",
      "|| activation_32 (Activation)  (128, 128, 10, 21)    0         ||\n",
      "||                                                             ||\n",
      "|| conv2d_37 (Conv2D)      (128, 128, 10, 21)        147584    ||\n",
      "||                                                             ||\n",
      "|| batch_normalization_38 (Bat  (128, 128, 10, 21)   512       ||\n",
      "|| chNormalization)                                            ||\n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| activation_33 (Activation)  multiple                0         |\n",
      "¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n",
      " stgcn_17 (STGCN)            multiple                  725248    \n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| sgcn_17 (SGCN)            multiple                  99072     |\n",
      "||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯||\n",
      "|| conv2d_38 (Conv2D)      multiple                  99072     ||\n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| sequential_20 (Sequential)  (128, 256, 5, 21)       592128    |\n",
      "||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯||\n",
      "|| batch_normalization_39 (Bat  (128, 256, 10, 21)   1024      ||\n",
      "|| chNormalization)                                            ||\n",
      "||                                                             ||\n",
      "|| activation_34 (Activation)  (128, 256, 10, 21)    0         ||\n",
      "||                                                             ||\n",
      "|| conv2d_39 (Conv2D)      (128, 256, 5, 21)         590080    ||\n",
      "||                                                             ||\n",
      "|| batch_normalization_40 (Bat  (128, 256, 5, 21)    1024      ||\n",
      "|| chNormalization)                                            ||\n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| activation_35 (Activation)  multiple                0         |\n",
      "|                                                               |\n",
      "| sequential_21 (Sequential)  (128, 256, 5, 21)       34048     |\n",
      "||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯||\n",
      "|| conv2d_40 (Conv2D)      (128, 256, 5, 21)         33024     ||\n",
      "||                                                             ||\n",
      "|| batch_normalization_41 (Bat  (128, 256, 5, 21)    1024      ||\n",
      "|| chNormalization)                                            ||\n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n",
      " stgcn_18 (STGCN)            multiple                  789504    \n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| sgcn_18 (SGCN)            multiple                  197376    |\n",
      "||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯||\n",
      "|| conv2d_41 (Conv2D)      multiple                  197376    ||\n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| sequential_22 (Sequential)  (128, 256, 5, 21)       592128    |\n",
      "||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯||\n",
      "|| batch_normalization_42 (Bat  (128, 256, 5, 21)    1024      ||\n",
      "|| chNormalization)                                            ||\n",
      "||                                                             ||\n",
      "|| activation_36 (Activation)  (128, 256, 5, 21)     0         ||\n",
      "||                                                             ||\n",
      "|| conv2d_42 (Conv2D)      (128, 256, 5, 21)         590080    ||\n",
      "||                                                             ||\n",
      "|| batch_normalization_43 (Bat  (128, 256, 5, 21)    1024      ||\n",
      "|| chNormalization)                                            ||\n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| activation_37 (Activation)  multiple                0         |\n",
      "¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n",
      " stgcn_19 (STGCN)            multiple                  789504    \n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| sgcn_19 (SGCN)            multiple                  197376    |\n",
      "||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯||\n",
      "|| conv2d_43 (Conv2D)      multiple                  197376    ||\n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| sequential_23 (Sequential)  (128, 256, 5, 21)       592128    |\n",
      "||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯||\n",
      "|| batch_normalization_44 (Bat  (128, 256, 5, 21)    1024      ||\n",
      "|| chNormalization)                                            ||\n",
      "||                                                             ||\n",
      "|| activation_38 (Activation)  (128, 256, 5, 21)     0         ||\n",
      "||                                                             ||\n",
      "|| conv2d_44 (Conv2D)      (128, 256, 5, 21)         590080    ||\n",
      "||                                                             ||\n",
      "|| batch_normalization_45 (Bat  (128, 256, 5, 21)    1024      ||\n",
      "|| chNormalization)                                            ||\n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| activation_39 (Activation)  multiple                0         |\n",
      "¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n",
      " global_average_pooling2d_1   multiple                 0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " conv2d_45 (Conv2D)          multiple                  64250     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,136,461\n",
      "Trainable params: 3,128,654\n",
      "Non-trainable params: 7,807\n",
      "_________________________________________________________________\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.001.\n",
      "learning rate: 1.00e-03, weight decay: 5.00e-05\n",
      "Epoch 1/100\n",
      "(128, 250)\n",
      "(128, 250)\n",
      "  5/590 [..............................] - ETA: 26s - loss: 13.2934 - accuracy: 0.0078WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0084s vs `on_train_batch_end` time: 0.0301s). Check your callbacks.\n",
      "589/590 [============================>.] - ETA: 0s - loss: 6.5179 - accuracy: 0.1514(128, 250)\n",
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.17878, saving model to models/weights_stgcn2.h5\n",
      "590/590 [==============================] - 34s 49ms/step - loss: 6.5139 - accuracy: 0.1516 - val_loss: 5.1600 - val_accuracy: 0.1788 - lr: 0.0010\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.0009997532801828658.\n",
      "learning rate: 1.00e-03, weight decay: 5.00e-05\n",
      "Epoch 2/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 3.8452 - accuracy: 0.3729\n",
      "Epoch 2: val_accuracy improved from 0.17878 to 0.35584, saving model to models/weights_stgcn2.h5\n",
      "590/590 [==============================] - 28s 47ms/step - loss: 3.8452 - accuracy: 0.3729 - val_loss: 3.8709 - val_accuracy: 0.3558 - lr: 9.9975e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.0009990133642141358.\n",
      "learning rate: 9.99e-04, weight decay: 5.00e-05\n",
      "Epoch 3/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 3.2777 - accuracy: 0.4648\n",
      "Epoch 3: val_accuracy improved from 0.35584 to 0.37768, saving model to models/weights_stgcn2.h5\n",
      "590/590 [==============================] - 27s 46ms/step - loss: 3.2777 - accuracy: 0.4648 - val_loss: 3.6289 - val_accuracy: 0.3777 - lr: 9.9901e-04\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.00099778098230154.\n",
      "learning rate: 9.98e-04, weight decay: 4.99e-05\n",
      "Epoch 4/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 3.0270 - accuracy: 0.5083\n",
      "Epoch 4: val_accuracy improved from 0.37768 to 0.44745, saving model to models/weights_stgcn2.h5\n",
      "590/590 [==============================] - 27s 46ms/step - loss: 3.0269 - accuracy: 0.5083 - val_loss: 3.2777 - val_accuracy: 0.4475 - lr: 9.9778e-04\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.000996057350657239.\n",
      "learning rate: 9.96e-04, weight decay: 4.98e-05\n",
      "Epoch 5/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.8904 - accuracy: 0.5361\n",
      "Epoch 5: val_accuracy did not improve from 0.44745\n",
      "590/590 [==============================] - 27s 46ms/step - loss: 2.8903 - accuracy: 0.5362 - val_loss: 3.2602 - val_accuracy: 0.4474 - lr: 9.9606e-04\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.0009938441702975688.\n",
      "learning rate: 9.94e-04, weight decay: 4.97e-05\n",
      "Epoch 6/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.8124 - accuracy: 0.5563\n",
      "Epoch 6: val_accuracy improved from 0.44745 to 0.51637, saving model to models/weights_stgcn2.h5\n",
      "590/590 [==============================] - 27s 46ms/step - loss: 2.8130 - accuracy: 0.5561 - val_loss: 3.0309 - val_accuracy: 0.5164 - lr: 9.9384e-04\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.0009911436253643444.\n",
      "learning rate: 9.91e-04, weight decay: 4.96e-05\n",
      "Epoch 7/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.7685 - accuracy: 0.5715\n",
      "Epoch 7: val_accuracy did not improve from 0.51637\n",
      "590/590 [==============================] - 27s 46ms/step - loss: 2.7684 - accuracy: 0.5716 - val_loss: 3.7081 - val_accuracy: 0.3972 - lr: 9.9114e-04\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 0.0009879583809693738.\n",
      "learning rate: 9.88e-04, weight decay: 4.94e-05\n",
      "Epoch 8/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.7372 - accuracy: 0.5826\n",
      "Epoch 8: val_accuracy did not improve from 0.51637\n",
      "590/590 [==============================] - 27s 46ms/step - loss: 2.7370 - accuracy: 0.5827 - val_loss: 3.1366 - val_accuracy: 0.4982 - lr: 9.8796e-04\n",
      "\n",
      "Epoch 9: LearningRateScheduler setting learning rate to 0.0009842915805643156.\n",
      "learning rate: 9.84e-04, weight decay: 4.92e-05\n",
      "Epoch 9/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.7184 - accuracy: 0.5966\n",
      "Epoch 9: val_accuracy improved from 0.51637 to 0.54094, saving model to models/weights_stgcn2.h5\n",
      "590/590 [==============================] - 27s 46ms/step - loss: 2.7181 - accuracy: 0.5966 - val_loss: 2.9592 - val_accuracy: 0.5409 - lr: 9.8429e-04\n",
      "\n",
      "Epoch 10: LearningRateScheduler setting learning rate to 0.0009801468428384716.\n",
      "learning rate: 9.80e-04, weight decay: 4.90e-05\n",
      "Epoch 10/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.7026 - accuracy: 0.6052\n",
      "Epoch 10: val_accuracy did not improve from 0.54094\n",
      "590/590 [==============================] - 27s 46ms/step - loss: 2.7030 - accuracy: 0.6052 - val_loss: 3.2316 - val_accuracy: 0.4951 - lr: 9.8015e-04\n",
      "\n",
      "Epoch 11: LearningRateScheduler setting learning rate to 0.0009755282581475768.\n",
      "learning rate: 9.76e-04, weight decay: 4.88e-05\n",
      "Epoch 11/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.6952 - accuracy: 0.6131\n",
      "Epoch 11: val_accuracy did not improve from 0.54094\n",
      "590/590 [==============================] - 27s 46ms/step - loss: 2.6952 - accuracy: 0.6130 - val_loss: 3.1660 - val_accuracy: 0.5088 - lr: 9.7553e-04\n",
      "\n",
      "Epoch 12: LearningRateScheduler setting learning rate to 0.0009704403844771128.\n",
      "learning rate: 9.70e-04, weight decay: 4.85e-05\n",
      "Epoch 12/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.6849 - accuracy: 0.6214\n",
      "Epoch 12: val_accuracy improved from 0.54094 to 0.56057, saving model to models/weights_stgcn2.h5\n",
      "590/590 [==============================] - 27s 46ms/step - loss: 2.6853 - accuracy: 0.6214 - val_loss: 2.9602 - val_accuracy: 0.5606 - lr: 9.7044e-04\n",
      "\n",
      "Epoch 13: LearningRateScheduler setting learning rate to 0.0009648882429441257.\n",
      "learning rate: 9.65e-04, weight decay: 4.82e-05\n",
      "Epoch 13/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.6820 - accuracy: 0.6261\n",
      "Epoch 13: val_accuracy did not improve from 0.56057\n",
      "590/590 [==============================] - 27s 46ms/step - loss: 2.6823 - accuracy: 0.6259 - val_loss: 3.0597 - val_accuracy: 0.5454 - lr: 9.6489e-04\n",
      "\n",
      "Epoch 14: LearningRateScheduler setting learning rate to 0.0009588773128419905.\n",
      "learning rate: 9.59e-04, weight decay: 4.79e-05\n",
      "Epoch 14/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.6873 - accuracy: 0.6321\n",
      "Epoch 14: val_accuracy did not improve from 0.56057\n",
      "590/590 [==============================] - 27s 46ms/step - loss: 2.6871 - accuracy: 0.6322 - val_loss: 3.1722 - val_accuracy: 0.5295 - lr: 9.5888e-04\n",
      "\n",
      "Epoch 15: LearningRateScheduler setting learning rate to 0.0009524135262330098.\n",
      "learning rate: 9.52e-04, weight decay: 4.76e-05\n",
      "Epoch 15/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.6791 - accuracy: 0.6382\n",
      "Epoch 15: val_accuracy improved from 0.56057 to 0.57615, saving model to models/weights_stgcn2.h5\n",
      "590/590 [==============================] - 27s 46ms/step - loss: 2.6790 - accuracy: 0.6382 - val_loss: 2.9740 - val_accuracy: 0.5761 - lr: 9.5241e-04\n",
      "\n",
      "Epoch 16: LearningRateScheduler setting learning rate to 0.0009455032620941839.\n",
      "learning rate: 9.46e-04, weight decay: 4.73e-05\n",
      "Epoch 16/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.6756 - accuracy: 0.6429\n",
      "Epoch 16: val_accuracy did not improve from 0.57615\n",
      "590/590 [==============================] - 27s 46ms/step - loss: 2.6757 - accuracy: 0.6429 - val_loss: 3.0457 - val_accuracy: 0.5592 - lr: 9.4550e-04\n",
      "\n",
      "Epoch 17: LearningRateScheduler setting learning rate to 0.0009381533400219318.\n",
      "learning rate: 9.38e-04, weight decay: 4.69e-05\n",
      "Epoch 17/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 2.6674 - accuracy: 0.6472\n",
      "Epoch 17: val_accuracy improved from 0.57615 to 0.57952, saving model to models/weights_stgcn2.h5\n",
      "590/590 [==============================] - 27s 46ms/step - loss: 2.6674 - accuracy: 0.6472 - val_loss: 2.9940 - val_accuracy: 0.5795 - lr: 9.3815e-04\n",
      "\n",
      "Epoch 18: LearningRateScheduler setting learning rate to 0.0009303710135019718.\n",
      "learning rate: 9.30e-04, weight decay: 4.65e-05\n",
      "Epoch 18/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.6587 - accuracy: 0.6521\n",
      "Epoch 18: val_accuracy did not improve from 0.57952\n",
      "590/590 [==============================] - 27s 46ms/step - loss: 2.6591 - accuracy: 0.6521 - val_loss: 3.0285 - val_accuracy: 0.5724 - lr: 9.3037e-04\n",
      "\n",
      "Epoch 19: LearningRateScheduler setting learning rate to 0.0009221639627510075.\n",
      "learning rate: 9.22e-04, weight decay: 4.61e-05\n",
      "Epoch 19/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.6551 - accuracy: 0.6553\n",
      "Epoch 19: val_accuracy did not improve from 0.57952\n",
      "590/590 [==============================] - 27s 46ms/step - loss: 2.6552 - accuracy: 0.6553 - val_loss: 3.0908 - val_accuracy: 0.5591 - lr: 9.2216e-04\n",
      "\n",
      "Epoch 20: LearningRateScheduler setting learning rate to 0.0009135402871372809.\n",
      "learning rate: 9.14e-04, weight decay: 4.57e-05\n",
      "Epoch 20/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.6516 - accuracy: 0.6619\n",
      "Epoch 20: val_accuracy improved from 0.57952 to 0.58107, saving model to models/weights_stgcn2.h5\n",
      "590/590 [==============================] - 27s 46ms/step - loss: 2.6516 - accuracy: 0.6618 - val_loss: 3.0132 - val_accuracy: 0.5811 - lr: 9.1354e-04\n",
      "\n",
      "Epoch 21: LearningRateScheduler setting learning rate to 0.0009045084971874737.\n",
      "learning rate: 9.05e-04, weight decay: 4.52e-05\n",
      "Epoch 21/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "589/590 [============================>.] - ETA: 0s - loss: 2.6479 - accuracy: 0.6651\n",
      "Epoch 21: val_accuracy did not improve from 0.58107\n",
      "590/590 [==============================] - 27s 46ms/step - loss: 2.6479 - accuracy: 0.6651 - val_loss: 3.0774 - val_accuracy: 0.5644 - lr: 9.0451e-04\n",
      "\n",
      "Epoch 22: LearningRateScheduler setting learning rate to 0.0008950775061878452.\n",
      "learning rate: 8.95e-04, weight decay: 4.48e-05\n",
      "Epoch 22/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.6388 - accuracy: 0.6683\n",
      "Epoch 22: val_accuracy did not improve from 0.58107\n",
      "590/590 [==============================] - 27s 46ms/step - loss: 2.6386 - accuracy: 0.6683 - val_loss: 3.0439 - val_accuracy: 0.5716 - lr: 8.9508e-04\n",
      "\n",
      "Epoch 23: LearningRateScheduler setting learning rate to 0.0008852566213878947.\n",
      "learning rate: 8.85e-04, weight decay: 4.43e-05\n",
      "Epoch 23/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.6291 - accuracy: 0.6719\n",
      "Epoch 23: val_accuracy did not improve from 0.58107\n",
      "590/590 [==============================] - 27s 46ms/step - loss: 2.6293 - accuracy: 0.6718 - val_loss: 3.2039 - val_accuracy: 0.5450 - lr: 8.8526e-04\n",
      "\n",
      "Epoch 24: LearningRateScheduler setting learning rate to 0.0008750555348152298.\n",
      "learning rate: 8.75e-04, weight decay: 4.38e-05\n",
      "Epoch 24/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.6255 - accuracy: 0.6738\n",
      "Epoch 24: val_accuracy did not improve from 0.58107\n",
      "590/590 [==============================] - 27s 46ms/step - loss: 2.6259 - accuracy: 0.6737 - val_loss: 3.0832 - val_accuracy: 0.5736 - lr: 8.7506e-04\n",
      "\n",
      "Epoch 25: LearningRateScheduler setting learning rate to 0.0008644843137107057.\n",
      "learning rate: 8.64e-04, weight decay: 4.32e-05\n",
      "Epoch 25/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.6227 - accuracy: 0.6781\n",
      "Epoch 25: val_accuracy did not improve from 0.58107\n",
      "590/590 [==============================] - 27s 46ms/step - loss: 2.6225 - accuracy: 0.6780 - val_loss: 3.1828 - val_accuracy: 0.5540 - lr: 8.6448e-04\n",
      "\n",
      "Epoch 26: LearningRateScheduler setting learning rate to 0.0008535533905932737.\n",
      "learning rate: 8.54e-04, weight decay: 4.27e-05\n",
      "Epoch 26/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.6114 - accuracy: 0.6827\n",
      "Epoch 26: val_accuracy improved from 0.58107 to 0.60723, saving model to models/weights_stgcn2.h5\n",
      "590/590 [==============================] - 27s 46ms/step - loss: 2.6112 - accuracy: 0.6827 - val_loss: 2.9546 - val_accuracy: 0.6072 - lr: 8.5355e-04\n",
      "\n",
      "Epoch 27: LearningRateScheduler setting learning rate to 0.0008422735529643444.\n",
      "learning rate: 8.42e-04, weight decay: 4.21e-05\n",
      "Epoch 27/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.6015 - accuracy: 0.6865\n",
      "Epoch 27: val_accuracy did not improve from 0.60723\n",
      "590/590 [==============================] - 27s 46ms/step - loss: 2.6017 - accuracy: 0.6864 - val_loss: 3.0265 - val_accuracy: 0.5871 - lr: 8.4227e-04\n",
      "\n",
      "Epoch 28: LearningRateScheduler setting learning rate to 0.0008306559326618259.\n",
      "learning rate: 8.31e-04, weight decay: 4.15e-05\n",
      "Epoch 28/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.5940 - accuracy: 0.6902\n",
      "Epoch 28: val_accuracy improved from 0.60723 to 0.61307, saving model to models/weights_stgcn2.h5\n",
      "590/590 [==============================] - 27s 46ms/step - loss: 2.5943 - accuracy: 0.6901 - val_loss: 2.9773 - val_accuracy: 0.6131 - lr: 8.3066e-04\n",
      "\n",
      "Epoch 29: LearningRateScheduler setting learning rate to 0.0008187119948743449.\n",
      "learning rate: 8.19e-04, weight decay: 4.09e-05\n",
      "Epoch 29/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.5888 - accuracy: 0.6943\n",
      "Epoch 29: val_accuracy improved from 0.61307 to 0.62345, saving model to models/weights_stgcn2.h5\n",
      "590/590 [==============================] - 27s 46ms/step - loss: 2.5889 - accuracy: 0.6942 - val_loss: 2.9096 - val_accuracy: 0.6234 - lr: 8.1871e-04\n",
      "\n",
      "Epoch 30: LearningRateScheduler setting learning rate to 0.0008064535268264883.\n",
      "learning rate: 8.06e-04, weight decay: 4.03e-05\n",
      "Epoch 30/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.5758 - accuracy: 0.6990\n",
      "Epoch 30: val_accuracy did not improve from 0.62345\n",
      "590/590 [==============================] - 27s 46ms/step - loss: 2.5758 - accuracy: 0.6990 - val_loss: 3.0372 - val_accuracy: 0.5999 - lr: 8.0645e-04\n",
      "\n",
      "Epoch 31: LearningRateScheduler setting learning rate to 0.0007938926261462366.\n",
      "learning rate: 7.94e-04, weight decay: 3.97e-05\n",
      "Epoch 31/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.5708 - accuracy: 0.7031\n",
      "Epoch 31: val_accuracy did not improve from 0.62345\n",
      "590/590 [==============================] - 27s 46ms/step - loss: 2.5710 - accuracy: 0.7030 - val_loss: 2.9822 - val_accuracy: 0.6144 - lr: 7.9389e-04\n",
      "\n",
      "Epoch 32: LearningRateScheduler setting learning rate to 0.0007810416889260653.\n",
      "learning rate: 7.81e-04, weight decay: 3.91e-05\n",
      "Epoch 32/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.5631 - accuracy: 0.7061\n",
      "Epoch 32: val_accuracy did not improve from 0.62345\n",
      "590/590 [==============================] - 27s 46ms/step - loss: 2.5639 - accuracy: 0.7059 - val_loss: 3.2982 - val_accuracy: 0.5490 - lr: 7.8104e-04\n",
      "\n",
      "Epoch 33: LearningRateScheduler setting learning rate to 0.0007679133974894983.\n",
      "learning rate: 7.68e-04, weight decay: 3.84e-05\n",
      "Epoch 33/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.5533 - accuracy: 0.7082\n",
      "Epoch 33: val_accuracy did not improve from 0.62345\n",
      "590/590 [==============================] - 27s 46ms/step - loss: 2.5535 - accuracy: 0.7082 - val_loss: 2.9816 - val_accuracy: 0.6129 - lr: 7.6791e-04\n",
      "\n",
      "Epoch 34: LearningRateScheduler setting learning rate to 0.0007545207078751857.\n",
      "learning rate: 7.55e-04, weight decay: 3.77e-05\n",
      "Epoch 34/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.5440 - accuracy: 0.7129\n",
      "Epoch 34: val_accuracy did not improve from 0.62345\n",
      "590/590 [==============================] - 27s 46ms/step - loss: 2.5443 - accuracy: 0.7129 - val_loss: 3.1417 - val_accuracy: 0.5854 - lr: 7.5452e-04\n",
      "\n",
      "Epoch 35: LearningRateScheduler setting learning rate to 0.0007408768370508576.\n",
      "learning rate: 7.41e-04, weight decay: 3.70e-05\n",
      "Epoch 35/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.5376 - accuracy: 0.7154\n",
      "Epoch 35: val_accuracy improved from 0.62345 to 0.63073, saving model to models/weights_stgcn2.h5\n",
      "590/590 [==============================] - 27s 46ms/step - loss: 2.5381 - accuracy: 0.7153 - val_loss: 2.9269 - val_accuracy: 0.6307 - lr: 7.4088e-04\n",
      "\n",
      "Epoch 36: LearningRateScheduler setting learning rate to 0.0007269952498697733.\n",
      "learning rate: 7.27e-04, weight decay: 3.63e-05\n",
      "Epoch 36/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.5236 - accuracy: 0.7216\n",
      "Epoch 36: val_accuracy improved from 0.63073 to 0.63699, saving model to models/weights_stgcn2.h5\n",
      "590/590 [==============================] - 27s 46ms/step - loss: 2.5236 - accuracy: 0.7215 - val_loss: 2.9169 - val_accuracy: 0.6370 - lr: 7.2700e-04\n",
      "\n",
      "Epoch 37: LearningRateScheduler setting learning rate to 0.0007128896457825364.\n",
      "learning rate: 7.13e-04, weight decay: 3.56e-05\n",
      "Epoch 37/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.5165 - accuracy: 0.7257\n",
      "Epoch 37: val_accuracy did not improve from 0.63699\n",
      "590/590 [==============================] - 27s 46ms/step - loss: 2.5166 - accuracy: 0.7257 - val_loss: 3.0342 - val_accuracy: 0.6078 - lr: 7.1289e-04\n",
      "\n",
      "Epoch 38: LearningRateScheduler setting learning rate to 0.0006985739453173903.\n",
      "learning rate: 6.99e-04, weight decay: 3.49e-05\n",
      "Epoch 38/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.5115 - accuracy: 0.7280\n",
      "Epoch 38: val_accuracy did not improve from 0.63699\n",
      "590/590 [==============================] - 27s 46ms/step - loss: 2.5113 - accuracy: 0.7281 - val_loss: 2.9463 - val_accuracy: 0.6366 - lr: 6.9857e-04\n",
      "\n",
      "Epoch 39: LearningRateScheduler setting learning rate to 0.0006840622763423391.\n",
      "learning rate: 6.84e-04, weight decay: 3.42e-05\n",
      "Epoch 39/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.5010 - accuracy: 0.7311\n",
      "Epoch 39: val_accuracy improved from 0.63699 to 0.63966, saving model to models/weights_stgcn2.h5\n",
      "590/590 [==============================] - 27s 46ms/step - loss: 2.5009 - accuracy: 0.7312 - val_loss: 2.9094 - val_accuracy: 0.6397 - lr: 6.8406e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 40: LearningRateScheduler setting learning rate to 0.0006693689601226458.\n",
      "learning rate: 6.69e-04, weight decay: 3.35e-05\n",
      "Epoch 40/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.4879 - accuracy: 0.7369\n",
      "Epoch 40: val_accuracy did not improve from 0.63966\n",
      "590/590 [==============================] - 27s 46ms/step - loss: 2.4874 - accuracy: 0.7370 - val_loss: 2.9430 - val_accuracy: 0.6381 - lr: 6.6937e-04\n",
      "\n",
      "Epoch 41: LearningRateScheduler setting learning rate to 0.0006545084971874737.\n",
      "learning rate: 6.55e-04, weight decay: 3.27e-05\n",
      "Epoch 41/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.4771 - accuracy: 0.7410\n",
      "Epoch 41: val_accuracy did not improve from 0.63966\n",
      "590/590 [==============================] - 27s 46ms/step - loss: 2.4770 - accuracy: 0.7410 - val_loss: 3.0545 - val_accuracy: 0.6132 - lr: 6.5451e-04\n",
      "\n",
      "Epoch 42: LearningRateScheduler setting learning rate to 0.0006394955530196147.\n",
      "learning rate: 6.39e-04, weight decay: 3.20e-05\n",
      "Epoch 42/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.4658 - accuracy: 0.7461\n",
      "Epoch 42: val_accuracy did not improve from 0.63966\n",
      "590/590 [==============================] - 27s 46ms/step - loss: 2.4657 - accuracy: 0.7462 - val_loss: 2.9573 - val_accuracy: 0.6369 - lr: 6.3950e-04\n",
      "\n",
      "Epoch 43: LearningRateScheduler setting learning rate to 0.0006243449435824276.\n",
      "learning rate: 6.24e-04, weight decay: 3.12e-05\n",
      "Epoch 43/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.4580 - accuracy: 0.7498\n",
      "Epoch 43: val_accuracy improved from 0.63966 to 0.65266, saving model to models/weights_stgcn2.h5\n",
      "590/590 [==============================] - 27s 46ms/step - loss: 2.4580 - accuracy: 0.7499 - val_loss: 2.8947 - val_accuracy: 0.6527 - lr: 6.2434e-04\n",
      "\n",
      "Epoch 44: LearningRateScheduler setting learning rate to 0.0006090716206982714.\n",
      "learning rate: 6.09e-04, weight decay: 3.05e-05\n",
      "Epoch 44/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.4415 - accuracy: 0.7541\n",
      "Epoch 44: val_accuracy did not improve from 0.65266\n",
      "590/590 [==============================] - 27s 46ms/step - loss: 2.4413 - accuracy: 0.7541 - val_loss: 2.9415 - val_accuracy: 0.6458 - lr: 6.0907e-04\n",
      "\n",
      "Epoch 45: LearningRateScheduler setting learning rate to 0.0005936906572928624.\n",
      "learning rate: 5.94e-04, weight decay: 2.97e-05\n",
      "Epoch 45/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.4274 - accuracy: 0.7577\n",
      "Epoch 45: val_accuracy did not improve from 0.65266\n",
      "590/590 [==============================] - 27s 46ms/step - loss: 2.4277 - accuracy: 0.7576 - val_loss: 3.0403 - val_accuracy: 0.6337 - lr: 5.9369e-04\n",
      "\n",
      "Epoch 46: LearningRateScheduler setting learning rate to 0.0005782172325201155.\n",
      "learning rate: 5.78e-04, weight decay: 2.89e-05\n",
      "Epoch 46/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.4139 - accuracy: 0.7649\n",
      "Epoch 46: val_accuracy did not improve from 0.65266\n",
      "590/590 [==============================] - 27s 46ms/step - loss: 2.4139 - accuracy: 0.7649 - val_loss: 2.9749 - val_accuracy: 0.6366 - lr: 5.7822e-04\n",
      "\n",
      "Epoch 47: LearningRateScheduler setting learning rate to 0.0005626666167821521.\n",
      "learning rate: 5.63e-04, weight decay: 2.81e-05\n",
      "Epoch 47/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.4006 - accuracy: 0.7679\n",
      "Epoch 47: val_accuracy did not improve from 0.65266\n",
      "590/590 [==============================] - 27s 46ms/step - loss: 2.4005 - accuracy: 0.7678 - val_loss: 3.0950 - val_accuracy: 0.6227 - lr: 5.6267e-04\n",
      "\n",
      "Epoch 48: LearningRateScheduler setting learning rate to 0.0005470541566592572.\n",
      "learning rate: 5.47e-04, weight decay: 2.74e-05\n",
      "Epoch 48/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.3856 - accuracy: 0.7735\n",
      "Epoch 48: val_accuracy did not improve from 0.65266\n",
      "590/590 [==============================] - 27s 46ms/step - loss: 2.3858 - accuracy: 0.7734 - val_loss: 3.0588 - val_accuracy: 0.6251 - lr: 5.4705e-04\n",
      "\n",
      "Epoch 49: LearningRateScheduler setting learning rate to 0.0005313952597646568.\n",
      "learning rate: 5.31e-04, weight decay: 2.66e-05\n",
      "Epoch 49/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.3715 - accuracy: 0.7802\n",
      "Epoch 49: val_accuracy did not improve from 0.65266\n",
      "590/590 [==============================] - 27s 46ms/step - loss: 2.3712 - accuracy: 0.7802 - val_loss: 2.9977 - val_accuracy: 0.6489 - lr: 5.3140e-04\n",
      "\n",
      "Epoch 50: LearningRateScheduler setting learning rate to 0.0005157053795390641.\n",
      "learning rate: 5.16e-04, weight decay: 2.58e-05\n",
      "Epoch 50/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.3518 - accuracy: 0.7848\n",
      "Epoch 50: val_accuracy improved from 0.65266 to 0.65588, saving model to models/weights_stgcn2.h5\n",
      "590/590 [==============================] - 27s 46ms/step - loss: 2.3521 - accuracy: 0.7848 - val_loss: 2.9574 - val_accuracy: 0.6559 - lr: 5.1571e-04\n",
      "\n",
      "Epoch 51: LearningRateScheduler setting learning rate to 0.0005.\n",
      "learning rate: 5.00e-04, weight decay: 2.50e-05\n",
      "Epoch 51/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.3322 - accuracy: 0.7902\n",
      "Epoch 51: val_accuracy did not improve from 0.65588\n",
      "590/590 [==============================] - 27s 46ms/step - loss: 2.3323 - accuracy: 0.7902 - val_loss: 2.9908 - val_accuracy: 0.6516 - lr: 5.0000e-04\n",
      "\n",
      "Epoch 52: LearningRateScheduler setting learning rate to 0.00048429462046093585.\n",
      "learning rate: 4.84e-04, weight decay: 2.42e-05\n",
      "Epoch 52/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.3146 - accuracy: 0.7952\n",
      "Epoch 52: val_accuracy improved from 0.65588 to 0.66594, saving model to models/weights_stgcn2.h5\n",
      "590/590 [==============================] - 27s 46ms/step - loss: 2.3152 - accuracy: 0.7951 - val_loss: 2.9217 - val_accuracy: 0.6659 - lr: 4.8429e-04\n",
      "\n",
      "Epoch 53: LearningRateScheduler setting learning rate to 0.0004686047402353433.\n",
      "learning rate: 4.69e-04, weight decay: 2.34e-05\n",
      "Epoch 53/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.2936 - accuracy: 0.8022\n",
      "Epoch 53: val_accuracy did not improve from 0.66594\n",
      "590/590 [==============================] - 27s 46ms/step - loss: 2.2937 - accuracy: 0.8022 - val_loss: 2.9485 - val_accuracy: 0.6596 - lr: 4.6860e-04\n",
      "\n",
      "Epoch 54: LearningRateScheduler setting learning rate to 0.00045294584334074284.\n",
      "learning rate: 4.53e-04, weight decay: 2.26e-05\n",
      "Epoch 54/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.2730 - accuracy: 0.8074\n",
      "Epoch 54: val_accuracy did not improve from 0.66594\n",
      "590/590 [==============================] - 27s 46ms/step - loss: 2.2735 - accuracy: 0.8073 - val_loss: 3.0025 - val_accuracy: 0.6453 - lr: 4.5295e-04\n",
      "\n",
      "Epoch 55: LearningRateScheduler setting learning rate to 0.00043733338321784784.\n",
      "learning rate: 4.37e-04, weight decay: 2.19e-05\n",
      "Epoch 55/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.2507 - accuracy: 0.8139\n",
      "Epoch 55: val_accuracy did not improve from 0.66594\n",
      "590/590 [==============================] - 27s 46ms/step - loss: 2.2507 - accuracy: 0.8139 - val_loss: 2.9655 - val_accuracy: 0.6604 - lr: 4.3733e-04\n",
      "\n",
      "Epoch 56: LearningRateScheduler setting learning rate to 0.0004217827674798845.\n",
      "learning rate: 4.22e-04, weight decay: 2.11e-05\n",
      "Epoch 56/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.2319 - accuracy: 0.8181\n",
      "Epoch 56: val_accuracy did not improve from 0.66594\n",
      "590/590 [==============================] - 27s 46ms/step - loss: 2.2317 - accuracy: 0.8181 - val_loss: 2.9577 - val_accuracy: 0.6600 - lr: 4.2178e-04\n",
      "\n",
      "Epoch 57: LearningRateScheduler setting learning rate to 0.0004063093427071376.\n",
      "learning rate: 4.06e-04, weight decay: 2.03e-05\n",
      "Epoch 57/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.2041 - accuracy: 0.8273\n",
      "Epoch 57: val_accuracy did not improve from 0.66594\n",
      "590/590 [==============================] - 27s 46ms/step - loss: 2.2039 - accuracy: 0.8274 - val_loss: 3.0196 - val_accuracy: 0.6567 - lr: 4.0631e-04\n",
      "\n",
      "Epoch 58: LearningRateScheduler setting learning rate to 0.0003909283793017289.\n",
      "learning rate: 3.91e-04, weight decay: 1.95e-05\n",
      "Epoch 58/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.1820 - accuracy: 0.8324\n",
      "Epoch 58: val_accuracy did not improve from 0.66594\n",
      "590/590 [==============================] - 27s 46ms/step - loss: 2.1821 - accuracy: 0.8324 - val_loss: 2.9434 - val_accuracy: 0.6643 - lr: 3.9093e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 59: LearningRateScheduler setting learning rate to 0.0003756550564175727.\n",
      "learning rate: 3.76e-04, weight decay: 1.88e-05\n",
      "Epoch 59/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.1605 - accuracy: 0.8387\n",
      "Epoch 59: val_accuracy did not improve from 0.66594\n",
      "590/590 [==============================] - 27s 46ms/step - loss: 2.1607 - accuracy: 0.8387 - val_loss: 2.9699 - val_accuracy: 0.6642 - lr: 3.7566e-04\n",
      "\n",
      "Epoch 60: LearningRateScheduler setting learning rate to 0.0003605044469803854.\n",
      "learning rate: 3.61e-04, weight decay: 1.80e-05\n",
      "Epoch 60/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.1358 - accuracy: 0.8456\n",
      "Epoch 60: val_accuracy improved from 0.66594 to 0.66850, saving model to models/weights_stgcn2.h5\n",
      "590/590 [==============================] - 27s 46ms/step - loss: 2.1359 - accuracy: 0.8457 - val_loss: 2.9529 - val_accuracy: 0.6685 - lr: 3.6050e-04\n",
      "\n",
      "Epoch 61: LearningRateScheduler setting learning rate to 0.00034549150281252633.\n",
      "learning rate: 3.45e-04, weight decay: 1.73e-05\n",
      "Epoch 61/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.1101 - accuracy: 0.8517\n",
      "Epoch 61: val_accuracy improved from 0.66850 to 0.67161, saving model to models/weights_stgcn2.h5\n",
      "590/590 [==============================] - 27s 46ms/step - loss: 2.1101 - accuracy: 0.8517 - val_loss: 2.9266 - val_accuracy: 0.6716 - lr: 3.4549e-04\n",
      "\n",
      "Epoch 62: LearningRateScheduler setting learning rate to 0.0003306310398773543.\n",
      "learning rate: 3.31e-04, weight decay: 1.65e-05\n",
      "Epoch 62/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.0813 - accuracy: 0.8592\n",
      "Epoch 62: val_accuracy did not improve from 0.67161\n",
      "590/590 [==============================] - 27s 46ms/step - loss: 2.0814 - accuracy: 0.8592 - val_loss: 2.9396 - val_accuracy: 0.6711 - lr: 3.3063e-04\n",
      "\n",
      "Epoch 63: LearningRateScheduler setting learning rate to 0.00031593772365766105.\n",
      "learning rate: 3.16e-04, weight decay: 1.58e-05\n",
      "Epoch 63/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.0567 - accuracy: 0.8643\n",
      "Epoch 63: val_accuracy improved from 0.67161 to 0.67171, saving model to models/weights_stgcn2.h5\n",
      "590/590 [==============================] - 27s 46ms/step - loss: 2.0568 - accuracy: 0.8643 - val_loss: 2.9467 - val_accuracy: 0.6717 - lr: 3.1594e-04\n",
      "\n",
      "Epoch 64: LearningRateScheduler setting learning rate to 0.00030142605468260977.\n",
      "learning rate: 3.01e-04, weight decay: 1.51e-05\n",
      "Epoch 64/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 2.0272 - accuracy: 0.8703\n",
      "Epoch 64: val_accuracy did not improve from 0.67171\n",
      "590/590 [==============================] - 27s 46ms/step - loss: 2.0274 - accuracy: 0.8703 - val_loss: 3.0522 - val_accuracy: 0.6544 - lr: 3.0143e-04\n",
      "\n",
      "Epoch 65: LearningRateScheduler setting learning rate to 0.00028711035421746366.\n",
      "learning rate: 2.87e-04, weight decay: 1.44e-05\n",
      "Epoch 65/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 1.9994 - accuracy: 0.8771\n",
      "Epoch 65: val_accuracy did not improve from 0.67171\n",
      "590/590 [==============================] - 27s 46ms/step - loss: 1.9997 - accuracy: 0.8771 - val_loss: 2.9980 - val_accuracy: 0.6678 - lr: 2.8711e-04\n",
      "\n",
      "Epoch 66: LearningRateScheduler setting learning rate to 0.00027300475013022663.\n",
      "learning rate: 2.73e-04, weight decay: 1.37e-05\n",
      "Epoch 66/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 1.9708 - accuracy: 0.8841\n",
      "Epoch 66: val_accuracy did not improve from 0.67171\n",
      "590/590 [==============================] - 27s 46ms/step - loss: 1.9710 - accuracy: 0.8840 - val_loss: 2.9480 - val_accuracy: 0.6636 - lr: 2.7300e-04\n",
      "\n",
      "Epoch 67: LearningRateScheduler setting learning rate to 0.0002591231629491423.\n",
      "learning rate: 2.59e-04, weight decay: 1.30e-05\n",
      "Epoch 67/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 1.9390 - accuracy: 0.8913\n",
      "Epoch 67: val_accuracy improved from 0.67171 to 0.67765, saving model to models/weights_stgcn2.h5\n",
      "590/590 [==============================] - 27s 46ms/step - loss: 1.9390 - accuracy: 0.8913 - val_loss: 2.9197 - val_accuracy: 0.6777 - lr: 2.5912e-04\n",
      "\n",
      "Epoch 68: LearningRateScheduler setting learning rate to 0.00024547929212481435.\n",
      "learning rate: 2.45e-04, weight decay: 1.23e-05\n",
      "Epoch 68/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 1.9093 - accuracy: 0.8987\n",
      "Epoch 68: val_accuracy did not improve from 0.67765\n",
      "590/590 [==============================] - 27s 46ms/step - loss: 1.9092 - accuracy: 0.8987 - val_loss: 2.9785 - val_accuracy: 0.6699 - lr: 2.4548e-04\n",
      "\n",
      "Epoch 69: LearningRateScheduler setting learning rate to 0.00023208660251050156.\n",
      "learning rate: 2.32e-04, weight decay: 1.16e-05\n",
      "Epoch 69/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 1.8819 - accuracy: 0.9030\n",
      "Epoch 69: val_accuracy did not improve from 0.67765\n",
      "590/590 [==============================] - 27s 46ms/step - loss: 1.8819 - accuracy: 0.9030 - val_loss: 2.9564 - val_accuracy: 0.6754 - lr: 2.3209e-04\n",
      "\n",
      "Epoch 70: LearningRateScheduler setting learning rate to 0.0002189583110739348.\n",
      "learning rate: 2.19e-04, weight decay: 1.09e-05\n",
      "Epoch 70/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 1.8545 - accuracy: 0.9094\n",
      "Epoch 70: val_accuracy did not improve from 0.67765\n",
      "590/590 [==============================] - 27s 46ms/step - loss: 1.8545 - accuracy: 0.9094 - val_loss: 2.9824 - val_accuracy: 0.6636 - lr: 2.1896e-04\n",
      "\n",
      "Epoch 71: LearningRateScheduler setting learning rate to 0.00020610737385376348.\n",
      "learning rate: 2.06e-04, weight decay: 1.03e-05\n",
      "Epoch 71/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 1.8214 - accuracy: 0.9157\n",
      "Epoch 71: val_accuracy improved from 0.67765 to 0.67910, saving model to models/weights_stgcn2.h5\n",
      "590/590 [==============================] - 27s 46ms/step - loss: 1.8215 - accuracy: 0.9156 - val_loss: 2.9362 - val_accuracy: 0.6791 - lr: 2.0611e-04\n",
      "\n",
      "Epoch 72: LearningRateScheduler setting learning rate to 0.00019354647317351188.\n",
      "learning rate: 1.94e-04, weight decay: 9.68e-06\n",
      "Epoch 72/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 1.7897 - accuracy: 0.9231\n",
      "Epoch 72: val_accuracy improved from 0.67910 to 0.68172, saving model to models/weights_stgcn2.h5\n",
      "590/590 [==============================] - 27s 46ms/step - loss: 1.7896 - accuracy: 0.9231 - val_loss: 2.9007 - val_accuracy: 0.6817 - lr: 1.9355e-04\n",
      "\n",
      "Epoch 73: LearningRateScheduler setting learning rate to 0.00018128800512565513.\n",
      "learning rate: 1.81e-04, weight decay: 9.06e-06\n",
      "Epoch 73/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 1.7628 - accuracy: 0.9274\n",
      "Epoch 73: val_accuracy did not improve from 0.68172\n",
      "590/590 [==============================] - 27s 46ms/step - loss: 1.7632 - accuracy: 0.9274 - val_loss: 2.9573 - val_accuracy: 0.6734 - lr: 1.8129e-04\n",
      "\n",
      "Epoch 74: LearningRateScheduler setting learning rate to 0.00016934406733817414.\n",
      "learning rate: 1.69e-04, weight decay: 8.47e-06\n",
      "Epoch 74/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 1.7281 - accuracy: 0.9337\n",
      "Epoch 74: val_accuracy improved from 0.68172 to 0.68301, saving model to models/weights_stgcn2.h5\n",
      "590/590 [==============================] - 27s 46ms/step - loss: 1.7280 - accuracy: 0.9337 - val_loss: 2.8994 - val_accuracy: 0.6830 - lr: 1.6934e-04\n",
      "\n",
      "Epoch 75: LearningRateScheduler setting learning rate to 0.00015772644703565563.\n",
      "learning rate: 1.58e-04, weight decay: 7.89e-06\n",
      "Epoch 75/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 1.6978 - accuracy: 0.9389\n",
      "Epoch 75: val_accuracy did not improve from 0.68301\n",
      "590/590 [==============================] - 27s 46ms/step - loss: 1.6980 - accuracy: 0.9388 - val_loss: 2.9269 - val_accuracy: 0.6797 - lr: 1.5773e-04\n",
      "\n",
      "Epoch 76: LearningRateScheduler setting learning rate to 0.00014644660940672628.\n",
      "learning rate: 1.46e-04, weight decay: 7.32e-06\n",
      "Epoch 76/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 1.6678 - accuracy: 0.9444\n",
      "Epoch 76: val_accuracy improved from 0.68301 to 0.68787, saving model to models/weights_stgcn2.h5\n",
      "590/590 [==============================] - 27s 46ms/step - loss: 1.6677 - accuracy: 0.9444 - val_loss: 2.8300 - val_accuracy: 0.6879 - lr: 1.4645e-04\n",
      "\n",
      "Epoch 77: LearningRateScheduler setting learning rate to 0.00013551568628929433.\n",
      "learning rate: 1.36e-04, weight decay: 6.78e-06\n",
      "Epoch 77/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "589/590 [============================>.] - ETA: 0s - loss: 1.6357 - accuracy: 0.9495\n",
      "Epoch 77: val_accuracy did not improve from 0.68787\n",
      "590/590 [==============================] - 27s 46ms/step - loss: 1.6358 - accuracy: 0.9495 - val_loss: 2.9365 - val_accuracy: 0.6793 - lr: 1.3552e-04\n",
      "\n",
      "Epoch 78: LearningRateScheduler setting learning rate to 0.0001249444651847702.\n",
      "learning rate: 1.25e-04, weight decay: 6.25e-06\n",
      "Epoch 78/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 1.6053 - accuracy: 0.9545\n",
      "Epoch 78: val_accuracy did not improve from 0.68787\n",
      "590/590 [==============================] - 27s 46ms/step - loss: 1.6053 - accuracy: 0.9545 - val_loss: 2.8833 - val_accuracy: 0.6804 - lr: 1.2494e-04\n",
      "\n",
      "Epoch 79: LearningRateScheduler setting learning rate to 0.00011474337861210544.\n",
      "learning rate: 1.15e-04, weight decay: 5.74e-06\n",
      "Epoch 79/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 1.5769 - accuracy: 0.9579\n",
      "Epoch 79: val_accuracy did not improve from 0.68787\n",
      "590/590 [==============================] - 27s 46ms/step - loss: 1.5771 - accuracy: 0.9579 - val_loss: 2.8774 - val_accuracy: 0.6836 - lr: 1.1474e-04\n",
      "\n",
      "Epoch 80: LearningRateScheduler setting learning rate to 0.00010492249381215479.\n",
      "learning rate: 1.05e-04, weight decay: 5.25e-06\n",
      "Epoch 80/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 1.5482 - accuracy: 0.9620\n",
      "Epoch 80: val_accuracy did not improve from 0.68787\n",
      "590/590 [==============================] - 27s 46ms/step - loss: 1.5483 - accuracy: 0.9620 - val_loss: 2.8017 - val_accuracy: 0.6871 - lr: 1.0492e-04\n",
      "\n",
      "Epoch 81: LearningRateScheduler setting learning rate to 9.549150281252633e-05.\n",
      "learning rate: 9.55e-05, weight decay: 4.77e-06\n",
      "Epoch 81/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 1.5188 - accuracy: 0.9665\n",
      "Epoch 81: val_accuracy improved from 0.68787 to 0.69034, saving model to models/weights_stgcn2.h5\n",
      "590/590 [==============================] - 27s 46ms/step - loss: 1.5187 - accuracy: 0.9666 - val_loss: 2.8064 - val_accuracy: 0.6903 - lr: 9.5492e-05\n",
      "\n",
      "Epoch 82: LearningRateScheduler setting learning rate to 8.645971286271903e-05.\n",
      "learning rate: 8.65e-05, weight decay: 4.32e-06\n",
      "Epoch 82/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 1.4911 - accuracy: 0.9701\n",
      "Epoch 82: val_accuracy did not improve from 0.69034\n",
      "590/590 [==============================] - 27s 46ms/step - loss: 1.4912 - accuracy: 0.9700 - val_loss: 2.8028 - val_accuracy: 0.6825 - lr: 8.6460e-05\n",
      "\n",
      "Epoch 83: LearningRateScheduler setting learning rate to 7.783603724899258e-05.\n",
      "learning rate: 7.78e-05, weight decay: 3.89e-06\n",
      "Epoch 83/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 1.4636 - accuracy: 0.9730\n",
      "Epoch 83: val_accuracy improved from 0.69034 to 0.69167, saving model to models/weights_stgcn2.h5\n",
      "590/590 [==============================] - 27s 46ms/step - loss: 1.4636 - accuracy: 0.9730 - val_loss: 2.7937 - val_accuracy: 0.6917 - lr: 7.7836e-05\n",
      "\n",
      "Epoch 84: LearningRateScheduler setting learning rate to 6.962898649802824e-05.\n",
      "learning rate: 6.96e-05, weight decay: 3.48e-06\n",
      "Epoch 84/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 1.4374 - accuracy: 0.9757\n",
      "Epoch 84: val_accuracy did not improve from 0.69167\n",
      "590/590 [==============================] - 27s 46ms/step - loss: 1.4373 - accuracy: 0.9757 - val_loss: 2.8131 - val_accuracy: 0.6864 - lr: 6.9629e-05\n",
      "\n",
      "Epoch 85: LearningRateScheduler setting learning rate to 6.184665997806832e-05.\n",
      "learning rate: 6.18e-05, weight decay: 3.09e-06\n",
      "Epoch 85/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 1.4135 - accuracy: 0.9788\n",
      "Epoch 85: val_accuracy improved from 0.69167 to 0.69430, saving model to models/weights_stgcn2.h5\n",
      "590/590 [==============================] - 27s 46ms/step - loss: 1.4135 - accuracy: 0.9788 - val_loss: 2.7538 - val_accuracy: 0.6943 - lr: 6.1847e-05\n",
      "\n",
      "Epoch 86: LearningRateScheduler setting learning rate to 5.449673790581611e-05.\n",
      "learning rate: 5.45e-05, weight decay: 2.72e-06\n",
      "Epoch 86/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 1.3904 - accuracy: 0.9811\n",
      "Epoch 86: val_accuracy improved from 0.69430 to 0.69660, saving model to models/weights_stgcn2.h5\n",
      "590/590 [==============================] - 28s 47ms/step - loss: 1.3905 - accuracy: 0.9811 - val_loss: 2.7388 - val_accuracy: 0.6966 - lr: 5.4497e-05\n",
      "\n",
      "Epoch 87: LearningRateScheduler setting learning rate to 4.758647376699032e-05.\n",
      "learning rate: 4.76e-05, weight decay: 2.38e-06\n",
      "Epoch 87/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 1.3697 - accuracy: 0.9834\n",
      "Epoch 87: val_accuracy did not improve from 0.69660\n",
      "590/590 [==============================] - 29s 50ms/step - loss: 1.3697 - accuracy: 0.9834 - val_loss: 2.7115 - val_accuracy: 0.6938 - lr: 4.7586e-05\n",
      "\n",
      "Epoch 88: LearningRateScheduler setting learning rate to 4.112268715800943e-05.\n",
      "learning rate: 4.11e-05, weight decay: 2.06e-06\n",
      "Epoch 88/100\n",
      "589/590 [============================>.] - ETA: 0s - loss: 1.3483 - accuracy: 0.9850\n",
      "Epoch 88: val_accuracy did not improve from 0.69660\n",
      "590/590 [==============================] - 29s 48ms/step - loss: 1.3483 - accuracy: 0.9850 - val_loss: 2.7040 - val_accuracy: 0.6956 - lr: 4.1123e-05\n",
      "\n",
      "Epoch 89: LearningRateScheduler setting learning rate to 3.5111757055874326e-05.\n",
      "learning rate: 3.51e-05, weight decay: 1.76e-06\n",
      "Epoch 89/100\n",
      "590/590 [==============================] - ETA: 0s - loss: 1.3292 - accuracy: 0.9872\n",
      "Epoch 89: val_accuracy did not improve from 0.69660\n",
      "590/590 [==============================] - 29s 48ms/step - loss: 1.3292 - accuracy: 0.9872 - val_loss: 2.7095 - val_accuracy: 0.6935 - lr: 3.5112e-05\n",
      "\n",
      "Epoch 90: LearningRateScheduler setting learning rate to 2.9559615522887274e-05.\n",
      "learning rate: 2.96e-05, weight decay: 1.48e-06\n",
      "Epoch 90/100\n",
      "459/590 [======================>.......] - ETA: 5s - loss: 1.3128 - accuracy: 0.9885"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [25], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39msummary(expand_nested\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Actual Training\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Only used for validation data since training data is a generator\u001b[39;49;00m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mModelCheckpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfile_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m            \u001b[49m\u001b[43msave_weights_only\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m            \u001b[49m\u001b[43msave_best_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mval_accuracy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m            \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlr_callback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m            \u001b[49m\u001b[43mWeightDecayCallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py:1656\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1654\u001b[0m logs \u001b[38;5;241m=\u001b[39m tmp_logs\n\u001b[1;32m   1655\u001b[0m end_step \u001b[38;5;241m=\u001b[39m step \u001b[38;5;241m+\u001b[39m data_handler\u001b[38;5;241m.\u001b[39mstep_increment\n\u001b[0;32m-> 1656\u001b[0m \u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_train_batch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mend_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1657\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n\u001b[1;32m   1658\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py:476\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;124;03m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001b[39;00m\n\u001b[1;32m    470\u001b[0m \n\u001b[1;32m    471\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;124;03m    batch: Integer, index of batch within the current epoch.\u001b[39;00m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;124;03m    logs: Dict. Aggregated metric results up until this batch.\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_call_train_batch_hooks:\n\u001b[0;32m--> 476\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mModeKeys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTRAIN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py:323\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    321\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_batch_begin_hook(mode, batch, logs)\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m hook \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 323\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_end_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    326\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized hook: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected values are [\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbegin\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    328\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py:346\u001b[0m, in \u001b[0;36mCallbackList._call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    343\u001b[0m     batch_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_start_time\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times\u001b[38;5;241m.\u001b[39mappend(batch_time)\n\u001b[0;32m--> 346\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_batches_for_timing_check:\n\u001b[1;32m    349\u001b[0m     end_hook_name \u001b[38;5;241m=\u001b[39m hook_name\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py:394\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[1;32m    393\u001b[0m     hook \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(callback, hook_name)\n\u001b[0;32m--> 394\u001b[0m     \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timing:\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hook_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hook_times:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py:1094\u001b[0m, in \u001b[0;36mProgbarLogger.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1093\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_train_batch_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, logs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m-> 1094\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_update_progbar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py:1170\u001b[0m, in \u001b[0;36mProgbarLogger._batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1166\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m add_seen\n\u001b[1;32m   1168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1169\u001b[0m     \u001b[38;5;66;03m# Only block async when verbose = 1.\u001b[39;00m\n\u001b[0;32m-> 1170\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[43mtf_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msync_to_numpy_or_python_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1171\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprogbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen, \u001b[38;5;28mlist\u001b[39m(logs\u001b[38;5;241m.\u001b[39mitems()), finalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/tf_utils.py:665\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    662\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\n\u001b[1;32m    663\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(t) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m t\n\u001b[0;32m--> 665\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_single_numpy_or_python_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/nest.py:917\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    913\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[1;32m    914\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 917\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [func(\u001b[38;5;241m*\u001b[39mx) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[1;32m    918\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/nest.py:917\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    913\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[1;32m    914\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 917\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[1;32m    918\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/tf_utils.py:658\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type.<locals>._to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    655\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_to_single_numpy_or_python_type\u001b[39m(t):\n\u001b[1;32m    656\u001b[0m     \u001b[38;5;66;03m# Don't turn ragged or sparse tensors to NumPy.\u001b[39;00m\n\u001b[1;32m    657\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, tf\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 658\u001b[0m         t \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    659\u001b[0m     \u001b[38;5;66;03m# Strings, ragged and sparse tensors don't have .item(). Return them\u001b[39;00m\n\u001b[1;32m    660\u001b[0m     \u001b[38;5;66;03m# as-is.\u001b[39;00m\n\u001b[1;32m    661\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, (np\u001b[38;5;241m.\u001b[39mndarray, np\u001b[38;5;241m.\u001b[39mgeneric)):\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:1155\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;124;03m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[1;32m   1133\u001b[0m \n\u001b[1;32m   1134\u001b[0m \u001b[38;5;124;03mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1152\u001b[0m \u001b[38;5;124;03m    NumPy dtype.\u001b[39;00m\n\u001b[1;32m   1153\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1154\u001b[0m \u001b[38;5;66;03m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[0;32m-> 1155\u001b[0m maybe_arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1156\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m maybe_arr\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(maybe_arr, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;28;01melse\u001b[39;00m maybe_arr\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:1121\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_numpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1120\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1122\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1123\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Get new fresh model\n",
    "file_name = 'models/weights_stgcn2.h5'\n",
    "#model = tf.keras.models.load_model('models/041423_21_02.h5')\n",
    "\n",
    "# Sanity Check\n",
    "model.summary(expand_nested=True)\n",
    "\n",
    "# Actual Training\n",
    "history = model.fit(\n",
    "        x=x_train,\n",
    "        y=y_train,\n",
    "        epochs=100,\n",
    "        # Only used for validation data since training data is a generator\n",
    "        batch_size=128,\n",
    "        validation_data=(x_test,y_test),\n",
    "        callbacks=[\n",
    "            tf.keras.callbacks.ModelCheckpoint(\n",
    "            file_name,\n",
    "            save_weights_only = True,\n",
    "            save_best_only=True, \n",
    "            monitor=\"val_accuracy\",\n",
    "            mode=\"max\",\n",
    "            verbose = 1),\n",
    "            lr_callback,\n",
    "            WeightDecayCallback(),\n",
    "        ],\n",
    "        verbose = 1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-11T13:19:17.068946Z",
     "iopub.status.busy": "2023-04-11T13:19:17.067991Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_ds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [46], line 12\u001b[0m\n\u001b[1;32m      7\u001b[0m loss_fn\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mSparseCategoricalCrossentropy()\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m30\u001b[39m): \u001b[38;5;66;03m# <----- start for loop, step 1\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \n\u001b[1;32m     10\u001b[0m   \u001b[38;5;66;03m# <-------- start for loop, step 2\u001b[39;00m\n\u001b[1;32m     11\u001b[0m   \u001b[38;5;66;03m# Iterate over the batches of the dataset.\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m step, (x_batch_train, y_batch_train) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mtrain_ds\u001b[49m):\n\u001b[1;32m     13\u001b[0m \n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# <-------- start gradient tape scope, step 3\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# Open a GradientTape to record the operations run\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# during the forward pass, which enables auto-differentiation.\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[1;32m     18\u001b[0m \n\u001b[1;32m     19\u001b[0m        \u001b[38;5;66;03m# Run the forward pass of the layer.\u001b[39;00m\n\u001b[1;32m     20\u001b[0m        \u001b[38;5;66;03m# The operations that the layer applies\u001b[39;00m\n\u001b[1;32m     21\u001b[0m        \u001b[38;5;66;03m# to its inputs are going to be recorded\u001b[39;00m\n\u001b[1;32m     22\u001b[0m        \u001b[38;5;66;03m# on the GradientTape.\u001b[39;00m\n\u001b[1;32m     23\u001b[0m        logits \u001b[38;5;241m=\u001b[39m model(x_batch_train, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_ds' is not defined"
     ]
    }
   ],
   "source": [
    "# https://towardsdatascience.com/model-sub-classing-and-custom-training-loop-from-scratch-in-tensorflow-2-cc1d4f10fb4e\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "train_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
    "val_acc_metric   = tf.keras.metrics.CategoricalAccuracy()\n",
    "# Instantiate a loss function\n",
    "# loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "loss_fn=tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "for epoch in range(30): # <----- start for loop, step 1\n",
    "\n",
    "  # <-------- start for loop, step 2\n",
    "  # Iterate over the batches of the dataset.\n",
    "  for step, (x_batch_train, y_batch_train) in enumerate(train_ds):\n",
    "\n",
    "    # <-------- start gradient tape scope, step 3\n",
    "    # Open a GradientTape to record the operations run\n",
    "    # during the forward pass, which enables auto-differentiation.\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "       # Run the forward pass of the layer.\n",
    "       # The operations that the layer applies\n",
    "       # to its inputs are going to be recorded\n",
    "       # on the GradientTape.\n",
    "       logits = model(x_batch_train, training=True) \n",
    "\n",
    "       # Compute the loss value for this minibatch.\n",
    "       loss_value = loss_fn(y_batch_train, logits)  \n",
    "       print(loss_value )\n",
    "\n",
    "    # compute the gradient of weights w.r.t. loss  <-------- step 5\n",
    "    # Use the gradient tape to automatically retrieve\n",
    "    # the gradients of the trainable variables with respect to the loss.\n",
    "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "\n",
    "    # update the weight based on gradient  <---------- step 6\n",
    "    # Run one step of gradient descent by updating\n",
    "    # the value of the variables to minimize the loss.\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    train_acc_metric.update_state(y_batch_train, logits)\n",
    "    print(train_acc_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 20, 61, 75520, 1)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-11T12:58:58.615002Z",
     "iopub.status.busy": "2023-04-11T12:58:58.614684Z",
     "iopub.status.idle": "2023-04-11T13:00:08.746327Z",
     "shell.execute_reply": "2023-04-11T13:00:08.744036Z",
     "shell.execute_reply.started": "2023-04-11T12:58:58.614967Z"
    }
   },
   "outputs": [],
   "source": [
    "if CFG.is_training:\n",
    "    file_name = \"model.h5\"\n",
    "#     callbacks = [\n",
    "#         tf.keras.callbacks.ModelCheckpoint(\n",
    "#             file_name, \n",
    "#             save_best_only=True, \n",
    "#             restore_best_weights=True, \n",
    "#             monitor=\"val_accuracy\",\n",
    "#             mode=\"max\"\n",
    "#         ),\n",
    "#         tf.keras.callbacks.EarlyStopping(\n",
    "#             patience=5, \n",
    "#             monitor=\"val_accuracy\",\n",
    "#             mode=\"max\"\n",
    "#         )\n",
    "#     ]\n",
    "    model.fit(train_ds, epochs=1, validation_data=valid_ds)\n",
    "    model.save('/kaggle/input/islr-convlstm1d/model.h5',save_format='tf')\n",
    "    model = tf.keras.models.load_model(file_name)\n",
    "# else:\n",
    "#     model = tf.keras.models.load_model(\"/kaggle/input/islr-convlstm1d/model.h5\")\n",
    "model.evaluate(valid_ds)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.069654,
     "end_time": "2023-03-02T08:47:03.916954",
     "exception": false,
     "start_time": "2023-03-02T08:47:03.8473",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Create Model for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-11T13:00:08.747883Z",
     "iopub.status.idle": "2023-04-11T13:00:08.748454Z",
     "shell.execute_reply": "2023-04-11T13:00:08.748205Z",
     "shell.execute_reply.started": "2023-04-11T13:00:08.748177Z"
    },
    "papermill": {
     "duration": 0.086334,
     "end_time": "2023-03-02T08:47:04.072776",
     "exception": false,
     "start_time": "2023-03-02T08:47:03.986442",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def get_inference_model(model):\n",
    "#     inputs = tf.keras.Input((543, 3), dtype=tf.float32, name=\"inputs\")\n",
    "#     vector = tf.image.resize(inputs, (CFG.sequence_length, 543))\n",
    "#     vector = tf.where(tf.math.is_nan(vector), tf.zeros_like(vector), vector)\n",
    "#     vector = tf.expand_dims(vector, axis=0)\n",
    "#     vector = model(vector)\n",
    "#     output = tf.keras.layers.Activation(activation=\"linear\", name=\"outputs\")(vector)\n",
    "#     inference_model = tf.keras.Model(inputs=inputs, outputs=output) \n",
    "#     inference_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics=[\"accuracy\"])\n",
    "#     return inference_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-11T13:00:08.750352Z",
     "iopub.status.idle": "2023-04-11T13:00:08.750858Z",
     "shell.execute_reply": "2023-04-11T13:00:08.750633Z",
     "shell.execute_reply.started": "2023-04-11T13:00:08.750603Z"
    },
    "papermill": {
     "duration": 5.194577,
     "end_time": "2023-03-02T08:47:09.336507",
     "exception": false,
     "start_time": "2023-03-02T08:47:04.14193",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# inference_model = get_inference_model(model)\n",
    "# inference_model.summary()\n",
    "# tf.keras.utils.plot_model(inference_model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.071953,
     "end_time": "2023-03-02T08:47:09.784691",
     "exception": false,
     "start_time": "2023-03-02T08:47:09.712738",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Create submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-11T13:00:08.752458Z",
     "iopub.status.idle": "2023-04-11T13:00:08.758015Z",
     "shell.execute_reply": "2023-04-11T13:00:08.757844Z",
     "shell.execute_reply.started": "2023-04-11T13:00:08.757823Z"
    },
    "papermill": {
     "duration": 153.429402,
     "end_time": "2023-03-02T08:49:43.286145",
     "exception": false,
     "start_time": "2023-03-02T08:47:09.856743",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# converter = tf.lite.TFLiteConverter.from_keras_model(inference_model)\n",
    "# tflite_model = converter.convert()\n",
    "# model_path = \"model.tflite\"\n",
    "# # Save the model.\n",
    "# with open(model_path, 'wb') as f:\n",
    "#     f.write(tflite_model)\n",
    "# !zip submission.zip $model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.1124,
     "end_time": "2023-03-02T08:49:46.762554",
     "exception": false,
     "start_time": "2023-03-02T08:49:46.650154",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-11T13:00:08.759302Z",
     "iopub.status.idle": "2023-04-11T13:00:08.759928Z",
     "shell.execute_reply": "2023-04-11T13:00:08.759646Z",
     "shell.execute_reply.started": "2023-04-11T13:00:08.759618Z"
    },
    "papermill": {
     "duration": 12.663338,
     "end_time": "2023-03-02T08:49:59.538473",
     "exception": false,
     "start_time": "2023-03-02T08:49:46.875135",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install tflite-runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-11T13:00:08.761722Z",
     "iopub.status.idle": "2023-04-11T13:00:08.762695Z",
     "shell.execute_reply": "2023-04-11T13:00:08.762418Z",
     "shell.execute_reply.started": "2023-04-11T13:00:08.762388Z"
    },
    "papermill": {
     "duration": 29.201279,
     "end_time": "2023-03-02T08:50:28.816671",
     "exception": false,
     "start_time": "2023-03-02T08:49:59.615392",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import tflite_runtime.interpreter as tflite\n",
    "# interpreter = tflite.Interpreter(model_path)\n",
    "# found_signatures = list(interpreter.get_signature_list().keys())\n",
    "# prediction_fn = interpreter.get_signature_runner(\"serving_default\")\n",
    "# for i in tqdm(range(10000)):\n",
    "#     frames = load_relevant_data_subset(f'/kaggle/input/asl-signs/{train.iloc[i].path}')\n",
    "#     output = prediction_fn(inputs=frames)\n",
    "#     if i < 100:\n",
    "#         sign = np.argmax(output[\"outputs\"])\n",
    "#         print(f\"Predicted label: {index_label[sign]}, Actual Label: {train.iloc[i].sign}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
