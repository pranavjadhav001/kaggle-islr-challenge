{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "from focal_loss import SparseCategoricalFocalLoss\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import tensorflow_io as tfio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_it_all(seed=7):\n",
    "    \"\"\" Attempt to be Reproducible \"\"\"\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "seed_it_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROWS_PER_FRAME = 543\n",
    "def load_relevant_data_subset(pq_path):\n",
    "    data_columns = ['x', 'y', 'z']\n",
    "    data = pd.read_parquet(pq_path, columns=data_columns)\n",
    "    n_frames = int(len(data) / ROWS_PER_FRAME)\n",
    "    data = data.values.reshape(n_frames, ROWS_PER_FRAME, len(data_columns))\n",
    "    return data.astype(np.float32)\n",
    "\n",
    "#test_data  = load_relevant_data_subset('train_landmark_files/16069/100015657.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIP = [\n",
    "            61, 185, 40, 39, 37, 0, 267, 269, 270, 409,\n",
    "            291, 146, 91, 181, 84, 17, 314, 405, 321, 375,\n",
    "            78, 191, 80, 81, 82, 13, 312, 311, 310, 415,\n",
    "            95, 88, 178, 87, 14, 317, 402, 318, 324, 308,\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    left_ROWS_per_frame = 21\n",
    "    sequence_length = 20\n",
    "    batch_size = 32\n",
    "    face_ROWS_per_frame = 468\n",
    "    lip_ROWS_per_frame = 40\n",
    "\n",
    "labels  = json.load(open('sign_to_prediction_index_map.json','r'))\n",
    "complete_df = pd.read_csv('train.csv')\n",
    "from sklearn.model_selection import train_test_split\n",
    "y = complete_df['sign']\n",
    "train_df, test_df = train_test_split(complete_df, test_size=0.2,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_loader(with_labels=True):\n",
    "    def load_video(video_path):\n",
    "        #print('herer')\n",
    "        video_df = tfio.IODataset.from_parquet(video_path)\n",
    "        #video_df = pd.read_parquet(video_path, engine='pyarrow')\n",
    "        #video_df.fillna(0,inplace=True)\n",
    "        left_df = video_df[video_df.type=='left_hand']\n",
    "        left_values = left_df[['x','y','z']].values\n",
    "        left_values = left_values.reshape(-1,CFG.left_ROWS_per_frame,3)\n",
    "        left_hand_array =  tf.image.resize(left_values, (CFG.sequence_length, CFG.left_ROWS_per_frame))\n",
    "        right_df = video_df[video_df.type=='right_hand']\n",
    "        right_values = right_df[['x','y','z']].values\n",
    "        right_values = right_values.reshape(-1,CFG.left_ROWS_per_frame,3)\n",
    "        right_hand_array =  tf.image.resize(right_values, (CFG.sequence_length, CFG.left_ROWS_per_frame))\n",
    "        return [left_hand_array, right_hand_array]\n",
    "    \n",
    "    def load_video_with_labels(path, label):\n",
    "        return load_video(path), labels[label]\n",
    "    \n",
    "    return load_video_with_labels if with_labels else load_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomData(tf.keras.utils.Sequence):\n",
    "    def __init__(self,df,num_frames=20,batch_size=8,shuffle=True,\\\n",
    "                 labels_path='sign_to_prediction_index_map.json'):\n",
    "        self.df = df\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.num_frames = num_frames\n",
    "        self.labels  = json.load(open('sign_to_prediction_index_map.json','r'))\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.df))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        batches = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        left_hand_input = np.zeros(shape=(self.batch_size,self.num_frames,CFG.left_ROWS_per_frame,2))\n",
    "        right_hand_input = np.zeros(shape=(self.batch_size,self.num_frames,CFG.left_ROWS_per_frame,2))\n",
    "        lip_input = np.zeros(shape=(self.batch_size,self.num_frames,CFG.lip_ROWS_per_frame,2))\n",
    "        labels = []\n",
    "        for i,row_val in enumerate(batches):\n",
    "            row = self.df.iloc[row_val]\n",
    "            left_hand,right_hand,lip = self.load_video(row['path'])\n",
    "            left_hand_input[i,:] = left_hand\n",
    "            right_hand_input[i,:] = right_hand\n",
    "            lip_input[i,:] = lip\n",
    "            labels.append(self.labels[row['sign']])\n",
    "        return [left_hand_input,right_hand_input,lip_input],np.asarray(labels)\n",
    "            \n",
    "    def load_video(self,video_path):\n",
    "        video_df = pd.read_parquet(video_path, engine='pyarrow')\n",
    "        video_df.dropna(inplace=True)\n",
    "        left_df = video_df[video_df.type=='left_hand']\n",
    "        left_values = left_df[['x','y']].values\n",
    "        left_values = left_values.reshape(-1,CFG.left_ROWS_per_frame,2)\n",
    "        if len(left_values)!=0:\n",
    "            left_values[:,:,0] = (left_values[:,:,0]- np.min(left_values[:,:,0]))/(left_values[:,:,0].max()- left_values[:,:,0].min())\n",
    "            left_values[:,:,1] = (left_values[:,:,1]- np.min(left_values[:,:,1]))/(left_values[:,:,1].max()- left_values[:,:,1].min())\n",
    "            left_hand_array =  tf.image.resize(left_values, (CFG.sequence_length, CFG.left_ROWS_per_frame))\n",
    "        else:\n",
    "            left_hand_array =  tf.zeros(shape=(CFG.sequence_length, CFG.left_ROWS_per_frame,2),dtype=tf.float32)\n",
    "        right_df = video_df[video_df.type=='right_hand']\n",
    "        right_values = right_df[['x','y']].values\n",
    "        right_values = right_values.reshape(-1,CFG.left_ROWS_per_frame,2)\n",
    "        if len(right_values) != 0:\n",
    "            right_values[:,:,0] = (right_values[:,:,0]- np.min(right_values[:,:,0]))/(right_values[:,:,0].max()- right_values[:,:,0].min())\n",
    "            right_values[:,:,1] = (right_values[:,:,1]- np.min(right_values[:,:,1]))/(right_values[:,:,1].max()- right_values[:,:,1].min())\n",
    "            right_hand_array =  tf.image.resize(right_values, (CFG.sequence_length, CFG.left_ROWS_per_frame))\n",
    "        else:\n",
    "            right_hand_array =  tf.zeros(shape=(CFG.sequence_length, CFG.left_ROWS_per_frame,2),dtype=tf.float32)\n",
    "        face_df = video_df[video_df.type=='face']\n",
    "        face_df = face_df[['x','y']].values\n",
    "        face_df = face_df.reshape(-1,CFG.face_ROWS_per_frame,2)\n",
    "        lip_values = face_df[:,LIP,:]\n",
    "        if len(lip_values) != 0:\n",
    "            lip_values[:,:,0] = (lip_values[:,:,0]- np.min(lip_values[:,:,0]))/(lip_values[:,:,0].max()- lip_values[:,:,0].min())\n",
    "            lip_values[:,:,1] = (lip_values[:,:,1]- np.min(lip_values[:,:,1]))/(lip_values[:,:,1].max()- lip_values[:,:,1].min())\n",
    "            lip_values_array =  tf.image.resize(lip_values, (CFG.sequence_length, CFG.lip_ROWS_per_frame))\n",
    "        else:\n",
    "            lip_values_array =  tf.zeros(shape=(CFG.sequence_length, CFG.lip_ROWS_per_frame,2),dtype=tf.float32)\n",
    "        return left_hand_array, right_hand_array,lip_values_array\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)//self.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = CustomData(train_df,num_frames=CFG.sequence_length,batch_size=CFG.batch_size)\n",
    "test_datagen = CustomData(test_df,num_frames=CFG.sequence_length,batch_size=CFG.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv1d_lstm_block(inputs, filters):\n",
    "    vector = tf.keras.layers.BatchNormalization()(inputs)\n",
    "    vector = tf.keras.layers.Conv2D(filters=32, kernel_size=3)(vector)\n",
    "    vector = tf.keras.layers.MaxPool2D()(vector)\n",
    "    vector = tf.keras.layers.BatchNormalization()(vector)\n",
    "    vector = tf.keras.layers.Conv2D(filters=64, kernel_size=3)(vector)\n",
    "    vector = tf.keras.layers.MaxPool2D()(vector)\n",
    "    vector = tf.keras.layers.BatchNormalization()(vector)\n",
    "    vector = tf.keras.layers.Conv2D(filters=64, kernel_size=3)(vector)\n",
    "    vector = tf.keras.layers.Flatten()(vector)\n",
    "#     #vector = tf.keras.layers.Dropout(0.2)(vector)\n",
    "#     vector = tf.keras.layers.BatchNormalization(axis=-1)(vector)\n",
    "#     vector = tf.keras.layers.ConvLSTM1D(filters=64, kernel_size=3,return_sequences=True,padding='same',\\\n",
    "#                                         kernel_regularizer=tf.keras.regularizers.L2(l2=0.01),\\\n",
    "#                                         recurrent_regularizer=tf.keras.regularizers.L2(l2=0.01))(vector)\n",
    "#     #vector = tf.keras.layers.Dropout(0.2)(vector)\n",
    "#     vector = tf.keras.layers.BatchNormalization(axis=-1)(vector)\n",
    "#     vector = tf.keras.layers.ConvLSTM1D(filters=64, kernel_size=3,padding='same',\\\n",
    "#                                        kernel_regularizer=tf.keras.regularizers.L2(l2=0.01),\\\n",
    "#                                         recurrent_regularizer=tf.keras.regularizers.L2(l2=0.01))(vector)\n",
    "#     #vector = tf.keras.layers.Dropout(0.2)(vector)\n",
    "    return vector\n",
    "\n",
    "def get_model():\n",
    "    input1 = tf.keras.Input((CFG.sequence_length, CFG.left_ROWS_per_frame, 2), dtype=tf.float32)\n",
    "    input2 = tf.keras.Input((CFG.sequence_length, CFG.left_ROWS_per_frame, 2), dtype=tf.float32)\n",
    "    input3 = tf.keras.Input((CFG.sequence_length, CFG.lip_ROWS_per_frame, 2), dtype=tf.float32)\n",
    "    left_hand_vector = conv1d_lstm_block(input1, [64])\n",
    "    right_hand_vector = conv1d_lstm_block(input2, [64])\n",
    "    lip_vector = conv1d_lstm_block(input3, [64])\n",
    "    vector = tf.keras.layers.Concatenate(axis=1)([left_hand_vector, right_hand_vector,lip_vector])\n",
    "    vector = tf.keras.layers.Flatten()(vector)\n",
    "    vector = tf.keras.layers.Dense(512, activation=\"relu\")(vector)\n",
    "    vector = tf.keras.layers.Dropout(0.3)(vector)\n",
    "    output = tf.keras.layers.Dense(250, activation=\"softmax\")(vector)\n",
    "    model = tf.keras.Model(inputs=[input1,input2,input3], outputs=output)\n",
    "    model.compile(tf.keras.optimizers.Adam(0.000333),loss=SparseCategoricalFocalLoss(gamma=2), metrics=\"accuracy\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_16 (InputLayer)          [(None, 20, 21, 2)]  0           []                               \n",
      "                                                                                                  \n",
      " input_17 (InputLayer)          [(None, 20, 21, 2)]  0           []                               \n",
      "                                                                                                  \n",
      " input_18 (InputLayer)          [(None, 20, 40, 2)]  0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 20, 21, 2)   8           ['input_16[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_14 (BatchN  (None, 20, 21, 2)   8           ['input_17[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_17 (BatchN  (None, 20, 40, 2)   8           ['input_18[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)             (None, 18, 19, 32)   608         ['batch_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)             (None, 18, 19, 32)   608         ['batch_normalization_14[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_21 (Conv2D)             (None, 18, 38, 32)   608         ['batch_normalization_17[0][0]'] \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 9, 9, 32)     0           ['conv2d_15[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPooling2D)  (None, 9, 9, 32)    0           ['conv2d_18[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling2d_4 (MaxPooling2D)  (None, 9, 19, 32)   0           ['conv2d_21[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 9, 9, 32)    128         ['max_pooling2d[0][0]']          \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_15 (BatchN  (None, 9, 9, 32)    128         ['max_pooling2d_2[0][0]']        \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_18 (BatchN  (None, 9, 19, 32)   128         ['max_pooling2d_4[0][0]']        \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)             (None, 7, 7, 64)     18496       ['batch_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)             (None, 7, 7, 64)     18496       ['batch_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_22 (Conv2D)             (None, 7, 17, 64)    18496       ['batch_normalization_18[0][0]'] \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 3, 3, 64)    0           ['conv2d_16[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling2d_3 (MaxPooling2D)  (None, 3, 3, 64)    0           ['conv2d_19[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling2d_5 (MaxPooling2D)  (None, 3, 8, 64)    0           ['conv2d_22[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_13 (BatchN  (None, 3, 3, 64)    256         ['max_pooling2d_1[0][0]']        \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_16 (BatchN  (None, 3, 3, 64)    256         ['max_pooling2d_3[0][0]']        \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_19 (BatchN  (None, 3, 8, 64)    256         ['max_pooling2d_5[0][0]']        \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)             (None, 1, 1, 64)     36928       ['batch_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_20 (Conv2D)             (None, 1, 1, 64)     36928       ['batch_normalization_16[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_23 (Conv2D)             (None, 1, 6, 64)     36928       ['batch_normalization_19[0][0]'] \n",
      "                                                                                                  \n",
      " flatten_5 (Flatten)            (None, 64)           0           ['conv2d_17[0][0]']              \n",
      "                                                                                                  \n",
      " flatten_6 (Flatten)            (None, 64)           0           ['conv2d_20[0][0]']              \n",
      "                                                                                                  \n",
      " flatten_7 (Flatten)            (None, 384)          0           ['conv2d_23[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 512)          0           ['flatten_5[0][0]',              \n",
      "                                                                  'flatten_6[0][0]',              \n",
      "                                                                  'flatten_7[0][0]']              \n",
      "                                                                                                  \n",
      " flatten_8 (Flatten)            (None, 512)          0           ['concatenate_2[0][0]']          \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 512)          262656      ['flatten_8[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 512)          0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 250)          128250      ['dropout_1[0][0]']              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 560,178\n",
      "Trainable params: 559,590\n",
      "Non-trainable params: 588\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 3.7010 - accuracy: 0.1834\n",
      "Epoch 1: val_accuracy improved from -inf to 0.39518, saving model to models/032623_15_16.h5\n",
      "2361/2361 [==============================] - 1326s 559ms/step - loss: 3.7010 - accuracy: 0.1834 - val_loss: 2.3931 - val_accuracy: 0.3952 - lr: 3.3300e-04\n",
      "Epoch 2/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 2.3426 - accuracy: 0.3846\n",
      "Epoch 2: val_accuracy improved from 0.39518 to 0.47850, saving model to models/032623_15_16.h5\n",
      "2361/2361 [==============================] - 1274s 540ms/step - loss: 2.3426 - accuracy: 0.3846 - val_loss: 1.9539 - val_accuracy: 0.4785 - lr: 3.3300e-04\n",
      "Epoch 3/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 1.9298 - accuracy: 0.4642\n",
      "Epoch 3: val_accuracy improved from 0.47850 to 0.51849, saving model to models/032623_15_16.h5\n",
      "2361/2361 [==============================] - 1276s 540ms/step - loss: 1.9298 - accuracy: 0.4642 - val_loss: 1.7502 - val_accuracy: 0.5185 - lr: 3.3300e-04\n",
      "Epoch 4/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 1.7010 - accuracy: 0.5119\n",
      "Epoch 4: val_accuracy improved from 0.51849 to 0.54894, saving model to models/032623_15_16.h5\n",
      "2361/2361 [==============================] - 1260s 534ms/step - loss: 1.7010 - accuracy: 0.5119 - val_loss: 1.6039 - val_accuracy: 0.5489 - lr: 3.3300e-04\n",
      "Epoch 5/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 1.5388 - accuracy: 0.5451\n",
      "Epoch 5: val_accuracy improved from 0.54894 to 0.57246, saving model to models/032623_15_16.h5\n",
      "2361/2361 [==============================] - 1258s 533ms/step - loss: 1.5388 - accuracy: 0.5451 - val_loss: 1.5424 - val_accuracy: 0.5725 - lr: 3.3300e-04\n",
      "Epoch 6/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 1.4155 - accuracy: 0.5733\n",
      "Epoch 6: val_accuracy improved from 0.57246 to 0.57564, saving model to models/032623_15_16.h5\n",
      "2361/2361 [==============================] - 1256s 532ms/step - loss: 1.4155 - accuracy: 0.5733 - val_loss: 1.5170 - val_accuracy: 0.5756 - lr: 3.3300e-04\n",
      "Epoch 7/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 1.3198 - accuracy: 0.5927\n",
      "Epoch 7: val_accuracy improved from 0.57564 to 0.59560, saving model to models/032623_15_16.h5\n",
      "2361/2361 [==============================] - 1267s 537ms/step - loss: 1.3198 - accuracy: 0.5927 - val_loss: 1.4290 - val_accuracy: 0.5956 - lr: 3.3300e-04\n",
      "Epoch 8/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 1.2435 - accuracy: 0.6088\n",
      "Epoch 8: val_accuracy improved from 0.59560 to 0.59952, saving model to models/032623_15_16.h5\n",
      "2361/2361 [==============================] - 1267s 537ms/step - loss: 1.2435 - accuracy: 0.6088 - val_loss: 1.4170 - val_accuracy: 0.5995 - lr: 3.3300e-04\n",
      "Epoch 9/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 1.1672 - accuracy: 0.6256\n",
      "Epoch 9: val_accuracy improved from 0.59952 to 0.60805, saving model to models/032623_15_16.h5\n",
      "2361/2361 [==============================] - 1259s 533ms/step - loss: 1.1672 - accuracy: 0.6256 - val_loss: 1.4000 - val_accuracy: 0.6081 - lr: 3.3300e-04\n",
      "Epoch 10/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 1.1127 - accuracy: 0.6382\n",
      "Epoch 10: val_accuracy did not improve from 0.60805\n",
      "2361/2361 [==============================] - 1264s 536ms/step - loss: 1.1127 - accuracy: 0.6382 - val_loss: 1.4164 - val_accuracy: 0.6064 - lr: 3.3300e-04\n",
      "Epoch 11/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 1.0558 - accuracy: 0.6485\n",
      "Epoch 11: val_accuracy improved from 0.60805 to 0.61271, saving model to models/032623_15_16.h5\n",
      "2361/2361 [==============================] - 1267s 537ms/step - loss: 1.0558 - accuracy: 0.6485 - val_loss: 1.3897 - val_accuracy: 0.6127 - lr: 3.3300e-04\n",
      "Epoch 12/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 1.0118 - accuracy: 0.6586\n",
      "Epoch 12: val_accuracy improved from 0.61271 to 0.62023, saving model to models/032623_15_16.h5\n",
      "2361/2361 [==============================] - 1266s 536ms/step - loss: 1.0118 - accuracy: 0.6586 - val_loss: 1.3807 - val_accuracy: 0.6202 - lr: 3.3300e-04\n",
      "Epoch 13/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 0.9679 - accuracy: 0.6683\n",
      "Epoch 13: val_accuracy improved from 0.62023 to 0.62897, saving model to models/032623_15_16.h5\n",
      "2361/2361 [==============================] - 1265s 536ms/step - loss: 0.9679 - accuracy: 0.6683 - val_loss: 1.3715 - val_accuracy: 0.6290 - lr: 3.3300e-04\n",
      "Epoch 14/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 0.9350 - accuracy: 0.6741\n",
      "Epoch 14: val_accuracy improved from 0.62897 to 0.63051, saving model to models/032623_15_16.h5\n",
      "2361/2361 [==============================] - 1266s 536ms/step - loss: 0.9350 - accuracy: 0.6741 - val_loss: 1.3600 - val_accuracy: 0.6305 - lr: 3.3300e-04\n",
      "Epoch 15/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 0.8931 - accuracy: 0.6857\n",
      "Epoch 15: val_accuracy did not improve from 0.63051\n",
      "2361/2361 [==============================] - 1269s 537ms/step - loss: 0.8931 - accuracy: 0.6857 - val_loss: 1.3898 - val_accuracy: 0.6246 - lr: 3.3300e-04\n",
      "Epoch 16/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 0.8664 - accuracy: 0.6917\n",
      "Epoch 16: val_accuracy did not improve from 0.63051\n",
      "2361/2361 [==============================] - 1287s 545ms/step - loss: 0.8664 - accuracy: 0.6917 - val_loss: 1.3810 - val_accuracy: 0.6288 - lr: 3.3300e-04\n",
      "Epoch 17/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 0.8364 - accuracy: 0.6963\n",
      "Epoch 17: val_accuracy improved from 0.63051 to 0.63607, saving model to models/032623_15_16.h5\n",
      "2361/2361 [==============================] - 1279s 542ms/step - loss: 0.8364 - accuracy: 0.6963 - val_loss: 1.3716 - val_accuracy: 0.6361 - lr: 3.3300e-04\n",
      "Epoch 18/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 0.8076 - accuracy: 0.7047\n",
      "Epoch 18: val_accuracy did not improve from 0.63607\n",
      "2361/2361 [==============================] - 1268s 537ms/step - loss: 0.8076 - accuracy: 0.7047 - val_loss: 1.4161 - val_accuracy: 0.6306 - lr: 3.3300e-04\n",
      "Epoch 19/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 0.7830 - accuracy: 0.7103\n",
      "Epoch 19: val_accuracy did not improve from 0.63607\n",
      "2361/2361 [==============================] - 1266s 536ms/step - loss: 0.7830 - accuracy: 0.7103 - val_loss: 1.3862 - val_accuracy: 0.6322 - lr: 3.3300e-04\n",
      "Epoch 20/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 0.7589 - accuracy: 0.7158\n",
      "Epoch 20: val_accuracy did not improve from 0.63607\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 3.330000035930425e-05.\n",
      "2361/2361 [==============================] - 1268s 537ms/step - loss: 0.7589 - accuracy: 0.7158 - val_loss: 1.3742 - val_accuracy: 0.6352 - lr: 3.3300e-04\n",
      "Epoch 21/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 0.5515 - accuracy: 0.7806\n",
      "Epoch 21: val_accuracy improved from 0.63607 to 0.66388, saving model to models/032623_15_16.h5\n",
      "2361/2361 [==============================] - 1268s 537ms/step - loss: 0.5515 - accuracy: 0.7806 - val_loss: 1.3088 - val_accuracy: 0.6639 - lr: 3.3300e-05\n",
      "Epoch 22/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 0.4992 - accuracy: 0.7976\n",
      "Epoch 22: val_accuracy improved from 0.66388 to 0.66483, saving model to models/032623_15_16.h5\n",
      "2361/2361 [==============================] - 1263s 535ms/step - loss: 0.4992 - accuracy: 0.7976 - val_loss: 1.3045 - val_accuracy: 0.6648 - lr: 3.3300e-05\n",
      "Epoch 23/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 0.4756 - accuracy: 0.8041\n",
      "Epoch 23: val_accuracy improved from 0.66483 to 0.66690, saving model to models/032623_15_16.h5\n",
      "2361/2361 [==============================] - 1265s 536ms/step - loss: 0.4756 - accuracy: 0.8041 - val_loss: 1.3120 - val_accuracy: 0.6669 - lr: 3.3300e-05\n",
      "Epoch 24/30\n",
      "2361/2361 [==============================] - ETA: 0s - loss: 0.4636 - accuracy: 0.8074\n",
      "Epoch 24: val_accuracy did not improve from 0.66690\n",
      "2361/2361 [==============================] - 1272s 539ms/step - loss: 0.4636 - accuracy: 0.8074 - val_loss: 1.3027 - val_accuracy: 0.6658 - lr: 3.3300e-05\n",
      "Epoch 25/30\n",
      " 635/2361 [=======>......................] - ETA: 13:16 - loss: 0.4429 - accuracy: 0.8121"
     ]
    }
   ],
   "source": [
    "file_name = \"models/032623_15_16.h5\"\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        file_name, \n",
    "        save_best_only=True, \n",
    "        monitor=\"val_accuracy\",\n",
    "        mode=\"max\",\n",
    "        verbose = 1\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', factor=0.1,mode='max',verbose=1,\n",
    "                              patience=3, min_lr=0.000001)\n",
    "]\n",
    "model.fit(train_datagen,validation_data=test_datagen,\\\n",
    "          epochs=30, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
